{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50_MODEL=tf.keras.applications.ResNet50(input_shape=(224,224,3),\n",
    "                                               include_top=False,\n",
    "                                               weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ResNet50_MODEL.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv2_block1_0_bn'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNet50_MODEL.layers[15].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in ResNet50_MODEL.layers:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ResNet50_MODEL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "l1_factor = 0.0001\n",
    "l2_factor = 0.001\n",
    "dropout = 0.5\n",
    "learning_rate = 0.0001\n",
    "target_size_image_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "Dropout_Regularization1 (Dro (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3896)              7982904   \n",
      "=================================================================\n",
      "Total params: 31,570,616\n",
      "Trainable params: 31,517,496\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "                                  ResNet50_MODEL,\n",
    "                                  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                  tf.keras.layers.Dropout(dropout, name='Dropout_Regularization1'),\n",
    "                                  tf.keras.layers.Dense(3896, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "train_data_dir = '../datasets/group7_set_224/set_224/train/'\n",
    "valid_data_dir = '../datasets/group7_set_224/set_224/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.25,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140556 images belonging to 3896 classes.\n"
     ]
    }
   ],
   "source": [
    "#flow training images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=target_size_image_shape,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35064 images belonging to 3896 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=target_size_image_shape,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Labels in  Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3896\n",
      "{0: '100054', 1: '100130', 2: '100178', 3: '100265', 4: '1003', 5: '100322', 6: '100599', 7: '100670', 8: '100693', 9: '100795', 10: '100852', 11: '100856', 12: '100879', 13: '100881', 14: '100890', 15: '100897', 16: '100919', 17: '101067', 18: '101073', 19: '101080', 20: '101150', 21: '101151', 22: '101174', 23: '101209', 24: '101221', 25: '101226', 26: '101292', 27: '101398', 28: '101425', 29: '101426', 30: '10148', 31: '1015', 32: '101522', 33: '101524', 34: '101527', 35: '101588', 36: '101603', 37: '101604', 38: '101669', 39: '101723', 40: '101794', 41: '101821', 42: '101827', 43: '101836', 44: '101873', 45: '101894', 46: '101981', 47: '10199', 48: '102033', 49: '102171', 50: '102183', 51: '1022', 52: '102223', 53: '102349', 54: '102379', 55: '102389', 56: '10241', 57: '102429', 58: '102462', 59: '10252', 60: '102524', 61: '102660', 62: '102770', 63: '102858', 64: '102888', 65: '102944', 66: '102947', 67: '103094', 68: '103098', 69: '103171', 70: '10322', 71: '10327', 72: '10329', 73: '103347', 74: '103500', 75: '103509', 76: '103518', 77: '103553', 78: '103555', 79: '103613', 80: '103630', 81: '10365', 82: '103733', 83: '103803', 84: '103872', 85: '103910', 86: '103955', 87: '103998', 88: '104012', 89: '104040', 90: '104109', 91: '104128', 92: '104229', 93: '104266', 94: '104318', 95: '104392', 96: '104407', 97: '104409', 98: '104533', 99: '104562', 100: '104566', 101: '10460', 102: '104660', 103: '104771', 104: '104809', 105: '104819', 106: '104957', 107: '104977', 108: '104990', 109: '105045', 110: '105121', 111: '105151', 112: '105168', 113: '105227', 114: '105242', 115: '105255', 116: '105380', 117: '105439', 118: '105445', 119: '105467', 120: '105559', 121: '105634', 122: '105646', 123: '105686', 124: '105757', 125: '105874', 126: '105920', 127: '106051', 128: '10606', 129: '106112', 130: '106180', 131: '106243', 132: '106374', 133: '106382', 134: '106439', 135: '106462', 136: '106473', 137: '106507', 138: '106573', 139: '106580', 140: '106693', 141: '106694', 142: '106740', 143: '106809', 144: '106847', 145: '106857', 146: '106945', 147: '106993', 148: '107021', 149: '107034', 150: '107082', 151: '107125', 152: '107157', 153: '107289', 154: '107363', 155: '107477', 156: '107567', 157: '107585', 158: '107590', 159: '107594', 160: '107651', 161: '107852', 162: '107888', 163: '107938', 164: '107990', 165: '108013', 166: '108049', 167: '108107', 168: '10812', 169: '108258', 170: '108292', 171: '108307', 172: '108494', 173: '108579', 174: '108624', 175: '108673', 176: '108720', 177: '108730', 178: '108749', 179: '108762', 180: '108855', 181: '10888', 182: '108907', 183: '108928', 184: '108992', 185: '109039', 186: '109091', 187: '109110', 188: '109123', 189: '109182', 190: '109261', 191: '109359', 192: '109447', 193: '109454', 194: '109530', 195: '109531', 196: '109574', 197: '109596', 198: '109599', 199: '109735', 200: '109857', 201: '10988', 202: '109895', 203: '10990', 204: '110007', 205: '110060', 206: '110094', 207: '11012', 208: '110201', 209: '11023', 210: '11048', 211: '110482', 212: '110593', 213: '110785', 214: '110823', 215: '11091', 216: '110912', 217: '110967', 218: '11100', 219: '111080', 220: '111097', 221: '111133', 222: '111153', 223: '111159', 224: '111197', 225: '11121', 226: '111210', 227: '111252', 228: '111296', 229: '111299', 230: '111378', 231: '111411', 232: '11145', 233: '111456', 234: '111495', 235: '111552', 236: '111592', 237: '111595', 238: '111598', 239: '11162', 240: '111629', 241: '111732', 242: '11177', 243: '11178', 244: '111964', 245: '111994', 246: '112010', 247: '112060', 248: '112125', 249: '112237', 250: '112260', 251: '112295', 252: '112362', 253: '112366', 254: '112512', 255: '112559', 256: '112575', 257: '112619', 258: '112671', 259: '112678', 260: '112723', 261: '112806', 262: '112828', 263: '112956', 264: '112957', 265: '112958', 266: '112970', 267: '112980', 268: '112987', 269: '113003', 270: '113076', 271: '113081', 272: '1131', 273: '113105', 274: '113281', 275: '113402', 276: '113448', 277: '11347', 278: '113501', 279: '11351', 280: '11362', 281: '113742', 282: '1138', 283: '113840', 284: '11388', 285: '11389', 286: '113925', 287: '113948', 288: '114011', 289: '114030', 290: '114100', 291: '114115', 292: '114200', 293: '114254', 294: '114261', 295: '114303', 296: '114378', 297: '114384', 298: '114411', 299: '114419', 300: '114422', 301: '114480', 302: '114489', 303: '114561', 304: '114602', 305: '114746', 306: '114753', 307: '114757', 308: '114838', 309: '114850', 310: '114862', 311: '114970', 312: '115', 313: '11505', 314: '115053', 315: '115054', 316: '115120', 317: '115148', 318: '115174', 319: '115185', 320: '115238', 321: '115312', 322: '115332', 323: '115378', 324: '115392', 325: '115428', 326: '115521', 327: '115532', 328: '115574', 329: '115714', 330: '115718', 331: '115758', 332: '115785', 333: '11579', 334: '115842', 335: '115845', 336: '115877', 337: '115882', 338: '115930', 339: '115956', 340: '115999', 341: '116033', 342: '116050', 343: '116079', 344: '116081', 345: '116101', 346: '116458', 347: '11647', 348: '116494', 349: '116512', 350: '116611', 351: '116616', 352: '116619', 353: '116659', 354: '116676', 355: '116829', 356: '116843', 357: '116936', 358: '117', 359: '117032', 360: '117123', 361: '117138', 362: '117151', 363: '117242', 364: '117273', 365: '1173', 366: '117321', 367: '11736', 368: '117360', 369: '117382', 370: '117409', 371: '117432', 372: '117465', 373: '117549', 374: '117590', 375: '117604', 376: '117669', 377: '117721', 378: '117750', 379: '117751', 380: '117878', 381: '117937', 382: '117951', 383: '117967', 384: '117996', 385: '118102', 386: '118140', 387: '118149', 388: '118153', 389: '118189', 390: '118211', 391: '118221', 392: '118233', 393: '118244', 394: '118290', 395: '118301', 396: '118433', 397: '118453', 398: '118486', 399: '118494', 400: '118515', 401: '118519', 402: '118554', 403: '118596', 404: '118609', 405: '11862', 406: '11864', 407: '118707', 408: '118722', 409: '118767', 410: '118846', 411: '118904', 412: '118915', 413: '119020', 414: '119044', 415: '119079', 416: '11912', 417: '119144', 418: '11918', 419: '1192', 420: '119265', 421: '119289', 422: '119295', 423: '119299', 424: '119328', 425: '119405', 426: '119515', 427: '11954', 428: '11955', 429: '119636', 430: '119690', 431: '119715', 432: '119731', 433: '119780', 434: '119792', 435: '119827', 436: '11995', 437: '119968', 438: '12012', 439: '120130', 440: '120152', 441: '12016', 442: '1202', 443: '120211', 444: '120269', 445: '12031', 446: '120330', 447: '120371', 448: '120385', 449: '120391', 450: '120396', 451: '120425', 452: '120475', 453: '120678', 454: '120679', 455: '120694', 456: '120759', 457: '120809', 458: '120812', 459: '12085', 460: '120881', 461: '120890', 462: '120899', 463: '120911', 464: '120936', 465: '120940', 466: '12096', 467: '121011', 468: '121019', 469: '121067', 470: '121085', 471: '12116', 472: '121202', 473: '121294', 474: '12135', 475: '121479', 476: '121482', 477: '121497', 478: '121516', 479: '121540', 480: '121669', 481: '121684', 482: '121689', 483: '121700', 484: '121709', 485: '121762', 486: '121764', 487: '121785', 488: '121800', 489: '121847', 490: '121899', 491: '121995', 492: '122019', 493: '122035', 494: '122080', 495: '122158', 496: '122199', 497: '122288', 498: '122346', 499: '122515', 500: '122528', 501: '12253', 502: '122557', 503: '122693', 504: '122702', 505: '122775', 506: '122850', 507: '122900', 508: '122921', 509: '122938', 510: '122939', 511: '123093', 512: '123229', 513: '123311', 514: '123376', 515: '12341', 516: '123450', 517: '123464', 518: '1235', 519: '123519', 520: '123525', 521: '123599', 522: '123619', 523: '123713', 524: '123758', 525: '1238', 526: '123800', 527: '123806', 528: '123810', 529: '123884', 530: '124', 531: '124006', 532: '124024', 533: '124085', 534: '124086', 535: '124212', 536: '124303', 537: '124359', 538: '124446', 539: '124469', 540: '124525', 541: '124574', 542: '124626', 543: '124699', 544: '124729', 545: '124738', 546: '124933', 547: '125130', 548: '125131', 549: '125154', 550: '125195', 551: '125202', 552: '125225', 553: '125240', 554: '12528', 555: '125296', 556: '125325', 557: '125328', 558: '12533', 559: '125341', 560: '125365', 561: '125428', 562: '125610', 563: '12565', 564: '125696', 565: '125830', 566: '125863', 567: '125868', 568: '125892', 569: '125899', 570: '125936', 571: '125994', 572: '126013', 573: '126033', 574: '126047', 575: '126121', 576: '126171', 577: '126216', 578: '126307', 579: '126315', 580: '126318', 581: '126475', 582: '126486', 583: '126642', 584: '126663', 585: '126671', 586: '126706', 587: '126758', 588: '126959', 589: '127044', 590: '127130', 591: '127199', 592: '127277', 593: '127297', 594: '127479', 595: '127483', 596: '127628', 597: '12773', 598: '127948', 599: '127980', 600: '128010', 601: '128039', 602: '128041', 603: '128053', 604: '128265', 605: '128319', 606: '128396', 607: '128438', 608: '128540', 609: '128556', 610: '128557', 611: '128703', 612: '128722', 613: '128748', 614: '128793', 615: '128935', 616: '128992', 617: '129035', 618: '129039', 619: '129058', 620: '129097', 621: '129099', 622: '129135', 623: '129144', 624: '129203', 625: '129265', 626: '129271', 627: '129294', 628: '129332', 629: '129339', 630: '129374', 631: '129418', 632: '129444', 633: '129447', 634: '129450', 635: '129497', 636: '129637', 637: '129723', 638: '12977', 639: '129821', 640: '129849', 641: '129856', 642: '129868', 643: '129898', 644: '129903', 645: '129973', 646: '129989', 647: '130066', 648: '130178', 649: '13020', 650: '130238', 651: '130242', 652: '130284', 653: '130341', 654: '130429', 655: '130523', 656: '130585', 657: '130586', 658: '130662', 659: '130708', 660: '1308', 661: '130824', 662: '130930', 663: '130965', 664: '130967', 665: '131020', 666: '131022', 667: '131054', 668: '131067', 669: '131077', 670: '131080', 671: '131093', 672: '131164', 673: '131221', 674: '131324', 675: '131340', 676: '131385', 677: '131425', 678: '131486', 679: '131497', 680: '131609', 681: '131649', 682: '131651', 683: '131696', 684: '131776', 685: '13190', 686: '131900', 687: '131940', 688: '131980', 689: '13204', 690: '132044', 691: '132103', 692: '132158', 693: '132200', 694: '132206', 695: '13222', 696: '132309', 697: '132428', 698: '132484', 699: '13252', 700: '132656', 701: '132667', 702: '132753', 703: '132759', 704: '132785', 705: '132896', 706: '132997', 707: '133161', 708: '133275', 709: '133295', 710: '133526', 711: '133534', 712: '133564', 713: '133618', 714: '133729', 715: '133774', 716: '133783', 717: '133786', 718: '133828', 719: '133842', 720: '13385', 721: '133851', 722: '133968', 723: '134007', 724: '13415', 725: '13426', 726: '134325', 727: '134344', 728: '134454', 729: '134502', 730: '134527', 731: '134534', 732: '134539', 733: '134553', 734: '13473', 735: '134773', 736: '134805', 737: '134882', 738: '134895', 739: '134945', 740: '134983', 741: '134994', 742: '135015', 743: '135025', 744: '135105', 745: '135107', 746: '135124', 747: '13516', 748: '135251', 749: '135255', 750: '135256', 751: '135294', 752: '135395', 753: '135406', 754: '135660', 755: '135685', 756: '135770', 757: '135772', 758: '135807', 759: '135846', 760: '136001', 761: '136024', 762: '136112', 763: '136137', 764: '136191', 765: '136243', 766: '136290', 767: '136348', 768: '13637', 769: '136406', 770: '136461', 771: '136469', 772: '13652', 773: '136587', 774: '136612', 775: '136647', 776: '136780', 777: '136794', 778: '136829', 779: '136843', 780: '136863', 781: '136893', 782: '137035', 783: '137067', 784: '137138', 785: '137159', 786: '137168', 787: '137183', 788: '137216', 789: '137229', 790: '137504', 791: '137529', 792: '137567', 793: '137615', 794: '137646', 795: '13766', 796: '137672', 797: '137740', 798: '137771', 799: '137772', 800: '137795', 801: '137900', 802: '137939', 803: '137975', 804: '138010', 805: '13810', 806: '138145', 807: '138155', 808: '13822', 809: '138241', 810: '13828', 811: '138287', 812: '138291', 813: '138410', 814: '138438', 815: '138445', 816: '138476', 817: '138612', 818: '138615', 819: '138628', 820: '138664', 821: '138820', 822: '138908', 823: '138931', 824: '13895', 825: '138988', 826: '138999', 827: '139042', 828: '139048', 829: '139092', 830: '139127', 831: '139243', 832: '139255', 833: '139257', 834: '139274', 835: '13931', 836: '139324', 837: '139331', 838: '139394', 839: '1395', 840: '139523', 841: '139692', 842: '139745', 843: '139782', 844: '139796', 845: '139858', 846: '139887', 847: '13990', 848: '139910', 849: '140048', 850: '140061', 851: '140129', 852: '140223', 853: '140229', 854: '140232', 855: '140270', 856: '140340', 857: '140389', 858: '140399', 859: '14040', 860: '140419', 861: '140422', 862: '140458', 863: '140481', 864: '140512', 865: '140543', 866: '140676', 867: '140777', 868: '140805', 869: '140832', 870: '14100', 871: '141053', 872: '14125', 873: '141295', 874: '141328', 875: '141352', 876: '141480', 877: '141484', 878: '141527', 879: '14153', 880: '14154', 881: '141574', 882: '141596', 883: '141718', 884: '141740', 885: '141745', 886: '141782', 887: '141818', 888: '141933', 889: '141957', 890: '141960', 891: '141984', 892: '142089', 893: '142129', 894: '142298', 895: '142353', 896: '14237', 897: '142415', 898: '142433', 899: '142452', 900: '142457', 901: '142471', 902: '14254', 903: '142541', 904: '142567', 905: '14262', 906: '142677', 907: '142738', 908: '142791', 909: '142822', 910: '142881', 911: '142961', 912: '142970', 913: '142997', 914: '143029', 915: '14305', 916: '143065', 917: '143161', 918: '143394', 919: '143484', 920: '143515', 921: '143676', 922: '143690', 923: '143714', 924: '143721', 925: '143774', 926: '143808', 927: '143828', 928: '14398', 929: '14403', 930: '144039', 931: '144041', 932: '144072', 933: '144275', 934: '144314', 935: '144364', 936: '144472', 937: '144528', 938: '144597', 939: '144669', 940: '144673', 941: '144711', 942: '144737', 943: '144749', 944: '144782', 945: '144785', 946: '14479', 947: '144887', 948: '144905', 949: '144930', 950: '144945', 951: '144974', 952: '144976', 953: '145027', 954: '145033', 955: '145068', 956: '145207', 957: '14521', 958: '145265', 959: '145277', 960: '145303', 961: '145306', 962: '145376', 963: '145471', 964: '145567', 965: '145588', 966: '145624', 967: '145656', 968: '14569', 969: '145691', 970: '145781', 971: '145908', 972: '146093', 973: '146135', 974: '146159', 975: '146172', 976: '14618', 977: '146201', 978: '146212', 979: '146249', 980: '146263', 981: '146411', 982: '14652', 983: '146572', 984: '14658', 985: '146623', 986: '146647', 987: '14666', 988: '146712', 989: '146727', 990: '146814', 991: '146837', 992: '146862', 993: '146887', 994: '146937', 995: '146939', 996: '147001', 997: '14701', 998: '147034', 999: '147124', 1000: '147156', 1001: '147176', 1002: '14725', 1003: '147313', 1004: '147349', 1005: '147381', 1006: '147427', 1007: '147574', 1008: '147640', 1009: '147680', 1010: '147708', 1011: '147719', 1012: '147764', 1013: '147874', 1014: '147876', 1015: '14788', 1016: '147936', 1017: '147971', 1018: '147974', 1019: '148117', 1020: '148168', 1021: '148174', 1022: '148187', 1023: '148205', 1024: '148213', 1025: '148245', 1026: '148462', 1027: '148490', 1028: '148514', 1029: '148597', 1030: '148676', 1031: '148693', 1032: '148724', 1033: '148782', 1034: '148828', 1035: '14883', 1036: '148888', 1037: '148941', 1038: '148944', 1039: '148974', 1040: '14901', 1041: '14902', 1042: '149106', 1043: '149108', 1044: '14921', 1045: '149233', 1046: '149246', 1047: '149248', 1048: '149256', 1049: '149324', 1050: '149414', 1051: '149446', 1052: '149486', 1053: '149540', 1054: '149548', 1055: '14958', 1056: '149590', 1057: '149669', 1058: '149739', 1059: '149783', 1060: '149845', 1061: '14988', 1062: '149961', 1063: '149981', 1064: '150001', 1065: '150064', 1066: '150142', 1067: '150144', 1068: '150160', 1069: '150236', 1070: '150244', 1071: '150259', 1072: '150279', 1073: '150303', 1074: '150336', 1075: '150348', 1076: '150388', 1077: '150392', 1078: '150418', 1079: '150434', 1080: '150480', 1081: '150489', 1082: '150552', 1083: '150653', 1084: '150666', 1085: '150667', 1086: '150668', 1087: '150701', 1088: '150742', 1089: '150785', 1090: '150835', 1091: '150837', 1092: '150944', 1093: '150950', 1094: '150969', 1095: '150985', 1096: '151023', 1097: '151057', 1098: '151063', 1099: '151121', 1100: '151267', 1101: '151352', 1102: '151387', 1103: '151397', 1104: '151409', 1105: '151479', 1106: '151495', 1107: '15158', 1108: '151627', 1109: '15163', 1110: '151674', 1111: '151748', 1112: '151758', 1113: '151781', 1114: '151787', 1115: '151831', 1116: '151862', 1117: '151882', 1118: '151949', 1119: '151964', 1120: '151987', 1121: '152011', 1122: '152035', 1123: '15205', 1124: '152066', 1125: '152079', 1126: '152093', 1127: '152102', 1128: '152124', 1129: '152146', 1130: '152148', 1131: '152184', 1132: '152337', 1133: '152373', 1134: '152428', 1135: '152447', 1136: '15248', 1137: '152502', 1138: '152613', 1139: '152704', 1140: '152712', 1141: '152769', 1142: '152914', 1143: '152961', 1144: '153101', 1145: '153116', 1146: '153186', 1147: '153207', 1148: '15324', 1149: '153289', 1150: '153317', 1151: '153340', 1152: '153387', 1153: '153396', 1154: '153448', 1155: '153500', 1156: '153520', 1157: '15358', 1158: '153608', 1159: '153664', 1160: '153955', 1161: '154075', 1162: '154118', 1163: '154160', 1164: '154177', 1165: '154186', 1166: '154245', 1167: '154341', 1168: '154368', 1169: '154382', 1170: '154406', 1171: '154480', 1172: '15454', 1173: '154581', 1174: '154586', 1175: '154604', 1176: '154664', 1177: '154709', 1178: '154742', 1179: '154780', 1180: '154832', 1181: '154835', 1182: '154856', 1183: '154911', 1184: '154919', 1185: '15493', 1186: '154952', 1187: '154984', 1188: '15499', 1189: '155019', 1190: '155061', 1191: '155095', 1192: '155109', 1193: '15514', 1194: '155148', 1195: '155230', 1196: '155319', 1197: '155462', 1198: '155508', 1199: '15557', 1200: '155582', 1201: '155591', 1202: '155653', 1203: '155761', 1204: '155771', 1205: '15583', 1206: '155847', 1207: '155882', 1208: '155908', 1209: '155971', 1210: '156000', 1211: '156018', 1212: '15602', 1213: '156062', 1214: '15608', 1215: '156117', 1216: '156130', 1217: '156154', 1218: '156183', 1219: '156261', 1220: '156317', 1221: '156318', 1222: '156338', 1223: '156365', 1224: '156401', 1225: '156412', 1226: '15645', 1227: '156501', 1228: '156521', 1229: '156552', 1230: '156554', 1231: '156667', 1232: '156674', 1233: '156793', 1234: '156795', 1235: '1568', 1236: '156801', 1237: '156819', 1238: '156858', 1239: '156873', 1240: '156932', 1241: '156945', 1242: '156958', 1243: '156981', 1244: '157032', 1245: '157047', 1246: '157071', 1247: '157147', 1248: '157161', 1249: '157162', 1250: '157197', 1251: '157216', 1252: '157217', 1253: '157307', 1254: '15732', 1255: '157368', 1256: '157369', 1257: '157383', 1258: '157433', 1259: '157437', 1260: '157497', 1261: '157530', 1262: '157680', 1263: '157825', 1264: '157840', 1265: '157891', 1266: '157905', 1267: '157912', 1268: '158050', 1269: '158060', 1270: '158094', 1271: '158097', 1272: '158170', 1273: '158187', 1274: '158202', 1275: '158295', 1276: '15856', 1277: '158604', 1278: '15865', 1279: '158655', 1280: '158686', 1281: '158861', 1282: '158899', 1283: '158917', 1284: '158970', 1285: '159122', 1286: '159125', 1287: '159147', 1288: '159181', 1289: '159204', 1290: '159241', 1291: '159247', 1292: '159278', 1293: '159311', 1294: '159472', 1295: '159489', 1296: '159498', 1297: '159560', 1298: '159581', 1299: '159737', 1300: '159778', 1301: '159796', 1302: '15984', 1303: '159961', 1304: '15998', 1305: '160095', 1306: '160191', 1307: '160256', 1308: '160350', 1309: '160451', 1310: '160453', 1311: '160467', 1312: '160528', 1313: '16053', 1314: '160547', 1315: '160573', 1316: '160782', 1317: '160850', 1318: '160852', 1319: '160872', 1320: '16090', 1321: '160967', 1322: '1610', 1323: '161015', 1324: '161044', 1325: '161066', 1326: '161101', 1327: '161131', 1328: '161140', 1329: '161199', 1330: '1612', 1331: '161229', 1332: '161261', 1333: '161299', 1334: '161346', 1335: '16135', 1336: '161375', 1337: '161391', 1338: '161406', 1339: '161480', 1340: '161519', 1341: '16152', 1342: '161539', 1343: '161688', 1344: '161726', 1345: '161759', 1346: '161781', 1347: '161859', 1348: '16186', 1349: '161898', 1350: '162001', 1351: '162019', 1352: '162027', 1353: '162056', 1354: '162062', 1355: '162218', 1356: '16222', 1357: '162246', 1358: '162317', 1359: '162589', 1360: '162619', 1361: '162660', 1362: '16277', 1363: '162796', 1364: '162862', 1365: '162863', 1366: '162929', 1367: '162937', 1368: '162960', 1369: '162999', 1370: '16304', 1371: '16316', 1372: '163168', 1373: '163182', 1374: '163215', 1375: '163310', 1376: '163325', 1377: '163337', 1378: '163395', 1379: '163459', 1380: '163468', 1381: '163517', 1382: '163537', 1383: '163633', 1384: '163638', 1385: '163645', 1386: '163676', 1387: '163765', 1388: '163787', 1389: '163850', 1390: '163925', 1391: '163936', 1392: '164056', 1393: '164222', 1394: '164256', 1395: '164274', 1396: '164339', 1397: '164415', 1398: '164474', 1399: '164483', 1400: '164581', 1401: '164609', 1402: '164613', 1403: '164693', 1404: '164851', 1405: '165004', 1406: '165121', 1407: '165312', 1408: '165431', 1409: '165523', 1410: '165539', 1411: '165588', 1412: '165595', 1413: '165638', 1414: '165662', 1415: '165708', 1416: '165800', 1417: '165808', 1418: '165836', 1419: '165874', 1420: '165938', 1421: '166029', 1422: '166045', 1423: '166052', 1424: '166169', 1425: '166178', 1426: '166281', 1427: '16629', 1428: '166304', 1429: '166328', 1430: '166329', 1431: '166344', 1432: '166358', 1433: '166437', 1434: '166498', 1435: '166516', 1436: '16653', 1437: '166534', 1438: '166552', 1439: '166572', 1440: '166602', 1441: '166637', 1442: '166694', 1443: '166717', 1444: '166734', 1445: '166753', 1446: '166935', 1447: '167013', 1448: '167015', 1449: '167047', 1450: '167097', 1451: '167099', 1452: '167118', 1453: '167231', 1454: '16727', 1455: '167273', 1456: '167282', 1457: '16742', 1458: '167437', 1459: '167527', 1460: '167546', 1461: '16756', 1462: '167633', 1463: '167676', 1464: '167681', 1465: '16770', 1466: '167742', 1467: '167762', 1468: '167889', 1469: '167913', 1470: '167914', 1471: '16793', 1472: '167937', 1473: '167957', 1474: '167972', 1475: '167979', 1476: '167995', 1477: '168', 1478: '168001', 1479: '168073', 1480: '16809', 1481: '168103', 1482: '168168', 1483: '168372', 1484: '168446', 1485: '168538', 1486: '1686', 1487: '16866', 1488: '168719', 1489: '168841', 1490: '168887', 1491: '168897', 1492: '1689', 1493: '168900', 1494: '168977', 1495: '169139', 1496: '169213', 1497: '169216', 1498: '169220', 1499: '169234', 1500: '169346', 1501: '169381', 1502: '16939', 1503: '169404', 1504: '169432', 1505: '169437', 1506: '169450', 1507: '169634', 1508: '169729', 1509: '169786', 1510: '169873', 1511: '169918', 1512: '16995', 1513: '16997', 1514: '170106', 1515: '170330', 1516: '170385', 1517: '170397', 1518: '170473', 1519: '170595', 1520: '170603', 1521: '170627', 1522: '170658', 1523: '170705', 1524: '170719', 1525: '170767', 1526: '170807', 1527: '170815', 1528: '170843', 1529: '170847', 1530: '170864', 1531: '170887', 1532: '170912', 1533: '17098', 1534: '170988', 1535: '171001', 1536: '171010', 1537: '171019', 1538: '171034', 1539: '171079', 1540: '171149', 1541: '171167', 1542: '171188', 1543: '17121', 1544: '171247', 1545: '171307', 1546: '171359', 1547: '17137', 1548: '171403', 1549: '171408', 1550: '171426', 1551: '171510', 1552: '171525', 1553: '171689', 1554: '171690', 1555: '171694', 1556: '17174', 1557: '171741', 1558: '171838', 1559: '172044', 1560: '172087', 1561: '17211', 1562: '172187', 1563: '17219', 1564: '172218', 1565: '172267', 1566: '172299', 1567: '17240', 1568: '172411', 1569: '172417', 1570: '17243', 1571: '172552', 1572: '172555', 1573: '172570', 1574: '172579', 1575: '172600', 1576: '172622', 1577: '172841', 1578: '172855', 1579: '17290', 1580: '172908', 1581: '173007', 1582: '173014', 1583: '173049', 1584: '173148', 1585: '173213', 1586: '173220', 1587: '173393', 1588: '17343', 1589: '173466', 1590: '173476', 1591: '173492', 1592: '173496', 1593: '17350', 1594: '17357', 1595: '17360', 1596: '173619', 1597: '173632', 1598: '173660', 1599: '173685', 1600: '17370', 1601: '17375', 1602: '173770', 1603: '173852', 1604: '173918', 1605: '173986', 1606: '174002', 1607: '174095', 1608: '174109', 1609: '174141', 1610: '174147', 1611: '17415', 1612: '174243', 1613: '174252', 1614: '174288', 1615: '174467', 1616: '174524', 1617: '174555', 1618: '174627', 1619: '174656', 1620: '174738', 1621: '174764', 1622: '174810', 1623: '174813', 1624: '174821', 1625: '174853', 1626: '174927', 1627: '175000', 1628: '175015', 1629: '17516', 1630: '175273', 1631: '175289', 1632: '175325', 1633: '175341', 1634: '175344', 1635: '175367', 1636: '175402', 1637: '175460', 1638: '175498', 1639: '175535', 1640: '175609', 1641: '175635', 1642: '175653', 1643: '175723', 1644: '175725', 1645: '175752', 1646: '175801', 1647: '175828', 1648: '175848', 1649: '175884', 1650: '175934', 1651: '17602', 1652: '176272', 1653: '17643', 1654: '176463', 1655: '176472', 1656: '17648', 1657: '176599', 1658: '176609', 1659: '17665', 1660: '176699', 1661: '176760', 1662: '176827', 1663: '176831', 1664: '176872', 1665: '176924', 1666: '177009', 1667: '177032', 1668: '177043', 1669: '177056', 1670: '177235', 1671: '177243', 1672: '177283', 1673: '177284', 1674: '177342', 1675: '177354', 1676: '177405', 1677: '177491', 1678: '177509', 1679: '177615', 1680: '177716', 1681: '177820', 1682: '17788', 1683: '177893', 1684: '177987', 1685: '177998', 1686: '178002', 1687: '178043', 1688: '178065', 1689: '17809', 1690: '178115', 1691: '178119', 1692: '178206', 1693: '178241', 1694: '178242', 1695: '178264', 1696: '178266', 1697: '178274', 1698: '178299', 1699: '178364', 1700: '178369', 1701: '178451', 1702: '178512', 1703: '178580', 1704: '178626', 1705: '178644', 1706: '178666', 1707: '178685', 1708: '178836', 1709: '178878', 1710: '178884', 1711: '178976', 1712: '179021', 1713: '179022', 1714: '179042', 1715: '179127', 1716: '179129', 1717: '179131', 1718: '179166', 1719: '179176', 1720: '179190', 1721: '179224', 1722: '179244', 1723: '179254', 1724: '17926', 1725: '179262', 1726: '179339', 1727: '179353', 1728: '179422', 1729: '179457', 1730: '17947', 1731: '179492', 1732: '179625', 1733: '179632', 1734: '179718', 1735: '179732', 1736: '179736', 1737: '179770', 1738: '179804', 1739: '179825', 1740: '179829', 1741: '179840', 1742: '179849', 1743: '179853', 1744: '179922', 1745: '180034', 1746: '180054', 1747: '180062', 1748: '180082', 1749: '180102', 1750: '180131', 1751: '18031', 1752: '180322', 1753: '180420', 1754: '180477', 1755: '18050', 1756: '18057', 1757: '180577', 1758: '180711', 1759: '180715', 1760: '180765', 1761: '180845', 1762: '180846', 1763: '180858', 1764: '180910', 1765: '180946', 1766: '181151', 1767: '181197', 1768: '181208', 1769: '181210', 1770: '181238', 1771: '181261', 1772: '181300', 1773: '18136', 1774: '181386', 1775: '181435', 1776: '181510', 1777: '181580', 1778: '181586', 1779: '181594', 1780: '181623', 1781: '181705', 1782: '181764', 1783: '181809', 1784: '181852', 1785: '181878', 1786: '181954', 1787: '181961', 1788: '181964', 1789: '18197', 1790: '181981', 1791: '1820', 1792: '18209', 1793: '18219', 1794: '182494', 1795: '182508', 1796: '182681', 1797: '182711', 1798: '182830', 1799: '182916', 1800: '182917', 1801: '183056', 1802: '183105', 1803: '183165', 1804: '183227', 1805: '183287', 1806: '183288', 1807: '183340', 1808: '183377', 1809: '183512', 1810: '183582', 1811: '183594', 1812: '183599', 1813: '183709', 1814: '183776', 1815: '183780', 1816: '183826', 1817: '183860', 1818: '183865', 1819: '183910', 1820: '183943', 1821: '184016', 1822: '184026', 1823: '18404', 1824: '184071', 1825: '184080', 1826: '184465', 1827: '184521', 1828: '184553', 1829: '184583', 1830: '184641', 1831: '18472', 1832: '18473', 1833: '184760', 1834: '184822', 1835: '184866', 1836: '184883', 1837: '18489', 1838: '18492', 1839: '184956', 1840: '184999', 1841: '185025', 1842: '185085', 1843: '185106', 1844: '185129', 1845: '185155', 1846: '185165', 1847: '185248', 1848: '185286', 1849: '18529', 1850: '185313', 1851: '185327', 1852: '185333', 1853: '18546', 1854: '185505', 1855: '185509', 1856: '185521', 1857: '185581', 1858: '185662', 1859: '185699', 1860: '185722', 1861: '185740', 1862: '185748', 1863: '185751', 1864: '185759', 1865: '185899', 1866: '185909', 1867: '186015', 1868: '186052', 1869: '186084', 1870: '18609', 1871: '186174', 1872: '186190', 1873: '18621', 1874: '186273', 1875: '18641', 1876: '186437', 1877: '186616', 1878: '186651', 1879: '186685', 1880: '186875', 1881: '186986', 1882: '186987', 1883: '186995', 1884: '187004', 1885: '187018', 1886: '187021', 1887: '187074', 1888: '187195', 1889: '187257', 1890: '18727', 1891: '187275', 1892: '187341', 1893: '187342', 1894: '187389', 1895: '187395', 1896: '18740', 1897: '187422', 1898: '187751', 1899: '187755', 1900: '187763', 1901: '187783', 1902: '187837', 1903: '187946', 1904: '187956', 1905: '18796', 1906: '188184', 1907: '188202', 1908: '188205', 1909: '188228', 1910: '188243', 1911: '188287', 1912: '188301', 1913: '18832', 1914: '188485', 1915: '188515', 1916: '18853', 1917: '188531', 1918: '188597', 1919: '188638', 1920: '188672', 1921: '188739', 1922: '188871', 1923: '188904', 1924: '188905', 1925: '18899', 1926: '1890', 1927: '189103', 1928: '189180', 1929: '189321', 1930: '189336', 1931: '189353', 1932: '189382', 1933: '189465', 1934: '189467', 1935: '189470', 1936: '189529', 1937: '189591', 1938: '189609', 1939: '189671', 1940: '189698', 1941: '189708', 1942: '189709', 1943: '189730', 1944: '189800', 1945: '189805', 1946: '189816', 1947: '189999', 1948: '19000', 1949: '190015', 1950: '190145', 1951: '190229', 1952: '190410', 1953: '190460', 1954: '190493', 1955: '190498', 1956: '190559', 1957: '19057', 1958: '190578', 1959: '190633', 1960: '190653', 1961: '190710', 1962: '190732', 1963: '190834', 1964: '19085', 1965: '190861', 1966: '190875', 1967: '190927', 1968: '190983', 1969: '191010', 1970: '191025', 1971: '19104', 1972: '191072', 1973: '191122', 1974: '19113', 1975: '191170', 1976: '191179', 1977: '191296', 1978: '191322', 1979: '191377', 1980: '191381', 1981: '191433', 1982: '191459', 1983: '191573', 1984: '19172', 1985: '191945', 1986: '191993', 1987: '192001', 1988: '192007', 1989: '192074', 1990: '192188', 1991: '192199', 1992: '192201', 1993: '192203', 1994: '192250', 1995: '192401', 1996: '192414', 1997: '192478', 1998: '192480', 1999: '192521', 2000: '192636', 2001: '192673', 2002: '19280', 2003: '192836', 2004: '192921', 2005: '192923', 2006: '192945', 2007: '192947', 2008: '192999', 2009: '193135', 2010: '193144', 2011: '193223', 2012: '193236', 2013: '193247', 2014: '193286', 2015: '193331', 2016: '193353', 2017: '193375', 2018: '193436', 2019: '193524', 2020: '193628', 2021: '193740', 2022: '193756', 2023: '193782', 2024: '193930', 2025: '193988', 2026: '194033', 2027: '194038', 2028: '194044', 2029: '19405', 2030: '194068', 2031: '194239', 2032: '194252', 2033: '194319', 2034: '194323', 2035: '194354', 2036: '194430', 2037: '194455', 2038: '194584', 2039: '194592', 2040: '194627', 2041: '194714', 2042: '194762', 2043: '194776', 2044: '19483', 2045: '194847', 2046: '194878', 2047: '194883', 2048: '19503', 2049: '195052', 2050: '195098', 2051: '195190', 2052: '195191', 2053: '195235', 2054: '195279', 2055: '195378', 2056: '19541', 2057: '195486', 2058: '195519', 2059: '195623', 2060: '195640', 2061: '195679', 2062: '19573', 2063: '195762', 2064: '19580', 2065: '195803', 2066: '195872', 2067: '195887', 2068: '195927', 2069: '196002', 2070: '196010', 2071: '196154', 2072: '196296', 2073: '196320', 2074: '196337', 2075: '196338', 2076: '196343', 2077: '19637', 2078: '196547', 2079: '196552', 2080: '196585', 2081: '196632', 2082: '196730', 2083: '196780', 2084: '196874', 2085: '196875', 2086: '196946', 2087: '196953', 2088: '196981', 2089: '197018', 2090: '197063', 2091: '197146', 2092: '197149', 2093: '19730', 2094: '19735', 2095: '197390', 2096: '197473', 2097: '197495', 2098: '197520', 2099: '197534', 2100: '197536', 2101: '197538', 2102: '197624', 2103: '197667', 2104: '197775', 2105: '197801', 2106: '197838', 2107: '197855', 2108: '19790', 2109: '198032', 2110: '198078', 2111: '198115', 2112: '198143', 2113: '198263', 2114: '198270', 2115: '198387', 2116: '198406', 2117: '198455', 2118: '198471', 2119: '198507', 2120: '19851', 2121: '198520', 2122: '198526', 2123: '198540', 2124: '198554', 2125: '198559', 2126: '198712', 2127: '198732', 2128: '19878', 2129: '198789', 2130: '198853', 2131: '198866', 2132: '198887', 2133: '198949', 2134: '199076', 2135: '199082', 2136: '199116', 2137: '199166', 2138: '199271', 2139: '19928', 2140: '199388', 2141: '199458', 2142: '199462', 2143: '199463', 2144: '199491', 2145: '199602', 2146: '199668', 2147: '199754', 2148: '199784', 2149: '199836', 2150: '199878', 2151: '199915', 2152: '199949', 2153: '199975', 2154: '199976', 2155: '200050', 2156: '200108', 2157: '200316', 2158: '20034', 2159: '200348', 2160: '200357', 2161: '200380', 2162: '200402', 2163: '200403', 2164: '20045', 2165: '200493', 2166: '200523', 2167: '200604', 2168: '200642', 2169: '200647', 2170: '200702', 2171: '200728', 2172: '200819', 2173: '200841', 2174: '200857', 2175: '200900', 2176: '200903', 2177: '20092', 2178: '200929', 2179: '200953', 2180: '201000', 2181: '201010', 2182: '201052', 2183: '201054', 2184: '201072', 2185: '201104', 2186: '201134', 2187: '201148', 2188: '201167', 2189: '201171', 2190: '201197', 2191: '201277', 2192: '201307', 2193: '201331', 2194: '201337', 2195: '201379', 2196: '201382', 2197: '201503', 2198: '201632', 2199: '20169', 2200: '201727', 2201: '201867', 2202: '201889', 2203: '201896', 2204: '201920', 2205: '201921', 2206: '201932', 2207: '201963', 2208: '201969', 2209: '20200', 2210: '202023', 2211: '20208', 2212: '20209', 2213: '202122', 2214: '202143', 2215: '202203', 2216: '202272', 2217: '202307', 2218: '202322', 2219: '202351', 2220: '202424', 2221: '202430', 2222: '202512', 2223: '202532', 2224: '202595', 2225: '202754', 2226: '202778', 2227: '202864', 2228: '202871', 2229: '202916', 2230: '202967', 2231: '203040', 2232: '203056', 2233: '203091', 2234: '20330', 2235: '20373', 2236: '20441', 2237: '20474', 2238: '20567', 2239: '20598', 2240: '20783', 2241: '20786', 2242: '20847', 2243: '20849', 2244: '20899', 2245: '20946', 2246: '2096', 2247: '20987', 2248: '2106', 2249: '21218', 2250: '21234', 2251: '21320', 2252: '21331', 2253: '21351', 2254: '21404', 2255: '21427', 2256: '21518', 2257: '21578', 2258: '21587', 2259: '21594', 2260: '21633', 2261: '21663', 2262: '21665', 2263: '21666', 2264: '21722', 2265: '21748', 2266: '21749', 2267: '2183', 2268: '22012', 2269: '22028', 2270: '22101', 2271: '22125', 2272: '22126', 2273: '22152', 2274: '22601', 2275: '22693', 2276: '22702', 2277: '22794', 2278: '2291', 2279: '2292', 2280: '22929', 2281: '22943', 2282: '22953', 2283: '23043', 2284: '23106', 2285: '23160', 2286: '23176', 2287: '23180', 2288: '23190', 2289: '23231', 2290: '23251', 2291: '23293', 2292: '23351', 2293: '23364', 2294: '23408', 2295: '23431', 2296: '23460', 2297: '23477', 2298: '23663', 2299: '23717', 2300: '2373', 2301: '23731', 2302: '23797', 2303: '23803', 2304: '23830', 2305: '23897', 2306: '24049', 2307: '24102', 2308: '24120', 2309: '24151', 2310: '24468', 2311: '24490', 2312: '24509', 2313: '24511', 2314: '24598', 2315: '24655', 2316: '247', 2317: '24798', 2318: '24805', 2319: '24816', 2320: '24839', 2321: '2487', 2322: '24950', 2323: '24983', 2324: '25116', 2325: '2516', 2326: '25198', 2327: '25222', 2328: '25278', 2329: '25304', 2330: '25451', 2331: '25564', 2332: '25582', 2333: '25632', 2334: '25650', 2335: '25655', 2336: '25659', 2337: '25676', 2338: '2573', 2339: '25734', 2340: '25749', 2341: '25828', 2342: '25916', 2343: '25924', 2344: '25933', 2345: '25969', 2346: '25980', 2347: '26030', 2348: '26150', 2349: '2632', 2350: '26321', 2351: '26383', 2352: '2639', 2353: '26431', 2354: '2650', 2355: '26716', 2356: '26722', 2357: '26761', 2358: '26911', 2359: '26996', 2360: '27001', 2361: '2711', 2362: '27254', 2363: '27274', 2364: '27291', 2365: '27303', 2366: '27365', 2367: '27367', 2368: '27387', 2369: '27401', 2370: '27473', 2371: '27490', 2372: '27665', 2373: '27753', 2374: '2776', 2375: '27769', 2376: '27778', 2377: '27789', 2378: '2783', 2379: '27850', 2380: '27937', 2381: '27997', 2382: '28046', 2383: '28069', 2384: '28169', 2385: '28178', 2386: '28197', 2387: '28251', 2388: '28364', 2389: '28385', 2390: '28407', 2391: '28477', 2392: '28535', 2393: '2856', 2394: '28629', 2395: '28717', 2396: '28728', 2397: '28761', 2398: '28763', 2399: '28880', 2400: '28887', 2401: '28912', 2402: '28951', 2403: '29082', 2404: '29133', 2405: '29134', 2406: '29176', 2407: '29182', 2408: '29189', 2409: '29267', 2410: '29298', 2411: '29328', 2412: '29341', 2413: '29342', 2414: '29432', 2415: '29464', 2416: '29523', 2417: '29566', 2418: '29595', 2419: '29708', 2420: '29709', 2421: '29751', 2422: '29775', 2423: '29781', 2424: '29834', 2425: '29836', 2426: '29850', 2427: '30028', 2428: '30076', 2429: '30079', 2430: '30082', 2431: '30139', 2432: '30151', 2433: '30184', 2434: '30189', 2435: '30226', 2436: '30237', 2437: '30261', 2438: '30343', 2439: '30370', 2440: '30404', 2441: '3051', 2442: '30573', 2443: '30620', 2444: '30631', 2445: '30747', 2446: '3077', 2447: '30771', 2448: '30824', 2449: '31101', 2450: '31131', 2451: '31194', 2452: '3123', 2453: '31242', 2454: '31317', 2455: '31370', 2456: '31390', 2457: '31395', 2458: '31403', 2459: '31524', 2460: '31555', 2461: '31556', 2462: '31751', 2463: '31801', 2464: '31816', 2465: '31882', 2466: '31927', 2467: '31969', 2468: '32059', 2469: '32082', 2470: '32199', 2471: '32213', 2472: '32230', 2473: '32234', 2474: '32238', 2475: '32361', 2476: '32455', 2477: '32707', 2478: '32720', 2479: '32721', 2480: '32749', 2481: '32827', 2482: '32848', 2483: '32911', 2484: '32922', 2485: '32926', 2486: '32961', 2487: '32998', 2488: '33041', 2489: '33046', 2490: '33087', 2491: '3319', 2492: '33264', 2493: '33282', 2494: '33288', 2495: '33296', 2496: '33328', 2497: '33404', 2498: '3352', 2499: '33528', 2500: '33560', 2501: '33600', 2502: '33844', 2503: '33910', 2504: '33911', 2505: '34037', 2506: '34097', 2507: '34175', 2508: '34241', 2509: '34270', 2510: '34328', 2511: '3441', 2512: '34453', 2513: '3464', 2514: '34737', 2515: '34764', 2516: '34777', 2517: '34787', 2518: '34792', 2519: '34796', 2520: '3483', 2521: '34878', 2522: '34886', 2523: '34909', 2524: '3494', 2525: '35140', 2526: '35207', 2527: '35283', 2528: '35304', 2529: '3532', 2530: '35506', 2531: '35519', 2532: '35574', 2533: '35645', 2534: '35684', 2535: '35743', 2536: '35755', 2537: '35772', 2538: '35776', 2539: '35801', 2540: '35866', 2541: '35889', 2542: '36028', 2543: '36076', 2544: '36154', 2545: '36171', 2546: '36182', 2547: '36209', 2548: '36218', 2549: '36223', 2550: '36234', 2551: '36263', 2552: '36266', 2553: '36327', 2554: '36334', 2555: '36576', 2556: '36613', 2557: '36617', 2558: '36643', 2559: '36681', 2560: '36744', 2561: '36769', 2562: '36797', 2563: '3680', 2564: '36860', 2565: '36901', 2566: '36947', 2567: '36952', 2568: '36972', 2569: '36987', 2570: '37015', 2571: '37040', 2572: '37149', 2573: '37195', 2574: '37261', 2575: '37270', 2576: '37305', 2577: '37337', 2578: '37363', 2579: '37402', 2580: '37403', 2581: '3743', 2582: '37498', 2583: '37723', 2584: '37743', 2585: '37778', 2586: '37786', 2587: '37948', 2588: '38037', 2589: '38053', 2590: '38095', 2591: '38111', 2592: '38149', 2593: '38196', 2594: '38248', 2595: '38283', 2596: '38364', 2597: '3840', 2598: '38483', 2599: '3851', 2600: '38521', 2601: '38562', 2602: '38584', 2603: '386', 2604: '3862', 2605: '38630', 2606: '38636', 2607: '38659', 2608: '38689', 2609: '3870', 2610: '38761', 2611: '38983', 2612: '39008', 2613: '39033', 2614: '3905', 2615: '39116', 2616: '3915', 2617: '3920', 2618: '3922', 2619: '39240', 2620: '39343', 2621: '39412', 2622: '39486', 2623: '39536', 2624: '39544', 2625: '3965', 2626: '39793', 2627: '39837', 2628: '39844', 2629: '39851', 2630: '39858', 2631: '39901', 2632: '39904', 2633: '3993', 2634: '39949', 2635: '39971', 2636: '40021', 2637: '40054', 2638: '40086', 2639: '40098', 2640: '40115', 2641: '40167', 2642: '40192', 2643: '40248', 2644: '40267', 2645: '40275', 2646: '40326', 2647: '40336', 2648: '40337', 2649: '40393', 2650: '40395', 2651: '40404', 2652: '40420', 2653: '40423', 2654: '40434', 2655: '40474', 2656: '40489', 2657: '40503', 2658: '40512', 2659: '4058', 2660: '40581', 2661: '40597', 2662: '40722', 2663: '40792', 2664: '40809', 2665: '40999', 2666: '41053', 2667: '41064', 2668: '41097', 2669: '41110', 2670: '41133', 2671: '41167', 2672: '41182', 2673: '41257', 2674: '41331', 2675: '41361', 2676: '41417', 2677: '41518', 2678: '4152', 2679: '41548', 2680: '41591', 2681: '41615', 2682: '41656', 2683: '41679', 2684: '41690', 2685: '41702', 2686: '41741', 2687: '41911', 2688: '42032', 2689: '42068', 2690: '42083', 2691: '42134', 2692: '42253', 2693: '42301', 2694: '42307', 2695: '4232', 2696: '42344', 2697: '42412', 2698: '42550', 2699: '42583', 2700: '42630', 2701: '42710', 2702: '42757', 2703: '42788', 2704: '4279', 2705: '42827', 2706: '42846', 2707: '43042', 2708: '43054', 2709: '43117', 2710: '43118', 2711: '43129', 2712: '43173', 2713: '43224', 2714: '43322', 2715: '43389', 2716: '43414', 2717: '43420', 2718: '43443', 2719: '43485', 2720: '43486', 2721: '43495', 2722: '43554', 2723: '43582', 2724: '43807', 2725: '43840', 2726: '43902', 2727: '44029', 2728: '44036', 2729: '44049', 2730: '44094', 2731: '44135', 2732: '44175', 2733: '44221', 2734: '44239', 2735: '44280', 2736: '44309', 2737: '44320', 2738: '44347', 2739: '4435', 2740: '44422', 2741: '44452', 2742: '44458', 2743: '44547', 2744: '44549', 2745: '44697', 2746: '44715', 2747: '4477', 2748: '44829', 2749: '44923', 2750: '45054', 2751: '45146', 2752: '4520', 2753: '45222', 2754: '45233', 2755: '45266', 2756: '45271', 2757: '45275', 2758: '45286', 2759: '45297', 2760: '45304', 2761: '45309', 2762: '45359', 2763: '45377', 2764: '45385', 2765: '45451', 2766: '45577', 2767: '45590', 2768: '45673', 2769: '45749', 2770: '45764', 2771: '4587', 2772: '45936', 2773: '4598', 2774: '460', 2775: '46024', 2776: '46312', 2777: '46356', 2778: '46367', 2779: '46389', 2780: '46394', 2781: '46451', 2782: '46600', 2783: '46719', 2784: '46730', 2785: '46792', 2786: '46895', 2787: '46917', 2788: '46966', 2789: '47032', 2790: '47054', 2791: '47057', 2792: '47116', 2793: '47132', 2794: '47194', 2795: '47258', 2796: '47316', 2797: '47330', 2798: '47334', 2799: '47367', 2800: '47427', 2801: '47471', 2802: '47649', 2803: '47715', 2804: '47725', 2805: '47739', 2806: '47746', 2807: '47750', 2808: '47756', 2809: '47836', 2810: '4786', 2811: '47994', 2812: '48002', 2813: '48064', 2814: '48168', 2815: '48173', 2816: '48266', 2817: '48321', 2818: '48382', 2819: '48397', 2820: '48438', 2821: '48452', 2822: '48529', 2823: '48546', 2824: '48562', 2825: '48711', 2826: '48741', 2827: '48907', 2828: '48919', 2829: '48947', 2830: '49035', 2831: '49111', 2832: '49142', 2833: '49198', 2834: '49229', 2835: '49244', 2836: '49255', 2837: '4928', 2838: '49282', 2839: '49335', 2840: '49342', 2841: '49460', 2842: '49462', 2843: '4952', 2844: '49555', 2845: '49578', 2846: '49586', 2847: '49588', 2848: '49683', 2849: '49687', 2850: '49711', 2851: '49731', 2852: '49780', 2853: '49840', 2854: '49888', 2855: '50017', 2856: '5002', 2857: '50023', 2858: '50079', 2859: '50159', 2860: '50213', 2861: '50289', 2862: '50320', 2863: '50381', 2864: '50460', 2865: '50466', 2866: '50582', 2867: '50615', 2868: '50619', 2869: '50646', 2870: '50799', 2871: '50849', 2872: '50889', 2873: '50961', 2874: '50993', 2875: '50994', 2876: '51080', 2877: '51085', 2878: '51209', 2879: '51220', 2880: '51343', 2881: '51410', 2882: '51427', 2883: '51576', 2884: '51579', 2885: '51606', 2886: '51632', 2887: '5165', 2888: '51718', 2889: '51727', 2890: '51818', 2891: '5185', 2892: '51941', 2893: '51955', 2894: '52010', 2895: '52095', 2896: '52126', 2897: '5219', 2898: '52196', 2899: '52215', 2900: '52239', 2901: '52241', 2902: '52321', 2903: '52337', 2904: '52346', 2905: '52363', 2906: '52497', 2907: '52581', 2908: '52646', 2909: '5265', 2910: '52666', 2911: '52698', 2912: '5270', 2913: '52716', 2914: '52726', 2915: '52810', 2916: '52880', 2917: '52941', 2918: '52983', 2919: '52992', 2920: '53019', 2921: '53146', 2922: '53157', 2923: '53210', 2924: '53248', 2925: '53250', 2926: '53270', 2927: '53331', 2928: '5336', 2929: '53427', 2930: '53436', 2931: '53466', 2932: '53471', 2933: '53588', 2934: '53676', 2935: '53860', 2936: '539', 2937: '53956', 2938: '53969', 2939: '54073', 2940: '54095', 2941: '5413', 2942: '5415', 2943: '54236', 2944: '54340', 2945: '54384', 2946: '54459', 2947: '54475', 2948: '54491', 2949: '54550', 2950: '54630', 2951: '54760', 2952: '54815', 2953: '54824', 2954: '54868', 2955: '54921', 2956: '54996', 2957: '54998', 2958: '54999', 2959: '5500', 2960: '55084', 2961: '55103', 2962: '55118', 2963: '55131', 2964: '55195', 2965: '55319', 2966: '5534', 2967: '55401', 2968: '55421', 2969: '55424', 2970: '55474', 2971: '55478', 2972: '5548', 2973: '55558', 2974: '55678', 2975: '55737', 2976: '55833', 2977: '5597', 2978: '56059', 2979: '56231', 2980: '56276', 2981: '56292', 2982: '56300', 2983: '56345', 2984: '56374', 2985: '56382', 2986: '56400', 2987: '56504', 2988: '56522', 2989: '56560', 2990: '56660', 2991: '56674', 2992: '56708', 2993: '56788', 2994: '56866', 2995: '56873', 2996: '56882', 2997: '56908', 2998: '56978', 2999: '57134', 3000: '57157', 3001: '57230', 3002: '5724', 3003: '57253', 3004: '57277', 3005: '57293', 3006: '57389', 3007: '57403', 3008: '57516', 3009: '57546', 3010: '5768', 3011: '577', 3012: '57714', 3013: '5772', 3014: '57735', 3015: '5780', 3016: '5781', 3017: '57816', 3018: '57826', 3019: '57853', 3020: '57884', 3021: '57887', 3022: '57953', 3023: '5797', 3024: '58067', 3025: '58072', 3026: '58085', 3027: '58125', 3028: '5817', 3029: '58187', 3030: '58211', 3031: '58256', 3032: '58274', 3033: '58294', 3034: '58390', 3035: '58404', 3036: '58437', 3037: '58578', 3038: '58642', 3039: '58652', 3040: '58657', 3041: '58715', 3042: '58760', 3043: '58767', 3044: '58828', 3045: '58843', 3046: '58885', 3047: '58974', 3048: '59003', 3049: '59119', 3050: '5916', 3051: '59173', 3052: '59187', 3053: '59200', 3054: '59220', 3055: '59258', 3056: '59283', 3057: '59284', 3058: '59504', 3059: '59580', 3060: '596', 3061: '59684', 3062: '59714', 3063: '59739', 3064: '59766', 3065: '59769', 3066: '59790', 3067: '59810', 3068: '59886', 3069: '59889', 3070: '59893', 3071: '59905', 3072: '59960', 3073: '59976', 3074: '59977', 3075: '60021', 3076: '60030', 3077: '60050', 3078: '60111', 3079: '60112', 3080: '60239', 3081: '60248', 3082: '60292', 3083: '60397', 3084: '60403', 3085: '60404', 3086: '60440', 3087: '60531', 3088: '60537', 3089: '60551', 3090: '6061', 3091: '60782', 3092: '60821', 3093: '60834', 3094: '60880', 3095: '60895', 3096: '60939', 3097: '61122', 3098: '61125', 3099: '61144', 3100: '61146', 3101: '61209', 3102: '61309', 3103: '61344', 3104: '6139', 3105: '61649', 3106: '61653', 3107: '61691', 3108: '61834', 3109: '6184', 3110: '61876', 3111: '61964', 3112: '61985', 3113: '62239', 3114: '62277', 3115: '62331', 3116: '62357', 3117: '62386', 3118: '62485', 3119: '62494', 3120: '62513', 3121: '62573', 3122: '62678', 3123: '62703', 3124: '62892', 3125: '629', 3126: '62911', 3127: '62916', 3128: '62945', 3129: '63004', 3130: '63038', 3131: '63301', 3132: '63331', 3133: '63350', 3134: '63358', 3135: '6337', 3136: '63498', 3137: '63537', 3138: '63559', 3139: '63566', 3140: '63617', 3141: '63620', 3142: '63631', 3143: '6368', 3144: '63788', 3145: '63810', 3146: '6402', 3147: '64048', 3148: '64058', 3149: '64062', 3150: '64078', 3151: '64131', 3152: '64145', 3153: '64196', 3154: '64331', 3155: '64366', 3156: '64371', 3157: '64423', 3158: '64455', 3159: '64499', 3160: '64509', 3161: '64525', 3162: '64606', 3163: '64634', 3164: '64773', 3165: '64796', 3166: '64858', 3167: '64875', 3168: '64910', 3169: '64939', 3170: '65150', 3171: '65176', 3172: '6519', 3173: '65217', 3174: '65261', 3175: '65312', 3176: '65465', 3177: '65476', 3178: '65627', 3179: '65719', 3180: '65824', 3181: '65919', 3182: '66101', 3183: '66108', 3184: '66112', 3185: '66214', 3186: '66221', 3187: '66246', 3188: '663', 3189: '66441', 3190: '66452', 3191: '66563', 3192: '66571', 3193: '66576', 3194: '66620', 3195: '66648', 3196: '66664', 3197: '6674', 3198: '66783', 3199: '66792', 3200: '66881', 3201: '66899', 3202: '669', 3203: '66997', 3204: '67103', 3205: '67208', 3206: '67209', 3207: '67242', 3208: '67430', 3209: '67436', 3210: '67467', 3211: '6747', 3212: '67475', 3213: '67506', 3214: '67507', 3215: '67539', 3216: '67608', 3217: '67634', 3218: '67780', 3219: '679', 3220: '67901', 3221: '67921', 3222: '67960', 3223: '6797', 3224: '67980', 3225: '68035', 3226: '68074', 3227: '68104', 3228: '68127', 3229: '68154', 3230: '68179', 3231: '68195', 3232: '68269', 3233: '68301', 3234: '68303', 3235: '6833', 3236: '68345', 3237: '68361', 3238: '68432', 3239: '68470', 3240: '68554', 3241: '6858', 3242: '68586', 3243: '6863', 3244: '68665', 3245: '68760', 3246: '68857', 3247: '6892', 3248: '68954', 3249: '68985', 3250: '69114', 3251: '69134', 3252: '69301', 3253: '69365', 3254: '69379', 3255: '69404', 3256: '69454', 3257: '69525', 3258: '6957', 3259: '6960', 3260: '69607', 3261: '6977', 3262: '69793', 3263: '69807', 3264: '69841', 3265: '69888', 3266: '69978', 3267: '7006', 3268: '7013', 3269: '70138', 3270: '70157', 3271: '7017', 3272: '70194', 3273: '70220', 3274: '70224', 3275: '70252', 3276: '70286', 3277: '70312', 3278: '70343', 3279: '70364', 3280: '70404', 3281: '70440', 3282: '70456', 3283: '70475', 3284: '70536', 3285: '70612', 3286: '70720', 3287: '70728', 3288: '70796', 3289: '70851', 3290: '70891', 3291: '70920', 3292: '70926', 3293: '70948', 3294: '70961', 3295: '71068', 3296: '71120', 3297: '71169', 3298: '71244', 3299: '71294', 3300: '71298', 3301: '71299', 3302: '713', 3303: '71325', 3304: '71330', 3305: '71331', 3306: '71359', 3307: '71478', 3308: '71566', 3309: '71593', 3310: '71604', 3311: '7161', 3312: '71636', 3313: '71667', 3314: '71806', 3315: '71826', 3316: '7189', 3317: '71897', 3318: '71918', 3319: '71922', 3320: '71955', 3321: '71978', 3322: '71987', 3323: '72101', 3324: '72170', 3325: '72288', 3326: '72411', 3327: '72441', 3328: '72485', 3329: '72498', 3330: '72566', 3331: '72587', 3332: '72678', 3333: '72732', 3334: '72755', 3335: '72756', 3336: '72761', 3337: '72788', 3338: '72877', 3339: '72936', 3340: '72941', 3341: '72966', 3342: '73086', 3343: '73142', 3344: '73185', 3345: '73326', 3346: '73343', 3347: '73380', 3348: '73399', 3349: '73419', 3350: '73422', 3351: '73429', 3352: '73465', 3353: '73494', 3354: '73510', 3355: '73597', 3356: '7366', 3357: '73670', 3358: '73798', 3359: '7380', 3360: '73879', 3361: '73884', 3362: '7389', 3363: '7392', 3364: '73923', 3365: '73971', 3366: '73991', 3367: '73992', 3368: '74057', 3369: '74066', 3370: '7408', 3371: '74124', 3372: '74142', 3373: '74162', 3374: '74270', 3375: '74351', 3376: '74370', 3377: '74379', 3378: '74486', 3379: '74561', 3380: '74584', 3381: '74670', 3382: '74685', 3383: '7472', 3384: '74802', 3385: '74865', 3386: '74901', 3387: '74932', 3388: '74953', 3389: '75069', 3390: '75111', 3391: '75142', 3392: '75231', 3393: '75240', 3394: '75258', 3395: '75377', 3396: '75424', 3397: '75464', 3398: '7558', 3399: '75592', 3400: '75770', 3401: '75836', 3402: '7600', 3403: '7603', 3404: '76054', 3405: '76058', 3406: '76064', 3407: '76078', 3408: '76153', 3409: '76219', 3410: '76260', 3411: '76276', 3412: '7629', 3413: '76428', 3414: '76502', 3415: '76540', 3416: '76547', 3417: '76617', 3418: '76890', 3419: '76898', 3420: '76920', 3421: '76972', 3422: '77036', 3423: '77055', 3424: '77094', 3425: '7712', 3426: '77209', 3427: '77251', 3428: '77273', 3429: '77351', 3430: '7740', 3431: '77400', 3432: '77434', 3433: '77544', 3434: '77578', 3435: '77586', 3436: '77602', 3437: '77676', 3438: '77697', 3439: '77706', 3440: '77738', 3441: '77740', 3442: '77828', 3443: '77963', 3444: '77975', 3445: '78017', 3446: '7802', 3447: '78126', 3448: '78127', 3449: '78161', 3450: '78165', 3451: '78250', 3452: '7839', 3453: '78433', 3454: '7852', 3455: '7853', 3456: '78560', 3457: '78586', 3458: '78628', 3459: '78684', 3460: '78695', 3461: '7874', 3462: '78742', 3463: '79061', 3464: '79067', 3465: '79119', 3466: '79130', 3467: '79246', 3468: '79249', 3469: '79348', 3470: '79425', 3471: '79470', 3472: '79537', 3473: '79765', 3474: '79783', 3475: '79793', 3476: '7989', 3477: '79941', 3478: '79960', 3479: '80132', 3480: '80183', 3481: '80214', 3482: '8030', 3483: '80411', 3484: '80430', 3485: '80432', 3486: '80529', 3487: '8055', 3488: '80579', 3489: '80592', 3490: '80598', 3491: '80788', 3492: '80812', 3493: '80929', 3494: '80945', 3495: '80980', 3496: '80997', 3497: '81031', 3498: '81039', 3499: '81040', 3500: '81046', 3501: '81048', 3502: '8106', 3503: '81142', 3504: '81154', 3505: '81195', 3506: '81388', 3507: '81481', 3508: '81488', 3509: '81519', 3510: '81530', 3511: '81623', 3512: '8166', 3513: '81674', 3514: '817', 3515: '81755', 3516: '81831', 3517: '81871', 3518: '8189', 3519: '81910', 3520: '82020', 3521: '82051', 3522: '8211', 3523: '82156', 3524: '82249', 3525: '82267', 3526: '82273', 3527: '82276', 3528: '82402', 3529: '82489', 3530: '82535', 3531: '82572', 3532: '82577', 3533: '82637', 3534: '82658', 3535: '82696', 3536: '82764', 3537: '82789', 3538: '82883', 3539: '82948', 3540: '82992', 3541: '83157', 3542: '83163', 3543: '83236', 3544: '8333', 3545: '83368', 3546: '83422', 3547: '83453', 3548: '83509', 3549: '8353', 3550: '83595', 3551: '83602', 3552: '83656', 3553: '83788', 3554: '83795', 3555: '83826', 3556: '83850', 3557: '83884', 3558: '83912', 3559: '8394', 3560: '83958', 3561: '8409', 3562: '84115', 3563: '84136', 3564: '84212', 3565: '84432', 3566: '84522', 3567: '84543', 3568: '84581', 3569: '84699', 3570: '84763', 3571: '84809', 3572: '84824', 3573: '84828', 3574: '84855', 3575: '84865', 3576: '85087', 3577: '85174', 3578: '85205', 3579: '85261', 3580: '85284', 3581: '85327', 3582: '85384', 3583: '85508', 3584: '85516', 3585: '85579', 3586: '85634', 3587: '85670', 3588: '85753', 3589: '85876', 3590: '85885', 3591: '85920', 3592: '85931', 3593: '85966', 3594: '8603', 3595: '86055', 3596: '86113', 3597: '86122', 3598: '86140', 3599: '86205', 3600: '86221', 3601: '86222', 3602: '86266', 3603: '86307', 3604: '86360', 3605: '86371', 3606: '86379', 3607: '86449', 3608: '8647', 3609: '86542', 3610: '86593', 3611: '86798', 3612: '8680', 3613: '86803', 3614: '86853', 3615: '86926', 3616: '86953', 3617: '86977', 3618: '8698', 3619: '87053', 3620: '87090', 3621: '87154', 3622: '87213', 3623: '87304', 3624: '87382', 3625: '87452', 3626: '87455', 3627: '87572', 3628: '8766', 3629: '87786', 3630: '87849', 3631: '87866', 3632: '88012', 3633: '88128', 3634: '88183', 3635: '88301', 3636: '88309', 3637: '88361', 3638: '88411', 3639: '8856', 3640: '88686', 3641: '8869', 3642: '88805', 3643: '88852', 3644: '88863', 3645: '88894', 3646: '88983', 3647: '89047', 3648: '89371', 3649: '89558', 3650: '89609', 3651: '89656', 3652: '897', 3653: '89702', 3654: '89745', 3655: '89776', 3656: '89897', 3657: '89914', 3658: '89933', 3659: '89940', 3660: '89945', 3661: '89992', 3662: '9000', 3663: '90032', 3664: '90050', 3665: '90058', 3666: '90059', 3667: '90106', 3668: '9017', 3669: '90189', 3670: '90315', 3671: '90352', 3672: '90405', 3673: '90683', 3674: '90689', 3675: '90778', 3676: '90788', 3677: '90825', 3678: '90829', 3679: '90937', 3680: '90989', 3681: '91147', 3682: '91298', 3683: '9131', 3684: '91349', 3685: '91381', 3686: '9147', 3687: '91491', 3688: '91532', 3689: '91653', 3690: '91668', 3691: '91702', 3692: '91738', 3693: '91873', 3694: '91894', 3695: '91903', 3696: '9196', 3697: '91973', 3698: '91977', 3699: '91991', 3700: '91994', 3701: '92025', 3702: '92028', 3703: '92063', 3704: '92080', 3705: '92109', 3706: '92141', 3707: '92163', 3708: '92176', 3709: '92190', 3710: '92221', 3711: '92236', 3712: '92320', 3713: '92388', 3714: '92400', 3715: '92444', 3716: '92473', 3717: '92480', 3718: '9249', 3719: '92506', 3720: '92532', 3721: '9257', 3722: '92587', 3723: '92592', 3724: '92690', 3725: '92707', 3726: '92712', 3727: '92718', 3728: '92830', 3729: '9290', 3730: '92920', 3731: '92946', 3732: '93011', 3733: '93038', 3734: '93044', 3735: '93060', 3736: '93170', 3737: '93222', 3738: '93329', 3739: '9333', 3740: '93340', 3741: '93366', 3742: '93374', 3743: '93389', 3744: '93402', 3745: '93448', 3746: '93518', 3747: '93533', 3748: '93605', 3749: '93750', 3750: '93778', 3751: '93871', 3752: '9388', 3753: '93881', 3754: '93882', 3755: '93948', 3756: '9397', 3757: '93977', 3758: '94333', 3759: '94414', 3760: '94451', 3761: '94486', 3762: '94493', 3763: '94516', 3764: '94524', 3765: '94531', 3766: '94653', 3767: '94670', 3768: '94728', 3769: '94729', 3770: '94759', 3771: '94840', 3772: '94848', 3773: '94858', 3774: '949', 3775: '94901', 3776: '94960', 3777: '95070', 3778: '95079', 3779: '95105', 3780: '95198', 3781: '95238', 3782: '95240', 3783: '95329', 3784: '95408', 3785: '95446', 3786: '95492', 3787: '95508', 3788: '95538', 3789: '95591', 3790: '95594', 3791: '95621', 3792: '95642', 3793: '95650', 3794: '95653', 3795: '95679', 3796: '95770', 3797: '95790', 3798: '95816', 3799: '95845', 3800: '95884', 3801: '95960', 3802: '96071', 3803: '96116', 3804: '96127', 3805: '96128', 3806: '96220', 3807: '96257', 3808: '96314', 3809: '9633', 3810: '96345', 3811: '96349', 3812: '96421', 3813: '96485', 3814: '96612', 3815: '96627', 3816: '96705', 3817: '96730', 3818: '96751', 3819: '96790', 3820: '96816', 3821: '96901', 3822: '96927', 3823: '96956', 3824: '97000', 3825: '97132', 3826: '97148', 3827: '97164', 3828: '97172', 3829: '97317', 3830: '97318', 3831: '97368', 3832: '9738', 3833: '97424', 3834: '97456', 3835: '97485', 3836: '97541', 3837: '97582', 3838: '97590', 3839: '97599', 3840: '97681', 3841: '97692', 3842: '9771', 3843: '97732', 3844: '9776', 3845: '97771', 3846: '97876', 3847: '97921', 3848: '97999', 3849: '98002', 3850: '9810', 3851: '98177', 3852: '98188', 3853: '98194', 3854: '98212', 3855: '98235', 3856: '9830', 3857: '98305', 3858: '98516', 3859: '98521', 3860: '986', 3861: '98635', 3862: '98650', 3863: '98768', 3864: '98838', 3865: '98902', 3866: '98907', 3867: '98908', 3868: '98919', 3869: '98943', 3870: '99', 3871: '99006', 3872: '9908', 3873: '99085', 3874: '9913', 3875: '99139', 3876: '99157', 3877: '99177', 3878: '99258', 3879: '99259', 3880: '99309', 3881: '99310', 3882: '99370', 3883: '99395', 3884: '99495', 3885: '99536', 3886: '99598', 3887: '99652', 3888: '99666', 3889: '99686', 3890: '99717', 3891: '99854', 3892: '99862', 3893: '99901', 3894: '99934', 3895: '99983'}\n"
     ]
    }
   ],
   "source": [
    "indices_to_class_labels_dict = {value : key for key, value in train_generator.class_indices.items()}\n",
    "print(len(indices_to_class_labels_dict))\n",
    "print(indices_to_class_labels_dict)\n",
    "with open(\"group7_indices_to_class_labels_dict.json\", \"wb\") as pickle_file:\n",
    "    pickle.dump(indices_to_class_labels_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks and Fitting Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFolder = 'checkpoints_group7'\n",
    "if not os.path.exists(outputFolder):\n",
    "    os.makedirs(outputFolder)\n",
    "checkpoint_filepath=outputFolder+\"/model-{epoch:02d}-{val_acc:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath, monitor='val_acc', verbose=1, mode='max',\n",
    "    save_best_only=True, save_weights_only=True,\n",
    "    save_frequency='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-4c3d897e268e>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 7.2705 - acc: 0.0275\n",
      "Epoch 00001: val_acc improved from -inf to 0.16853, saving model to checkpoints_group7/model-01-0.17.hdf5\n",
      "2196/2196 [==============================] - 682s 310ms/step - loss: 7.2705 - acc: 0.0275 - val_loss: 5.1112 - val_acc: 0.1685\n",
      "Epoch 2/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 4.7077 - acc: 0.2047\n",
      "Epoch 00002: val_acc improved from 0.16853 to 0.27463, saving model to checkpoints_group7/model-02-0.27.hdf5\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 4.7077 - acc: 0.2047 - val_loss: 4.3596 - val_acc: 0.2746\n",
      "Epoch 3/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 3.6498 - acc: 0.3335\n",
      "Epoch 00003: val_acc improved from 0.27463 to 0.38563, saving model to checkpoints_group7/model-03-0.39.hdf5\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 3.6498 - acc: 0.3335 - val_loss: 3.4830 - val_acc: 0.3856\n",
      "Epoch 4/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 3.0596 - acc: 0.4164\n",
      "Epoch 00004: val_acc improved from 0.38563 to 0.44134, saving model to checkpoints_group7/model-04-0.44.hdf5\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 3.0596 - acc: 0.4164 - val_loss: 3.1579 - val_acc: 0.4413\n",
      "Epoch 5/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 2.6363 - acc: 0.4786\n",
      "Epoch 00005: val_acc improved from 0.44134 to 0.47317, saving model to checkpoints_group7/model-05-0.47.hdf5\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 2.6363 - acc: 0.4786 - val_loss: 3.0182 - val_acc: 0.4732\n",
      "Epoch 6/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 2.3095 - acc: 0.5295\n",
      "Epoch 00006: val_acc did not improve from 0.47317\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 2.3095 - acc: 0.5295 - val_loss: 3.3172 - val_acc: 0.4285\n",
      "Epoch 7/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 2.0500 - acc: 0.5712\n",
      "Epoch 00007: val_acc improved from 0.47317 to 0.49506, saving model to checkpoints_group7/model-07-0.50.hdf5\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 2.0500 - acc: 0.5712 - val_loss: 2.9713 - val_acc: 0.4951\n",
      "Epoch 8/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 1.8277 - acc: 0.6073\n",
      "Epoch 00008: val_acc improved from 0.49506 to 0.53157, saving model to checkpoints_group7/model-08-0.53.hdf5\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 1.8277 - acc: 0.6073 - val_loss: 2.7347 - val_acc: 0.5316\n",
      "Epoch 9/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 1.6377 - acc: 0.6404\n",
      "Epoch 00009: val_acc did not improve from 0.53157\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 1.6377 - acc: 0.6404 - val_loss: 2.9053 - val_acc: 0.5136\n",
      "Epoch 10/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 1.4819 - acc: 0.6681\n",
      "Epoch 00010: val_acc improved from 0.53157 to 0.55109, saving model to checkpoints_group7/model-10-0.55.hdf5\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 1.4819 - acc: 0.6681 - val_loss: 2.6509 - val_acc: 0.5511\n",
      "Epoch 11/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 1.3349 - acc: 0.6941\n",
      "Epoch 00011: val_acc did not improve from 0.55109\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 1.3349 - acc: 0.6941 - val_loss: 2.9380 - val_acc: 0.5333\n",
      "Epoch 12/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 1.2103 - acc: 0.7162\n",
      "Epoch 00012: val_acc did not improve from 0.55109\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 1.2103 - acc: 0.7162 - val_loss: 3.6879 - val_acc: 0.4610\n",
      "Epoch 13/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 1.1009 - acc: 0.7387\n",
      "Epoch 00013: val_acc improved from 0.55109 to 0.57129, saving model to checkpoints_group7/model-13-0.57.hdf5\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 1.1009 - acc: 0.7387 - val_loss: 2.7309 - val_acc: 0.5713\n",
      "Epoch 14/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 1.0049 - acc: 0.7563\n",
      "Epoch 00014: val_acc improved from 0.57129 to 0.57586, saving model to checkpoints_group7/model-14-0.58.hdf5\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 1.0049 - acc: 0.7563 - val_loss: 2.6242 - val_acc: 0.5759\n",
      "Epoch 15/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.9135 - acc: 0.7771\n",
      "Epoch 00015: val_acc did not improve from 0.57586\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.9135 - acc: 0.7771 - val_loss: 2.7327 - val_acc: 0.5740\n",
      "Epoch 16/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.8333 - acc: 0.7917\n",
      "Epoch 00016: val_acc improved from 0.57586 to 0.57954, saving model to checkpoints_group7/model-16-0.58.hdf5\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.8333 - acc: 0.7917 - val_loss: 2.7061 - val_acc: 0.5795\n",
      "Epoch 17/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.7684 - acc: 0.8048\n",
      "Epoch 00017: val_acc did not improve from 0.57954\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.7684 - acc: 0.8048 - val_loss: 2.8348 - val_acc: 0.5722\n",
      "Epoch 18/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.7073 - acc: 0.8182\n",
      "Epoch 00018: val_acc improved from 0.57954 to 0.58729, saving model to checkpoints_group7/model-18-0.59.hdf5\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.7073 - acc: 0.8182 - val_loss: 2.7932 - val_acc: 0.5873\n",
      "Epoch 19/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.6515 - acc: 0.8309\n",
      "Epoch 00019: val_acc did not improve from 0.58729\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.6515 - acc: 0.8309 - val_loss: 3.1303 - val_acc: 0.5474\n",
      "Epoch 20/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.5996 - acc: 0.8428\n",
      "Epoch 00020: val_acc did not improve from 0.58729\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.5996 - acc: 0.8428 - val_loss: 2.9342 - val_acc: 0.5589\n",
      "Epoch 21/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.5586 - acc: 0.8518\n",
      "Epoch 00021: val_acc did not improve from 0.58729\n",
      "2196/2196 [==============================] - 686s 312ms/step - loss: 0.5586 - acc: 0.8518 - val_loss: 3.2413 - val_acc: 0.5406\n",
      "Epoch 22/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.5157 - acc: 0.8629\n",
      "Epoch 00022: val_acc did not improve from 0.58729\n",
      "2196/2196 [==============================] - 686s 313ms/step - loss: 0.5157 - acc: 0.8629 - val_loss: 3.5980 - val_acc: 0.5109\n",
      "Epoch 23/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.4868 - acc: 0.8696\n",
      "Epoch 00023: val_acc did not improve from 0.58729\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.4868 - acc: 0.8696 - val_loss: 3.4166 - val_acc: 0.5377\n",
      "Epoch 24/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.4575 - acc: 0.8770\n",
      "Epoch 00024: val_acc improved from 0.58729 to 0.59103, saving model to checkpoints_group7/model-24-0.59.hdf5\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.4575 - acc: 0.8770 - val_loss: 2.9787 - val_acc: 0.5910\n",
      "Epoch 25/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.4284 - acc: 0.8837\n",
      "Epoch 00025: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.4284 - acc: 0.8837 - val_loss: 3.3081 - val_acc: 0.5588\n",
      "Epoch 26/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.4009 - acc: 0.8906\n",
      "Epoch 00026: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.4009 - acc: 0.8906 - val_loss: 3.0612 - val_acc: 0.5812\n",
      "Epoch 27/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.3866 - acc: 0.8941\n",
      "Epoch 00027: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.3866 - acc: 0.8941 - val_loss: 3.1564 - val_acc: 0.5701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.3605 - acc: 0.9008\n",
      "Epoch 00028: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.3605 - acc: 0.9008 - val_loss: 3.0683 - val_acc: 0.5885\n",
      "Epoch 29/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.3432 - acc: 0.9062\n",
      "Epoch 00029: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.3432 - acc: 0.9062 - val_loss: 3.5400 - val_acc: 0.5681\n",
      "Epoch 30/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.3268 - acc: 0.9093\n",
      "Epoch 00030: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.3268 - acc: 0.9093 - val_loss: 3.5152 - val_acc: 0.5750\n",
      "Epoch 31/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.3145 - acc: 0.9129\n",
      "Epoch 00031: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.3145 - acc: 0.9129 - val_loss: 3.4440 - val_acc: 0.5526\n",
      "Epoch 32/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2961 - acc: 0.9174\n",
      "Epoch 00032: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.2961 - acc: 0.9174 - val_loss: 3.3253 - val_acc: 0.5825\n",
      "Epoch 33/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2878 - acc: 0.9199\n",
      "Epoch 00033: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 687s 313ms/step - loss: 0.2878 - acc: 0.9199 - val_loss: 3.5152 - val_acc: 0.5633\n",
      "Epoch 34/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2755 - acc: 0.9232\n",
      "Epoch 00034: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.2755 - acc: 0.9232 - val_loss: 3.2603 - val_acc: 0.5905\n",
      "Epoch 35/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2613 - acc: 0.9276\n",
      "Epoch 00035: val_acc did not improve from 0.59103\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 0.2613 - acc: 0.9276 - val_loss: 3.5341 - val_acc: 0.5721\n",
      "Epoch 36/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2552 - acc: 0.9287\n",
      "Epoch 00036: val_acc improved from 0.59103 to 0.61574, saving model to checkpoints_group7/model-36-0.62.hdf5\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 0.2552 - acc: 0.9287 - val_loss: 3.0856 - val_acc: 0.6157\n",
      "Epoch 37/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2522 - acc: 0.9289\n",
      "Epoch 00037: val_acc did not improve from 0.61574\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.2522 - acc: 0.9289 - val_loss: 3.4698 - val_acc: 0.5794\n",
      "Epoch 38/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2370 - acc: 0.9335\n",
      "Epoch 00038: val_acc did not improve from 0.61574\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.2370 - acc: 0.9335 - val_loss: 3.1252 - val_acc: 0.6133\n",
      "Epoch 39/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2312 - acc: 0.9354\n",
      "Epoch 00039: val_acc did not improve from 0.61574\n",
      "2196/2196 [==============================] - 691s 314ms/step - loss: 0.2312 - acc: 0.9354 - val_loss: 3.4419 - val_acc: 0.6030\n",
      "Epoch 40/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2192 - acc: 0.9386\n",
      "Epoch 00040: val_acc did not improve from 0.61574\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.2192 - acc: 0.9386 - val_loss: 3.2795 - val_acc: 0.5994\n",
      "Epoch 41/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2189 - acc: 0.9387\n",
      "Epoch 00041: val_acc improved from 0.61574 to 0.61720, saving model to checkpoints_group7/model-41-0.62.hdf5\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.2189 - acc: 0.9387 - val_loss: 3.2747 - val_acc: 0.6172\n",
      "Epoch 42/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2091 - acc: 0.9413\n",
      "Epoch 00042: val_acc did not improve from 0.61720\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.2091 - acc: 0.9413 - val_loss: 3.7143 - val_acc: 0.5735\n",
      "Epoch 43/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.2046 - acc: 0.9426\n",
      "Epoch 00043: val_acc did not improve from 0.61720\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.2046 - acc: 0.9426 - val_loss: 3.9332 - val_acc: 0.5559\n",
      "Epoch 44/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1991 - acc: 0.9440\n",
      "Epoch 00044: val_acc improved from 0.61720 to 0.62494, saving model to checkpoints_group7/model-44-0.62.hdf5\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.1991 - acc: 0.9440 - val_loss: 3.1291 - val_acc: 0.6249\n",
      "Epoch 45/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1905 - acc: 0.9460\n",
      "Epoch 00045: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 0.1905 - acc: 0.9460 - val_loss: 3.6152 - val_acc: 0.5797\n",
      "Epoch 46/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1865 - acc: 0.9478\n",
      "Epoch 00046: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 0.1865 - acc: 0.9478 - val_loss: 3.7481 - val_acc: 0.5479\n",
      "Epoch 47/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1825 - acc: 0.9485\n",
      "Epoch 00047: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 0.1825 - acc: 0.9485 - val_loss: 3.7450 - val_acc: 0.5621\n",
      "Epoch 48/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1780 - acc: 0.9496\n",
      "Epoch 00048: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.1780 - acc: 0.9496 - val_loss: 4.1746 - val_acc: 0.5378\n",
      "Epoch 49/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1736 - acc: 0.9507\n",
      "Epoch 00049: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.1736 - acc: 0.9507 - val_loss: 4.3781 - val_acc: 0.5293\n",
      "Epoch 50/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1710 - acc: 0.9514\n",
      "Epoch 00050: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.1710 - acc: 0.9514 - val_loss: 3.5326 - val_acc: 0.6084\n",
      "Epoch 51/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1643 - acc: 0.9539\n",
      "Epoch 00051: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.1643 - acc: 0.9539 - val_loss: 3.6908 - val_acc: 0.6060\n",
      "Epoch 52/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1627 - acc: 0.9543\n",
      "Epoch 00052: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.1627 - acc: 0.9543 - val_loss: 3.5832 - val_acc: 0.6035\n",
      "Epoch 53/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1579 - acc: 0.9554\n",
      "Epoch 00053: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.1579 - acc: 0.9554 - val_loss: 3.8158 - val_acc: 0.5717\n",
      "Epoch 54/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1557 - acc: 0.9556\n",
      "Epoch 00054: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 687s 313ms/step - loss: 0.1557 - acc: 0.9556 - val_loss: 3.6272 - val_acc: 0.5934\n",
      "Epoch 55/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1517 - acc: 0.9571\n",
      "Epoch 00055: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 687s 313ms/step - loss: 0.1517 - acc: 0.9571 - val_loss: 3.3931 - val_acc: 0.6109\n",
      "Epoch 56/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1486 - acc: 0.9581\n",
      "Epoch 00056: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 686s 313ms/step - loss: 0.1486 - acc: 0.9581 - val_loss: 3.8338 - val_acc: 0.5915\n",
      "Epoch 57/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1446 - acc: 0.9582\n",
      "Epoch 00057: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 0.1446 - acc: 0.9582 - val_loss: 3.6839 - val_acc: 0.6137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1438 - acc: 0.9591\n",
      "Epoch 00058: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 691s 315ms/step - loss: 0.1438 - acc: 0.9591 - val_loss: 3.5179 - val_acc: 0.6070\n",
      "Epoch 59/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1394 - acc: 0.9600\n",
      "Epoch 00059: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.1394 - acc: 0.9600 - val_loss: 3.6242 - val_acc: 0.6097\n",
      "Epoch 60/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1351 - acc: 0.9616\n",
      "Epoch 00060: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.1351 - acc: 0.9616 - val_loss: 4.2601 - val_acc: 0.5332\n",
      "Epoch 61/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1356 - acc: 0.9610\n",
      "Epoch 00061: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 688s 313ms/step - loss: 0.1356 - acc: 0.9610 - val_loss: 4.0586 - val_acc: 0.5559\n",
      "Epoch 62/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1303 - acc: 0.9631\n",
      "Epoch 00062: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.1303 - acc: 0.9631 - val_loss: 3.8623 - val_acc: 0.5953\n",
      "Epoch 63/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1299 - acc: 0.9630\n",
      "Epoch 00063: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 686s 312ms/step - loss: 0.1299 - acc: 0.9630 - val_loss: 3.6317 - val_acc: 0.6027\n",
      "Epoch 64/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1294 - acc: 0.9629\n",
      "Epoch 00064: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.1294 - acc: 0.9629 - val_loss: 4.2672 - val_acc: 0.5596\n",
      "Epoch 65/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1231 - acc: 0.9648\n",
      "Epoch 00065: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.1231 - acc: 0.9648 - val_loss: 3.6229 - val_acc: 0.6239\n",
      "Epoch 66/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1229 - acc: 0.9648\n",
      "Epoch 00066: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1229 - acc: 0.9648 - val_loss: 3.7778 - val_acc: 0.6096\n",
      "Epoch 67/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1210 - acc: 0.9653\n",
      "Epoch 00067: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1210 - acc: 0.9653 - val_loss: 3.7743 - val_acc: 0.6197\n",
      "Epoch 68/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1230 - acc: 0.9649\n",
      "Epoch 00068: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1230 - acc: 0.9649 - val_loss: 3.6184 - val_acc: 0.6031\n",
      "Epoch 69/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1150 - acc: 0.9668\n",
      "Epoch 00069: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 687s 313ms/step - loss: 0.1150 - acc: 0.9668 - val_loss: 3.9243 - val_acc: 0.5973\n",
      "Epoch 70/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1182 - acc: 0.9661\n",
      "Epoch 00070: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 689s 314ms/step - loss: 0.1182 - acc: 0.9661 - val_loss: 4.0181 - val_acc: 0.5898\n",
      "Epoch 71/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1160 - acc: 0.9665\n",
      "Epoch 00071: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 690s 314ms/step - loss: 0.1160 - acc: 0.9665 - val_loss: 4.7011 - val_acc: 0.5299\n",
      "Epoch 72/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1108 - acc: 0.9682\n",
      "Epoch 00072: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 687s 313ms/step - loss: 0.1108 - acc: 0.9682 - val_loss: 4.1242 - val_acc: 0.5878\n",
      "Epoch 73/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1110 - acc: 0.9683\n",
      "Epoch 00073: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.1110 - acc: 0.9683 - val_loss: 4.4677 - val_acc: 0.5444\n",
      "Epoch 74/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1113 - acc: 0.9677\n",
      "Epoch 00074: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.1113 - acc: 0.9677 - val_loss: 4.3409 - val_acc: 0.5580\n",
      "Epoch 75/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1066 - acc: 0.9692\n",
      "Epoch 00075: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.1066 - acc: 0.9692 - val_loss: 4.0934 - val_acc: 0.5848\n",
      "Epoch 76/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1063 - acc: 0.9692\n",
      "Epoch 00076: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 685s 312ms/step - loss: 0.1063 - acc: 0.9692 - val_loss: 4.0045 - val_acc: 0.5876\n",
      "Epoch 77/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1054 - acc: 0.9696\n",
      "Epoch 00077: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1054 - acc: 0.9696 - val_loss: 4.1107 - val_acc: 0.5955\n",
      "Epoch 78/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1035 - acc: 0.9702\n",
      "Epoch 00078: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1035 - acc: 0.9702 - val_loss: 4.0298 - val_acc: 0.5905\n",
      "Epoch 79/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1034 - acc: 0.9701\n",
      "Epoch 00079: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1034 - acc: 0.9701 - val_loss: 3.9191 - val_acc: 0.5983\n",
      "Epoch 80/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1014 - acc: 0.9704\n",
      "Epoch 00080: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1014 - acc: 0.9704 - val_loss: 3.9462 - val_acc: 0.6064\n",
      "Epoch 81/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1017 - acc: 0.9707\n",
      "Epoch 00081: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1017 - acc: 0.9707 - val_loss: 3.6904 - val_acc: 0.6100\n",
      "Epoch 82/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0962 - acc: 0.9717\n",
      "Epoch 00082: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.0962 - acc: 0.9717 - val_loss: 3.6874 - val_acc: 0.6237\n",
      "Epoch 83/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.1001 - acc: 0.9710\n",
      "Epoch 00083: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.1001 - acc: 0.9710 - val_loss: 3.8488 - val_acc: 0.6059\n",
      "Epoch 84/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0949 - acc: 0.9730\n",
      "Epoch 00084: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0949 - acc: 0.9730 - val_loss: 3.9134 - val_acc: 0.6175\n",
      "Epoch 85/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0958 - acc: 0.9724\n",
      "Epoch 00085: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0958 - acc: 0.9724 - val_loss: 4.0885 - val_acc: 0.5697\n",
      "Epoch 86/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0938 - acc: 0.9729\n",
      "Epoch 00086: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.0938 - acc: 0.9729 - val_loss: 4.3832 - val_acc: 0.5675\n",
      "Epoch 87/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0931 - acc: 0.9733\n",
      "Epoch 00087: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.0931 - acc: 0.9733 - val_loss: 3.8979 - val_acc: 0.6041\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0921 - acc: 0.9731\n",
      "Epoch 00088: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 312ms/step - loss: 0.0921 - acc: 0.9731 - val_loss: 3.8815 - val_acc: 0.6057\n",
      "Epoch 89/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0924 - acc: 0.9735\n",
      "Epoch 00089: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0924 - acc: 0.9735 - val_loss: 4.2569 - val_acc: 0.5583\n",
      "Epoch 90/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0897 - acc: 0.9739\n",
      "Epoch 00090: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0897 - acc: 0.9739 - val_loss: 4.2748 - val_acc: 0.5899\n",
      "Epoch 91/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0893 - acc: 0.9742\n",
      "Epoch 00091: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0893 - acc: 0.9742 - val_loss: 3.8500 - val_acc: 0.6075\n",
      "Epoch 92/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0864 - acc: 0.9751\n",
      "Epoch 00092: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0864 - acc: 0.9751 - val_loss: 4.0570 - val_acc: 0.6058\n",
      "Epoch 93/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0869 - acc: 0.9749\n",
      "Epoch 00093: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.0869 - acc: 0.9749 - val_loss: 4.3028 - val_acc: 0.5865\n",
      "Epoch 94/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0855 - acc: 0.9753\n",
      "Epoch 00094: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0855 - acc: 0.9753 - val_loss: 4.1942 - val_acc: 0.6039\n",
      "Epoch 95/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0836 - acc: 0.9759\n",
      "Epoch 00095: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0836 - acc: 0.9759 - val_loss: 3.7400 - val_acc: 0.6195\n",
      "Epoch 96/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0839 - acc: 0.9760\n",
      "Epoch 00096: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0839 - acc: 0.9760 - val_loss: 4.4293 - val_acc: 0.5834\n",
      "Epoch 97/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0848 - acc: 0.9749\n",
      "Epoch 00097: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 683s 311ms/step - loss: 0.0848 - acc: 0.9749 - val_loss: 4.0255 - val_acc: 0.6070\n",
      "Epoch 98/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0817 - acc: 0.9762\n",
      "Epoch 00098: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0817 - acc: 0.9762 - val_loss: 4.1058 - val_acc: 0.6025\n",
      "Epoch 99/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0800 - acc: 0.9768\n",
      "Epoch 00099: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0800 - acc: 0.9768 - val_loss: 3.9324 - val_acc: 0.5909\n",
      "Epoch 100/100\n",
      "2196/2196 [==============================] - ETA: 0s - loss: 0.0817 - acc: 0.9764\n",
      "Epoch 00100: val_acc did not improve from 0.62494\n",
      "2196/2196 [==============================] - 684s 311ms/step - loss: 0.0817 - acc: 0.9764 - val_loss: 4.1776 - val_acc: 0.6060\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch = train_generator.n // batch_size,\n",
    "    validation_data = valid_generator,\n",
    "    validation_steps = valid_generator.n // batch_size,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    epochs = 100,\n",
    "    workers = 8,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training accuracy')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsedrfgdffg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
