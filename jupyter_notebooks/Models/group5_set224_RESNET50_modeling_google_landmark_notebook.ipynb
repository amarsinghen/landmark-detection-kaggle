{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50_MODEL=tf.keras.applications.ResNet50(input_shape=(224,224,3),\n",
    "                                               include_top=False,\n",
    "                                               weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ResNet50_MODEL.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv2_block1_0_bn'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNet50_MODEL.layers[15].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in ResNet50_MODEL.layers:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ResNet50_MODEL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "l1_factor = 0.0001\n",
    "l2_factor = 0.001\n",
    "dropout = 0.5\n",
    "learning_rate = 0.0001\n",
    "target_size_image_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "Dropout_Regularization1 (Dro (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4908)              10056492  \n",
      "=================================================================\n",
      "Total params: 33,644,204\n",
      "Trainable params: 33,591,084\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "                                  ResNet50_MODEL,\n",
    "                                  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                  tf.keras.layers.Dropout(dropout, name='Dropout_Regularization1'),\n",
    "                                  tf.keras.layers.Dense(4908, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "train_data_dir = '../datasets/group5_set_224/set_224/train/'\n",
    "valid_data_dir = '../datasets/group5_set_224/set_224/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.25,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 107289 images belonging to 4908 classes.\n"
     ]
    }
   ],
   "source": [
    "#flow training images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=target_size_image_shape,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29448 images belonging to 4908 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=target_size_image_shape,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Labels in  Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908\n",
      "{0: '100015', 1: '100039', 2: '100084', 3: '100119', 4: '100125', 5: '100152', 6: '100166', 7: '100194', 8: '100349', 9: '100437', 10: '100441', 11: '100458', 12: '100463', 13: '100541', 14: '100582', 15: '100608', 16: '100638', 17: '100640', 18: '100655', 19: '100663', 20: '100702', 21: '10073', 22: '100737', 23: '100774', 24: '100788', 25: '100802', 26: '100805', 27: '100849', 28: '10087', 29: '100871', 30: '101', 31: '101007', 32: '101014', 33: '101032', 34: '101044', 35: '101051', 36: '101075', 37: '10111', 38: '101165', 39: '101204', 40: '10121', 41: '101224', 42: '101242', 43: '101247', 44: '10128', 45: '101379', 46: '10139', 47: '101393', 48: '101479', 49: '101488', 50: '10149', 51: '101568', 52: '101618', 53: '101621', 54: '101706', 55: '101716', 56: '101732', 57: '101759', 58: '101963', 59: '101979', 60: '102052', 61: '102072', 62: '102134', 63: '102205', 64: '102262', 65: '102292', 66: '102301', 67: '102356', 68: '102400', 69: '102420', 70: '102489', 71: '102499', 72: '102520', 73: '10259', 74: '102729', 75: '102750', 76: '102760', 77: '102810', 78: '102906', 79: '102922', 80: '103000', 81: '103087', 82: '103123', 83: '103131', 84: '103135', 85: '103196', 86: '1032', 87: '103216', 88: '103223', 89: '103254', 90: '10339', 91: '103442', 92: '103461', 93: '103478', 94: '103490', 95: '103496', 96: '103502', 97: '103571', 98: '103591', 99: '103684', 100: '103711', 101: '103736', 102: '103759', 103: '103790', 104: '103794', 105: '103813', 106: '103829', 107: '103846', 108: '103896', 109: '10396', 110: '103967', 111: '103980', 112: '104013', 113: '104041', 114: '104085', 115: '104134', 116: '104277', 117: '104320', 118: '104343', 119: '104359', 120: '104432', 121: '104439', 122: '104447', 123: '104477', 124: '104496', 125: '104514', 126: '104542', 127: '10455', 128: '104553', 129: '104599', 130: '104649', 131: '104670', 132: '104686', 133: '10471', 134: '104845', 135: '104873', 136: '104907', 137: '104950', 138: '1050', 139: '105060', 140: '105107', 141: '105152', 142: '105273', 143: '105280', 144: '105314', 145: '105325', 146: '105343', 147: '105359', 148: '105361', 149: '105418', 150: '105425', 151: '10547', 152: '105488', 153: '10550', 154: '105503', 155: '105506', 156: '105508', 157: '105509', 158: '105558', 159: '10556', 160: '105570', 161: '105590', 162: '105656', 163: '105671', 164: '105721', 165: '105762', 166: '10580', 167: '105817', 168: '105910', 169: '105940', 170: '105951', 171: '105980', 172: '106029', 173: '106054', 174: '106056', 175: '106069', 176: '106100', 177: '106102', 178: '106110', 179: '106137', 180: '106160', 181: '106229', 182: '106302', 183: '106395', 184: '106412', 185: '106416', 186: '106432', 187: '106553', 188: '106578', 189: '106622', 190: '106650', 191: '106669', 192: '106689', 193: '1067', 194: '106739', 195: '10677', 196: '106787', 197: '106865', 198: '106869', 199: '106886', 200: '106931', 201: '106936', 202: '10697', 203: '10699', 204: '107001', 205: '107020', 206: '107022', 207: '107062', 208: '107074', 209: '107090', 210: '107091', 211: '107096', 212: '107113', 213: '107117', 214: '107190', 215: '107193', 216: '10723', 217: '107232', 218: '107245', 219: '107362', 220: '107395', 221: '107438', 222: '107472', 223: '107518', 224: '107519', 225: '107562', 226: '107633', 227: '107659', 228: '107669', 229: '10771', 230: '107748', 231: '107829', 232: '107832', 233: '10785', 234: '107865', 235: '107876', 236: '107894', 237: '107899', 238: '107967', 239: '107982', 240: '107986', 241: '108044', 242: '108064', 243: '108082', 244: '108083', 245: '108135', 246: '108166', 247: '10819', 248: '108221', 249: '108235', 250: '108239', 251: '1083', 252: '108340', 253: '108371', 254: '108401', 255: '10849', 256: '108660', 257: '108775', 258: '108793', 259: '108806', 260: '108893', 261: '108920', 262: '1090', 263: '109004', 264: '109012', 265: '109050', 266: '109101', 267: '109175', 268: '109217', 269: '109242', 270: '109273', 271: '109277', 272: '109327', 273: '109361', 274: '109449', 275: '109494', 276: '109529', 277: '10955', 278: '109600', 279: '109627', 280: '109680', 281: '109711', 282: '109716', 283: '109736', 284: '109781', 285: '109786', 286: '109805', 287: '109848', 288: '109852', 289: '109868', 290: '109910', 291: '109937', 292: '109977', 293: '11', 294: '110054', 295: '110077', 296: '110104', 297: '110138', 298: '110150', 299: '110158', 300: '110160', 301: '110190', 302: '110192', 303: '11020', 304: '110222', 305: '110263', 306: '110265', 307: '110268', 308: '110273', 309: '110328', 310: '110332', 311: '110350', 312: '110355', 313: '11042', 314: '110491', 315: '110550', 316: '110566', 317: '110635', 318: '110645', 319: '110662', 320: '110674', 321: '110707', 322: '110849', 323: '110859', 324: '110881', 325: '110925', 326: '110991', 327: '111', 328: '111017', 329: '111024', 330: '111111', 331: '111121', 332: '111155', 333: '111251', 334: '111258', 335: '111297', 336: '111312', 337: '111338', 338: '111418', 339: '111422', 340: '111510', 341: '111514', 342: '111530', 343: '111554', 344: '111597', 345: '111642', 346: '111685', 347: '111710', 348: '111729', 349: '111789', 350: '111812', 351: '111852', 352: '111871', 353: '111878', 354: '111923', 355: '111974', 356: '112119', 357: '112136', 358: '11228', 359: '112328', 360: '112389', 361: '11240', 362: '112426', 363: '112455', 364: '112474', 365: '112504', 366: '112580', 367: '112724', 368: '112773', 369: '112790', 370: '112792', 371: '1129', 372: '112945', 373: '113002', 374: '113026', 375: '113048', 376: '113075', 377: '113135', 378: '113161', 379: '113177', 380: '113187', 381: '113195', 382: '113199', 383: '113257', 384: '113297', 385: '113337', 386: '113359', 387: '11337', 388: '113373', 389: '11344', 390: '113460', 391: '113527', 392: '113528', 393: '113546', 394: '113600', 395: '113632', 396: '113637', 397: '113696', 398: '113700', 399: '113735', 400: '113844', 401: '113862', 402: '11393', 403: '113975', 404: '114033', 405: '11405', 406: '114060', 407: '114144', 408: '114211', 409: '114297', 410: '114298', 411: '114301', 412: '114305', 413: '114337', 414: '114349', 415: '114361', 416: '114376', 417: '114434', 418: '114445', 419: '114471', 420: '114532', 421: '114542', 422: '114543', 423: '114570', 424: '114590', 425: '114593', 426: '114651', 427: '114657', 428: '114664', 429: '114692', 430: '114700', 431: '114779', 432: '114780', 433: '114783', 434: '114907', 435: '114927', 436: '114971', 437: '11499', 438: '115031', 439: '115055', 440: '115132', 441: '115172', 442: '11518', 443: '115227', 444: '115347', 445: '115539', 446: '115546', 447: '115568', 448: '115577', 449: '115586', 450: '115624', 451: '115651', 452: '115753', 453: '115796', 454: '115923', 455: '11593', 456: '116022', 457: '11604', 458: '116057', 459: '116085', 460: '1161', 461: '116102', 462: '116123', 463: '116189', 464: '116209', 465: '116247', 466: '116375', 467: '116380', 468: '116473', 469: '116517', 470: '116534', 471: '11656', 472: '116567', 473: '116635', 474: '11667', 475: '116692', 476: '116759', 477: '116783', 478: '116851', 479: '116856', 480: '116862', 481: '116863', 482: '116977', 483: '11698', 484: '116990', 485: '116991', 486: '117029', 487: '117080', 488: '117086', 489: '117094', 490: '117102', 491: '11712', 492: '117156', 493: '11717', 494: '117195', 495: '117212', 496: '11724', 497: '117268', 498: '117278', 499: '117318', 500: '117418', 501: '117454', 502: '11752', 503: '117583', 504: '11759', 505: '117601', 506: '117632', 507: '117645', 508: '117680', 509: '117728', 510: '117784', 511: '117794', 512: '117831', 513: '117883', 514: '117884', 515: '117910', 516: '117940', 517: '117956', 518: '117968', 519: '117978', 520: '11799', 521: '118132', 522: '118184', 523: '118192', 524: '118218', 525: '11825', 526: '118314', 527: '118317', 528: '118342', 529: '118378', 530: '118404', 531: '11843', 532: '118442', 533: '118443', 534: '118454', 535: '118473', 536: '118481', 537: '118533', 538: '118555', 539: '118666', 540: '118669', 541: '118681', 542: '11869', 543: '118763', 544: '11877', 545: '118817', 546: '118969', 547: '11908', 548: '11916', 549: '119168', 550: '119171', 551: '119176', 552: '11922', 553: '119242', 554: '119254', 555: '119258', 556: '119259', 557: '119311', 558: '119316', 559: '119342', 560: '119346', 561: '119348', 562: '119372', 563: '11939', 564: '119475', 565: '119493', 566: '119509', 567: '119545', 568: '119568', 569: '119569', 570: '119587', 571: '11959', 572: '119592', 573: '119616', 574: '119654', 575: '119672', 576: '119673', 577: '119692', 578: '119754', 579: '119801', 580: '119839', 581: '119929', 582: '119954', 583: '119984', 584: '120024', 585: '120065', 586: '120166', 587: '120192', 588: '1203', 589: '120338', 590: '120552', 591: '120561', 592: '1206', 593: '120627', 594: '120736', 595: '120753', 596: '120767', 597: '120790', 598: '120806', 599: '121018', 600: '121093', 601: '121117', 602: '121119', 603: '121122', 604: '121140', 605: '121145', 606: '121172', 607: '121221', 608: '121329', 609: '121383', 610: '121397', 611: '121437', 612: '12149', 613: '121508', 614: '121563', 615: '121579', 616: '121651', 617: '121758', 618: '121790', 619: '121831', 620: '121850', 621: '121927', 622: '122001', 623: '122039', 624: '12210', 625: '122152', 626: '122155', 627: '12216', 628: '122312', 629: '122339', 630: '122399', 631: '122422', 632: '122479', 633: '122486', 634: '122514', 635: '122538', 636: '122593', 637: '122629', 638: '122637', 639: '122673', 640: '122675', 641: '122714', 642: '122839', 643: '122863', 644: '122872', 645: '122881', 646: '122958', 647: '122960', 648: '123113', 649: '123168', 650: '123173', 651: '123192', 652: '12323', 653: '123239', 654: '123269', 655: '123329', 656: '123377', 657: '123385', 658: '123459', 659: '123463', 660: '123523', 661: '123530', 662: '123588', 663: '123591', 664: '12365', 665: '123665', 666: '12373', 667: '123752', 668: '123763', 669: '123765', 670: '123820', 671: '12383', 672: '123896', 673: '123915', 674: '123929', 675: '123939', 676: '123998', 677: '124060', 678: '124062', 679: '124137', 680: '124152', 681: '124185', 682: '124216', 683: '124240', 684: '124420', 685: '124434', 686: '124436', 687: '124473', 688: '124543', 689: '12455', 690: '124568', 691: '124629', 692: '124727', 693: '124751', 694: '124755', 695: '124789', 696: '124841', 697: '124847', 698: '124867', 699: '124878', 700: '124886', 701: '124888', 702: '124895', 703: '124916', 704: '124920', 705: '124922', 706: '124930', 707: '124955', 708: '125137', 709: '125159', 710: '125171', 711: '125201', 712: '125246', 713: '125257', 714: '125332', 715: '125339', 716: '125444', 717: '125508', 718: '125522', 719: '125526', 720: '125528', 721: '125533', 722: '125647', 723: '12580', 724: '125801', 725: '125809', 726: '125891', 727: '125900', 728: '125933', 729: '125973', 730: '126072', 731: '126076', 732: '126102', 733: '12612', 734: '126132', 735: '126183', 736: '126209', 737: '126243', 738: '126270', 739: '126289', 740: '126378', 741: '126515', 742: '126525', 743: '126552', 744: '126573', 745: '126605', 746: '126609', 747: '126661', 748: '126690', 749: '126716', 750: '126717', 751: '12675', 752: '126755', 753: '126776', 754: '126786', 755: '1268', 756: '12681', 757: '126830', 758: '126937', 759: '126963', 760: '127018', 761: '127053', 762: '127062', 763: '127137', 764: '127144', 765: '127152', 766: '127220', 767: '127257', 768: '12726', 769: '127261', 770: '127389', 771: '127411', 772: '127427', 773: '127429', 774: '127462', 775: '127507', 776: '12754', 777: '127587', 778: '127588', 779: '127635', 780: '127674', 781: '127696', 782: '127725', 783: '127803', 784: '127999', 785: '128021', 786: '128059', 787: '128081', 788: '128096', 789: '12812', 790: '128211', 791: '128358', 792: '128390', 793: '128404', 794: '128415', 795: '128447', 796: '12850', 797: '128506', 798: '128542', 799: '128574', 800: '128608', 801: '128647', 802: '128751', 803: '128777', 804: '128897', 805: '128972', 806: '129003', 807: '129017', 808: '12907', 809: '129096', 810: '129102', 811: '129103', 812: '12918', 813: '129226', 814: '129292', 815: '129363', 816: '129376', 817: '129386', 818: '129452', 819: '129492', 820: '129570', 821: '129578', 822: '129579', 823: '129590', 824: '129596', 825: '129598', 826: '129673', 827: '129824', 828: '12983', 829: '129838', 830: '129858', 831: '129889', 832: '129951', 833: '129994', 834: '130034', 835: '130035', 836: '130042', 837: '130076', 838: '130123', 839: '130211', 840: '130222', 841: '13025', 842: '130328', 843: '130349', 844: '130404', 845: '130458', 846: '130529', 847: '130531', 848: '130573', 849: '130583', 850: '130587', 851: '130709', 852: '130837', 853: '131012', 854: '131033', 855: '131133', 856: '131188', 857: '131211', 858: '13136', 859: '131467', 860: '131502', 861: '131505', 862: '131527', 863: '131596', 864: '131615', 865: '131638', 866: '131712', 867: '131741', 868: '131798', 869: '131807', 870: '131873', 871: '131941', 872: '13197', 873: '132037', 874: '132123', 875: '132133', 876: '132143', 877: '132173', 878: '132338', 879: '132377', 880: '132413', 881: '132440', 882: '132466', 883: '132482', 884: '132498', 885: '132525', 886: '132544', 887: '132591', 888: '13262', 889: '132620', 890: '132631', 891: '132636', 892: '132708', 893: '132765', 894: '132786', 895: '132804', 896: '132830', 897: '132855', 898: '13286', 899: '132870', 900: '13288', 901: '132893', 902: '132916', 903: '132994', 904: '133006', 905: '133038', 906: '13312', 907: '133160', 908: '133177', 909: '133219', 910: '133257', 911: '133259', 912: '133416', 913: '13342', 914: '133491', 915: '133506', 916: '133513', 917: '133514', 918: '133520', 919: '13359', 920: '133620', 921: '133686', 922: '133701', 923: '133715', 924: '133743', 925: '13377', 926: '133779', 927: '133789', 928: '133823', 929: '133826', 930: '133849', 931: '133956', 932: '133995', 933: '134010', 934: '134011', 935: '134036', 936: '134051', 937: '134060', 938: '134065', 939: '134084', 940: '134114', 941: '134417', 942: '134422', 943: '134507', 944: '134514', 945: '13454', 946: '134547', 947: '134592', 948: '134634', 949: '134648', 950: '134659', 951: '134728', 952: '134740', 953: '1348', 954: '134802', 955: '134883', 956: '134887', 957: '134906', 958: '134942', 959: '134965', 960: '135047', 961: '135056', 962: '13506', 963: '135065', 964: '135076', 965: '135084', 966: '135108', 967: '135119', 968: '135222', 969: '13531', 970: '135318', 971: '135331', 972: '13535', 973: '135414', 974: '135425', 975: '13552', 976: '135567', 977: '135581', 978: '135608', 979: '135612', 980: '135618', 981: '135621', 982: '135659', 983: '135691', 984: '135800', 985: '13584', 986: '13585', 987: '135864', 988: '135902', 989: '135913', 990: '13593', 991: '13604', 992: '136076', 993: '136087', 994: '136107', 995: '136121', 996: '136138', 997: '136143', 998: '136158', 999: '136310', 1000: '136320', 1001: '136367', 1002: '136532', 1003: '136571', 1004: '136693', 1005: '136705', 1006: '13672', 1007: '136744', 1008: '136779', 1009: '13681', 1010: '13690', 1011: '136924', 1012: '137048', 1013: '137115', 1014: '137155', 1015: '13717', 1016: '137221', 1017: '137251', 1018: '137256', 1019: '137260', 1020: '137277', 1021: '137307', 1022: '137308', 1023: '137423', 1024: '137474', 1025: '137493', 1026: '13750', 1027: '137524', 1028: '137532', 1029: '137576', 1030: '13758', 1031: '137602', 1032: '137705', 1033: '137756', 1034: '137775', 1035: '137778', 1036: '137855', 1037: '137862', 1038: '137922', 1039: '137961', 1040: '137982', 1041: '13800', 1042: '138093', 1043: '138165', 1044: '138182', 1045: '138194', 1046: '138201', 1047: '138250', 1048: '138268', 1049: '138335', 1050: '138374', 1051: '138385', 1052: '138442', 1053: '138489', 1054: '138590', 1055: '13867', 1056: '138711', 1057: '138804', 1058: '138903', 1059: '13891', 1060: '138958', 1061: '139055', 1062: '139067', 1063: '139109', 1064: '139150', 1065: '139175', 1066: '139191', 1067: '139225', 1068: '139231', 1069: '139246', 1070: '139288', 1071: '139314', 1072: '139455', 1073: '139462', 1074: '139476', 1075: '139512', 1076: '139534', 1077: '139566', 1078: '139590', 1079: '139641', 1080: '139685', 1081: '139688', 1082: '139929', 1083: '139930', 1084: '139951', 1085: '139971', 1086: '139992', 1087: '140108', 1088: '140155', 1089: '140163', 1090: '140264', 1091: '140272', 1092: '140273', 1093: '140302', 1094: '140447', 1095: '140467', 1096: '140509', 1097: '140541', 1098: '140571', 1099: '140720', 1100: '140731', 1101: '14077', 1102: '140819', 1103: '14083', 1104: '140833', 1105: '140918', 1106: '140982', 1107: '14102', 1108: '141067', 1109: '141084', 1110: '141117', 1111: '141139', 1112: '141171', 1113: '141206', 1114: '141281', 1115: '14132', 1116: '141350', 1117: '141353', 1118: '141358', 1119: '141446', 1120: '141476', 1121: '141488', 1122: '14151', 1123: '141545', 1124: '141635', 1125: '141697', 1126: '141698', 1127: '141732', 1128: '141746', 1129: '141825', 1130: '141894', 1131: '141899', 1132: '14195', 1133: '141980', 1134: '141988', 1135: '142010', 1136: '14202', 1137: '142057', 1138: '142069', 1139: '142088', 1140: '142130', 1141: '142140', 1142: '142144', 1143: '142176', 1144: '1422', 1145: '142206', 1146: '142320', 1147: '142325', 1148: '142334', 1149: '142345', 1150: '142373', 1151: '142401', 1152: '142443', 1153: '14246', 1154: '142615', 1155: '142621', 1156: '142665', 1157: '142682', 1158: '142689', 1159: '142720', 1160: '142724', 1161: '142745', 1162: '142801', 1163: '1430', 1164: '143004', 1165: '143027', 1166: '14303', 1167: '143035', 1168: '1431', 1169: '143105', 1170: '143111', 1171: '143121', 1172: '143151', 1173: '143164', 1174: '14317', 1175: '143196', 1176: '143307', 1177: '143319', 1178: '143438', 1179: '143501', 1180: '143625', 1181: '143636', 1182: '143674', 1183: '143694', 1184: '143716', 1185: '143735', 1186: '143807', 1187: '143907', 1188: '143944', 1189: '143946', 1190: '143988', 1191: '143997', 1192: '144022', 1193: '144100', 1194: '14416', 1195: '144195', 1196: '144234', 1197: '144260', 1198: '144261', 1199: '144297', 1200: '144308', 1201: '144337', 1202: '144381', 1203: '144465', 1204: '144574', 1205: '144625', 1206: '144659', 1207: '144670', 1208: '144700', 1209: '144714', 1210: '144751', 1211: '144783', 1212: '144816', 1213: '14484', 1214: '144853', 1215: '144860', 1216: '144886', 1217: '145043', 1218: '145058', 1219: '145113', 1220: '145131', 1221: '145138', 1222: '145141', 1223: '14515', 1224: '14517', 1225: '145197', 1226: '145217', 1227: '145226', 1228: '145245', 1229: '145281', 1230: '145332', 1231: '145360', 1232: '145371', 1233: '145390', 1234: '145421', 1235: '145507', 1236: '145562', 1237: '145627', 1238: '145633', 1239: '145649', 1240: '145673', 1241: '145715', 1242: '145748', 1243: '145767', 1244: '145769', 1245: '145798', 1246: '145875', 1247: '145979', 1248: '146040', 1249: '146074', 1250: '146085', 1251: '146088', 1252: '146107', 1253: '146117', 1254: '146130', 1255: '14616', 1256: '146248', 1257: '146317', 1258: '146376', 1259: '14653', 1260: '146547', 1261: '14656', 1262: '146601', 1263: '146605', 1264: '146607', 1265: '146768', 1266: '146792', 1267: '146811', 1268: '146844', 1269: '146845', 1270: '146900', 1271: '146905', 1272: '146910', 1273: '146949', 1274: '14697', 1275: '147118', 1276: '147161', 1277: '147209', 1278: '147222', 1279: '147226', 1280: '147228', 1281: '147233', 1282: '147246', 1283: '147283', 1284: '147360', 1285: '147368', 1286: '147390', 1287: '147426', 1288: '147461', 1289: '147494', 1290: '14759', 1291: '147590', 1292: '147682', 1293: '147817', 1294: '147818', 1295: '14790', 1296: '147952', 1297: '147991', 1298: '148047', 1299: '148054', 1300: '148070', 1301: '148128', 1302: '148202', 1303: '148203', 1304: '148221', 1305: '148269', 1306: '148308', 1307: '148352', 1308: '148387', 1309: '148403', 1310: '148541', 1311: '148546', 1312: '148586', 1313: '148712', 1314: '148781', 1315: '148825', 1316: '148865', 1317: '14898', 1318: '148980', 1319: '148986', 1320: '148996', 1321: '149021', 1322: '14903', 1323: '149035', 1324: '149051', 1325: '149076', 1326: '149115', 1327: '149175', 1328: '149254', 1329: '149268', 1330: '14927', 1331: '14929', 1332: '149290', 1333: '149364', 1334: '149367', 1335: '149456', 1336: '149528', 1337: '149557', 1338: '149592', 1339: '149719', 1340: '149724', 1341: '149856', 1342: '149868', 1343: '149898', 1344: '149913', 1345: '149944', 1346: '15006', 1347: '150123', 1348: '150193', 1349: '150293', 1350: '150368', 1351: '150574', 1352: '15058', 1353: '150610', 1354: '150651', 1355: '150693', 1356: '150705', 1357: '150743', 1358: '150877', 1359: '150889', 1360: '150899', 1361: '150905', 1362: '150954', 1363: '150980', 1364: '15099', 1365: '150997', 1366: '151079', 1367: '151099', 1368: '151111', 1369: '151119', 1370: '151165', 1371: '151170', 1372: '151192', 1373: '151233', 1374: '151236', 1375: '151323', 1376: '151410', 1377: '151474', 1378: '151486', 1379: '151507', 1380: '151563', 1381: '151594', 1382: '151608', 1383: '151650', 1384: '151695', 1385: '15171', 1386: '151724', 1387: '151739', 1388: '15182', 1389: '151908', 1390: '151910', 1391: '151937', 1392: '151975', 1393: '152005', 1394: '152008', 1395: '152009', 1396: '152151', 1397: '15218', 1398: '15233', 1399: '152336', 1400: '152372', 1401: '15240', 1402: '152436', 1403: '152456', 1404: '152523', 1405: '152608', 1406: '152609', 1407: '15268', 1408: '152707', 1409: '152711', 1410: '152758', 1411: '152815', 1412: '15289', 1413: '153015', 1414: '153041', 1415: '153079', 1416: '153085', 1417: '153136', 1418: '15316', 1419: '153187', 1420: '153276', 1421: '153422', 1422: '153432', 1423: '153454', 1424: '153463', 1425: '153474', 1426: '153495', 1427: '153511', 1428: '15352', 1429: '153522', 1430: '153527', 1431: '153573', 1432: '153577', 1433: '153640', 1434: '153662', 1435: '15376', 1436: '15381', 1437: '153816', 1438: '15382', 1439: '153836', 1440: '153849', 1441: '153864', 1442: '153869', 1443: '153874', 1444: '153880', 1445: '153881', 1446: '153902', 1447: '153911', 1448: '153980', 1449: '154093', 1450: '15422', 1451: '154228', 1452: '154235', 1453: '154258', 1454: '154381', 1455: '154392', 1456: '154397', 1457: '154426', 1458: '154433', 1459: '154436', 1460: '154526', 1461: '154559', 1462: '154627', 1463: '154657', 1464: '154718', 1465: '154734', 1466: '154749', 1467: '154751', 1468: '154753', 1469: '154784', 1470: '154787', 1471: '154828', 1472: '154829', 1473: '154876', 1474: '154882', 1475: '154896', 1476: '154924', 1477: '154937', 1478: '154957', 1479: '154962', 1480: '155016', 1481: '155066', 1482: '155078', 1483: '155144', 1484: '155147', 1485: '155150', 1486: '155155', 1487: '155167', 1488: '155183', 1489: '155206', 1490: '155311', 1491: '155333', 1492: '155395', 1493: '155410', 1494: '155461', 1495: '155482', 1496: '155530', 1497: '15554', 1498: '155546', 1499: '155619', 1500: '155661', 1501: '15572', 1502: '155758', 1503: '155806', 1504: '15587', 1505: '155888', 1506: '155903', 1507: '155924', 1508: '155925', 1509: '155944', 1510: '155949', 1511: '155957', 1512: '156024', 1513: '156052', 1514: '156139', 1515: '156175', 1516: '156217', 1517: '156255', 1518: '156291', 1519: '156296', 1520: '156311', 1521: '156336', 1522: '156446', 1523: '156464', 1524: '156500', 1525: '156534', 1526: '156590', 1527: '156637', 1528: '156668', 1529: '156671', 1530: '156696', 1531: '156708', 1532: '156727', 1533: '156772', 1534: '156787', 1535: '156865', 1536: '156885', 1537: '156956', 1538: '15707', 1539: '157076', 1540: '157167', 1541: '157184', 1542: '157220', 1543: '157233', 1544: '157345', 1545: '15737', 1546: '157390', 1547: '157504', 1548: '157524', 1549: '15757', 1550: '157702', 1551: '157713', 1552: '157808', 1553: '157828', 1554: '157843', 1555: '157850', 1556: '157851', 1557: '157874', 1558: '157881', 1559: '157894', 1560: '157911', 1561: '157919', 1562: '157962', 1563: '157975', 1564: '157977', 1565: '158055', 1566: '158074', 1567: '158113', 1568: '158127', 1569: '158262', 1570: '158303', 1571: '158357', 1572: '158392', 1573: '158453', 1574: '15848', 1575: '158481', 1576: '158497', 1577: '158499', 1578: '158534', 1579: '158538', 1580: '158617', 1581: '158649', 1582: '158669', 1583: '158687', 1584: '158709', 1585: '158752', 1586: '15889', 1587: '158968', 1588: '158975', 1589: '159004', 1590: '159070', 1591: '15914', 1592: '159144', 1593: '159171', 1594: '159213', 1595: '159236', 1596: '15927', 1597: '159313', 1598: '159317', 1599: '159357', 1600: '159389', 1601: '159447', 1602: '159496', 1603: '159532', 1604: '159583', 1605: '159593', 1606: '159609', 1607: '15961', 1608: '159712', 1609: '159750', 1610: '159820', 1611: '159831', 1612: '159867', 1613: '159983', 1614: '160023', 1615: '160030', 1616: '160051', 1617: '160063', 1618: '160143', 1619: '160183', 1620: '160184', 1621: '160232', 1622: '160252', 1623: '160322', 1624: '160324', 1625: '16034', 1626: '160351', 1627: '160392', 1628: '160413', 1629: '160479', 1630: '160500', 1631: '160502', 1632: '160535', 1633: '160551', 1634: '160581', 1635: '16062', 1636: '160642', 1637: '160694', 1638: '160739', 1639: '160752', 1640: '160803', 1641: '160823', 1642: '160856', 1643: '160867', 1644: '160882', 1645: '160930', 1646: '160933', 1647: '161051', 1648: '161059', 1649: '161074', 1650: '161107', 1651: '161156', 1652: '161157', 1653: '161161', 1654: '161235', 1655: '161245', 1656: '16126', 1657: '161421', 1658: '161577', 1659: '161594', 1660: '161596', 1661: '161612', 1662: '161641', 1663: '161658', 1664: '161679', 1665: '161693', 1666: '161746', 1667: '161747', 1668: '16178', 1669: '161851', 1670: '161933', 1671: '161978', 1672: '161989', 1673: '161990', 1674: '162011', 1675: '162079', 1676: '162080', 1677: '162148', 1678: '162179', 1679: '162202', 1680: '162238', 1681: '16225', 1682: '162262', 1683: '162303', 1684: '162328', 1685: '162342', 1686: '162344', 1687: '16236', 1688: '162460', 1689: '162476', 1690: '16261', 1691: '162667', 1692: '16271', 1693: '162731', 1694: '162746', 1695: '162770', 1696: '162789', 1697: '162835', 1698: '162857', 1699: '162880', 1700: '162887', 1701: '162953', 1702: '163003', 1703: '163017', 1704: '163028', 1705: '163067', 1706: '163082', 1707: '163132', 1708: '163141', 1709: '16318', 1710: '163207', 1711: '163271', 1712: '163273', 1713: '163359', 1714: '163449', 1715: '163460', 1716: '163500', 1717: '163567', 1718: '163582', 1719: '163668', 1720: '163684', 1721: '163709', 1722: '163725', 1723: '163732', 1724: '163762', 1725: '163770', 1726: '163798', 1727: '163831', 1728: '163876', 1729: '163894', 1730: '163935', 1731: '163986', 1732: '164047', 1733: '164142', 1734: '164149', 1735: '164169', 1736: '164199', 1737: '164297', 1738: '164308', 1739: '164314', 1740: '164354', 1741: '164370', 1742: '16441', 1743: '164427', 1744: '164477', 1745: '16448', 1746: '164481', 1747: '164554', 1748: '16457', 1749: '164616', 1750: '164659', 1751: '164746', 1752: '164764', 1753: '164769', 1754: '16479', 1755: '164818', 1756: '164835', 1757: '164837', 1758: '164846', 1759: '164926', 1760: '164927', 1761: '164967', 1762: '164988', 1763: '165018', 1764: '165049', 1765: '165092', 1766: '165171', 1767: '165197', 1768: '165243', 1769: '165269', 1770: '165271', 1771: '165298', 1772: '165382', 1773: '165383', 1774: '165418', 1775: '165435', 1776: '165440', 1777: '165451', 1778: '165552', 1779: '165622', 1780: '165646', 1781: '165679', 1782: '165699', 1783: '165709', 1784: '165723', 1785: '165725', 1786: '165782', 1787: '165790', 1788: '165810', 1789: '165811', 1790: '165815', 1791: '165866', 1792: '165963', 1793: '166090', 1794: '166092', 1795: '166140', 1796: '166190', 1797: '166216', 1798: '166234', 1799: '166237', 1800: '166291', 1801: '166317', 1802: '166347', 1803: '166360', 1804: '166439', 1805: '166471', 1806: '166503', 1807: '166605', 1808: '166635', 1809: '166644', 1810: '166650', 1811: '166677', 1812: '166679', 1813: '166702', 1814: '166769', 1815: '16678', 1816: '166818', 1817: '166821', 1818: '166823', 1819: '166825', 1820: '166856', 1821: '166874', 1822: '166909', 1823: '166932', 1824: '166961', 1825: '166977', 1826: '167061', 1827: '167109', 1828: '167154', 1829: '167209', 1830: '167219', 1831: '167324', 1832: '167350', 1833: '167352', 1834: '167405', 1835: '167416', 1836: '167484', 1837: '167577', 1838: '167593', 1839: '16761', 1840: '16762', 1841: '167660', 1842: '167695', 1843: '167753', 1844: '16779', 1845: '167808', 1846: '167861', 1847: '167879', 1848: '167912', 1849: '167994', 1850: '168146', 1851: '168155', 1852: '168226', 1853: '168247', 1854: '168267', 1855: '168274', 1856: '168352', 1857: '168457', 1858: '168458', 1859: '168576', 1860: '168606', 1861: '168630', 1862: '168653', 1863: '16868', 1864: '16870', 1865: '168709', 1866: '168759', 1867: '168859', 1868: '168862', 1869: '16888', 1870: '168974', 1871: '16902', 1872: '169114', 1873: '169215', 1874: '169250', 1875: '169261', 1876: '169316', 1877: '16932', 1878: '169341', 1879: '169410', 1880: '169416', 1881: '16942', 1882: '169515', 1883: '169525', 1884: '169540', 1885: '169542', 1886: '169556', 1887: '169590', 1888: '169635', 1889: '169656', 1890: '16974', 1891: '169747', 1892: '169749', 1893: '169828', 1894: '169844', 1895: '169979', 1896: '17003', 1897: '170051', 1898: '170088', 1899: '170119', 1900: '170132', 1901: '170153', 1902: '170181', 1903: '170243', 1904: '17029', 1905: '17032', 1906: '170321', 1907: '170428', 1908: '170449', 1909: '170477', 1910: '170521', 1911: '170524', 1912: '170538', 1913: '170587', 1914: '170607', 1915: '170608', 1916: '170616', 1917: '170639', 1918: '170671', 1919: '170672', 1920: '170800', 1921: '170816', 1922: '170857', 1923: '170858', 1924: '170885', 1925: '17090', 1926: '170929', 1927: '170945', 1928: '170973', 1929: '171107', 1930: '171170', 1931: '171276', 1932: '171282', 1933: '171304', 1934: '171306', 1935: '171322', 1936: '171499', 1937: '171602', 1938: '171609', 1939: '171637', 1940: '171668', 1941: '17167', 1942: '171698', 1943: '17172', 1944: '171721', 1945: '171726', 1946: '171794', 1947: '171795', 1948: '171843', 1949: '171955', 1950: '172024', 1951: '172043', 1952: '172066', 1953: '172130', 1954: '172148', 1955: '172149', 1956: '17223', 1957: '172360', 1958: '172376', 1959: '172378', 1960: '172401', 1961: '172515', 1962: '172525', 1963: '172556', 1964: '172571', 1965: '172630', 1966: '172660', 1967: '1727', 1968: '172788', 1969: '172797', 1970: '172869', 1971: '172902', 1972: '172909', 1973: '172947', 1974: '172976', 1975: '172987', 1976: '1730', 1977: '17301', 1978: '173033', 1979: '173038', 1980: '173042', 1981: '173077', 1982: '173095', 1983: '173129', 1984: '173166', 1985: '173189', 1986: '173194', 1987: '173293', 1988: '173353', 1989: '17337', 1990: '173395', 1991: '173417', 1992: '173457', 1993: '173469', 1994: '173471', 1995: '173490', 1996: '173533', 1997: '173600', 1998: '173625', 1999: '173629', 2000: '173640', 2001: '173646', 2002: '173651', 2003: '173665', 2004: '173675', 2005: '173703', 2006: '173733', 2007: '173778', 2008: '173817', 2009: '173872', 2010: '173876', 2011: '173911', 2012: '173936', 2013: '173949', 2014: '173957', 2015: '173972', 2016: '173996', 2017: '174006', 2018: '174055', 2019: '174103', 2020: '174127', 2021: '174157', 2022: '17417', 2023: '174198', 2024: '174249', 2025: '174416', 2026: '174473', 2027: '174484', 2028: '174530', 2029: '174562', 2030: '174659', 2031: '174805', 2032: '174959', 2033: '175025', 2034: '175080', 2035: '175098', 2036: '175123', 2037: '175215', 2038: '1753', 2039: '175353', 2040: '175355', 2041: '175398', 2042: '175401', 2043: '175457', 2044: '175462', 2045: '17547', 2046: '175488', 2047: '17552', 2048: '175531', 2049: '175577', 2050: '17569', 2051: '17571', 2052: '175739', 2053: '175820', 2054: '175833', 2055: '175894', 2056: '175937', 2057: '176070', 2058: '176229', 2059: '176241', 2060: '176245', 2061: '176251', 2062: '176279', 2063: '176286', 2064: '176289', 2065: '176358', 2066: '176403', 2067: '176427', 2068: '176458', 2069: '176477', 2070: '1765', 2071: '176522', 2072: '176557', 2073: '176708', 2074: '176778', 2075: '176785', 2076: '176791', 2077: '176892', 2078: '176925', 2079: '176936', 2080: '177002', 2081: '177006', 2082: '177038', 2083: '177149', 2084: '177165', 2085: '177167', 2086: '177210', 2087: '177218', 2088: '17738', 2089: '177383', 2090: '177414', 2091: '177437', 2092: '177462', 2093: '177502', 2094: '177550', 2095: '177621', 2096: '177625', 2097: '177655', 2098: '177701', 2099: '177854', 2100: '177886', 2101: '177895', 2102: '177916', 2103: '177961', 2104: '177971', 2105: '177972', 2106: '177981', 2107: '178010', 2108: '178017', 2109: '178158', 2110: '178177', 2111: '178196', 2112: '178214', 2113: '178216', 2114: '17823', 2115: '178269', 2116: '178270', 2117: '178332', 2118: '178354', 2119: '178371', 2120: '178402', 2121: '178426', 2122: '178492', 2123: '178569', 2124: '178577', 2125: '178579', 2126: '178596', 2127: '17862', 2128: '178627', 2129: '178651', 2130: '178723', 2131: '178742', 2132: '178761', 2133: '178792', 2134: '1788', 2135: '178877', 2136: '178887', 2137: '178900', 2138: '178917', 2139: '178984', 2140: '179016', 2141: '179062', 2142: '179105', 2143: '17912', 2144: '17916', 2145: '179211', 2146: '179293', 2147: '179328', 2148: '179404', 2149: '179419', 2150: '179467', 2151: '179526', 2152: '179538', 2153: '179610', 2154: '179624', 2155: '179643', 2156: '179665', 2157: '179717', 2158: '179787', 2159: '179838', 2160: '179861', 2161: '179879', 2162: '179901', 2163: '179909', 2164: '18004', 2165: '180046', 2166: '180105', 2167: '180163', 2168: '18019', 2169: '180208', 2170: '180220', 2171: '180231', 2172: '180284', 2173: '1803', 2174: '18032', 2175: '18037', 2176: '180404', 2177: '180417', 2178: '180453', 2179: '180485', 2180: '180526', 2181: '180576', 2182: '180582', 2183: '180586', 2184: '180608', 2185: '180609', 2186: '180612', 2187: '180690', 2188: '18070', 2189: '180717', 2190: '180738', 2191: '180770', 2192: '18078', 2193: '180851', 2194: '180891', 2195: '18090', 2196: '180922', 2197: '180938', 2198: '180981', 2199: '181017', 2200: '181093', 2201: '181102', 2202: '181120', 2203: '181157', 2204: '181196', 2205: '181218', 2206: '181263', 2207: '181275', 2208: '1813', 2209: '18133', 2210: '181336', 2211: '181349', 2212: '181365', 2213: '181375', 2214: '181427', 2215: '181440', 2216: '181530', 2217: '181546', 2218: '181660', 2219: '181832', 2220: '181865', 2221: '181867', 2222: '181875', 2223: '181965', 2224: '181991', 2225: '182002', 2226: '182007', 2227: '182050', 2228: '182065', 2229: '182072', 2230: '182138', 2231: '182232', 2232: '182277', 2233: '182324', 2234: '182332', 2235: '182342', 2236: '182374', 2237: '182445', 2238: '182464', 2239: '182674', 2240: '182676', 2241: '182731', 2242: '182810', 2243: '182827', 2244: '182877', 2245: '182915', 2246: '182926', 2247: '182953', 2248: '18306', 2249: '183184', 2250: '183198', 2251: '18322', 2252: '183224', 2253: '183249', 2254: '183253', 2255: '183307', 2256: '183325', 2257: '183343', 2258: '183390', 2259: '183413', 2260: '183435', 2261: '183466', 2262: '183489', 2263: '183490', 2264: '183501', 2265: '183509', 2266: '183536', 2267: '183547', 2268: '183591', 2269: '1836', 2270: '18360', 2271: '183650', 2272: '183692', 2273: '183758', 2274: '183772', 2275: '183834', 2276: '18391', 2277: '183926', 2278: '183983', 2279: '183999', 2280: '1840', 2281: '184014', 2282: '184061', 2283: '184118', 2284: '184136', 2285: '184150', 2286: '184168', 2287: '184181', 2288: '184252', 2289: '1843', 2290: '184303', 2291: '184381', 2292: '18439', 2293: '184422', 2294: '184438', 2295: '184456', 2296: '184498', 2297: '184506', 2298: '184573', 2299: '184646', 2300: '184710', 2301: '184733', 2302: '184754', 2303: '184783', 2304: '184786', 2305: '184814', 2306: '184820', 2307: '184827', 2308: '184877', 2309: '184890', 2310: '184920', 2311: '184967', 2312: '184972', 2313: '184991', 2314: '185089', 2315: '185154', 2316: '18524', 2317: '185320', 2318: '185342', 2319: '185383', 2320: '185413', 2321: '185417', 2322: '185449', 2323: '185560', 2324: '185580', 2325: '18559', 2326: '185600', 2327: '185614', 2328: '185619', 2329: '185630', 2330: '185658', 2331: '185682', 2332: '185694', 2333: '185767', 2334: '1858', 2335: '18583', 2336: '185900', 2337: '186002', 2338: '18601', 2339: '186032', 2340: '186107', 2341: '186122', 2342: '186162', 2343: '186182', 2344: '186186', 2345: '186233', 2346: '186252', 2347: '18634', 2348: '186362', 2349: '186434', 2350: '186534', 2351: '186549', 2352: '186591', 2353: '186599', 2354: '186614', 2355: '186618', 2356: '186807', 2357: '186845', 2358: '186849', 2359: '186854', 2360: '187066', 2361: '18711', 2362: '18712', 2363: '187155', 2364: '187175', 2365: '187180', 2366: '187181', 2367: '187187', 2368: '187190', 2369: '187219', 2370: '187271', 2371: '187282', 2372: '187302', 2373: '187316', 2374: '18733', 2375: '187355', 2376: '187392', 2377: '187398', 2378: '187411', 2379: '18743', 2380: '187455', 2381: '187464', 2382: '187498', 2383: '187516', 2384: '187553', 2385: '187568', 2386: '187588', 2387: '187651', 2388: '187669', 2389: '187686', 2390: '187688', 2391: '187746', 2392: '18775', 2393: '18779', 2394: '187813', 2395: '187857', 2396: '187896', 2397: '187933', 2398: '187983', 2399: '187996', 2400: '188004', 2401: '188037', 2402: '188109', 2403: '18814', 2404: '188193', 2405: '188227', 2406: '188245', 2407: '188302', 2408: '188352', 2409: '188359', 2410: '188382', 2411: '188405', 2412: '188479', 2413: '188594', 2414: '188601', 2415: '188612', 2416: '188732', 2417: '18874', 2418: '188752', 2419: '188805', 2420: '188885', 2421: '18894', 2422: '188972', 2423: '188997', 2424: '189001', 2425: '189005', 2426: '189057', 2427: '189105', 2428: '189148', 2429: '189175', 2430: '189201', 2431: '189206', 2432: '189236', 2433: '18925', 2434: '189287', 2435: '189333', 2436: '189363', 2437: '189421', 2438: '189425', 2439: '189436', 2440: '189480', 2441: '189482', 2442: '18950', 2443: '189531', 2444: '18965', 2445: '189654', 2446: '189732', 2447: '189744', 2448: '189789', 2449: '189951', 2450: '189996', 2451: '190050', 2452: '190061', 2453: '190111', 2454: '190113', 2455: '19018', 2456: '19020', 2457: '190217', 2458: '190249', 2459: '190322', 2460: '190339', 2461: '190343', 2462: '190359', 2463: '190368', 2464: '190449', 2465: '190469', 2466: '19047', 2467: '190500', 2468: '190529', 2469: '190553', 2470: '190646', 2471: '190691', 2472: '190748', 2473: '190816', 2474: '190884', 2475: '190912', 2476: '191000', 2477: '191009', 2478: '191038', 2479: '191065', 2480: '191071', 2481: '19112', 2482: '191163', 2483: '191320', 2484: '191442', 2485: '19148', 2486: '191543', 2487: '191600', 2488: '191623', 2489: '19166', 2490: '191681', 2491: '191753', 2492: '191809', 2493: '191816', 2494: '191825', 2495: '19187', 2496: '191938', 2497: '191991', 2498: '192050', 2499: '192125', 2500: '192136', 2501: '192141', 2502: '1922', 2503: '192340', 2504: '192352', 2505: '192365', 2506: '192406', 2507: '192420', 2508: '192439', 2509: '192457', 2510: '192474', 2511: '192491', 2512: '192497', 2513: '192523', 2514: '192532', 2515: '192556', 2516: '192565', 2517: '192576', 2518: '192577', 2519: '192590', 2520: '192593', 2521: '192616', 2522: '192651', 2523: '192667', 2524: '192687', 2525: '1927', 2526: '19273', 2527: '192793', 2528: '192827', 2529: '192843', 2530: '192867', 2531: '193023', 2532: '193053', 2533: '193055', 2534: '193078', 2535: '19309', 2536: '193107', 2537: '193158', 2538: '193182', 2539: '193207', 2540: '193281', 2541: '193287', 2542: '193391', 2543: '193446', 2544: '193464', 2545: '193476', 2546: '193478', 2547: '193506', 2548: '193507', 2549: '19353', 2550: '19355', 2551: '193559', 2552: '193601', 2553: '193640', 2554: '193659', 2555: '193668', 2556: '193689', 2557: '19369', 2558: '193694', 2559: '193793', 2560: '193827', 2561: '19386', 2562: '193904', 2563: '193905', 2564: '193983', 2565: '19399', 2566: '19404', 2567: '194090', 2568: '194118', 2569: '194167', 2570: '194186', 2571: '194210', 2572: '194219', 2573: '194235', 2574: '194246', 2575: '194262', 2576: '194332', 2577: '194357', 2578: '194373', 2579: '19441', 2580: '194438', 2581: '194453', 2582: '194607', 2583: '194657', 2584: '19470', 2585: '194709', 2586: '194727', 2587: '194730', 2588: '194757', 2589: '194771', 2590: '19479', 2591: '194805', 2592: '194814', 2593: '194848', 2594: '194904', 2595: '194976', 2596: '194997', 2597: '195123', 2598: '19514', 2599: '195150', 2600: '195180', 2601: '195185', 2602: '195285', 2603: '195336', 2604: '195390', 2605: '195424', 2606: '195429', 2607: '195507', 2608: '195538', 2609: '195570', 2610: '19560', 2611: '195601', 2612: '19563', 2613: '195657', 2614: '195694', 2615: '195710', 2616: '195752', 2617: '195766', 2618: '195778', 2619: '195791', 2620: '195913', 2621: '195955', 2622: '195994', 2623: '196007', 2624: '196008', 2625: '19602', 2626: '196026', 2627: '196028', 2628: '19604', 2629: '19607', 2630: '196106', 2631: '196195', 2632: '196204', 2633: '196236', 2634: '196238', 2635: '196328', 2636: '196329', 2637: '196331', 2638: '196340', 2639: '196365', 2640: '196379', 2641: '196381', 2642: '196384', 2643: '196412', 2644: '196430', 2645: '196439', 2646: '196450', 2647: '196495', 2648: '196516', 2649: '196532', 2650: '196572', 2651: '196580', 2652: '1966', 2653: '196712', 2654: '196741', 2655: '196767', 2656: '196823', 2657: '196912', 2658: '196975', 2659: '196986', 2660: '196987', 2661: '197011', 2662: '197079', 2663: '197119', 2664: '197139', 2665: '197298', 2666: '197410', 2667: '197414', 2668: '197419', 2669: '197426', 2670: '197454', 2671: '197463', 2672: '197494', 2673: '197506', 2674: '197511', 2675: '197600', 2676: '197659', 2677: '19768', 2678: '197882', 2679: '19797', 2680: '198012', 2681: '198020', 2682: '198041', 2683: '198042', 2684: '198058', 2685: '198081', 2686: '1981', 2687: '198128', 2688: '198154', 2689: '198162', 2690: '19817', 2691: '198179', 2692: '198268', 2693: '198274', 2694: '198285', 2695: '198349', 2696: '198369', 2697: '198385', 2698: '1984', 2699: '198483', 2700: '198508', 2701: '198534', 2702: '198617', 2703: '198636', 2704: '198796', 2705: '198992', 2706: '198993', 2707: '199069', 2708: '199168', 2709: '199213', 2710: '199266', 2711: '199326', 2712: '199412', 2713: '199443', 2714: '199553', 2715: '199584', 2716: '199630', 2717: '199643', 2718: '199654', 2719: '199656', 2720: '19969', 2721: '199695', 2722: '199846', 2723: '199896', 2724: '199907', 2725: '199929', 2726: '199955', 2727: '199974', 2728: '200092', 2729: '20010', 2730: '200120', 2731: '200128', 2732: '200134', 2733: '200140', 2734: '200206', 2735: '200216', 2736: '200325', 2737: '200370', 2738: '200389', 2739: '20040', 2740: '200407', 2741: '200472', 2742: '200554', 2743: '200573', 2744: '200576', 2745: '200608', 2746: '200737', 2747: '20080', 2748: '200821', 2749: '200840', 2750: '200917', 2751: '20097', 2752: '200986', 2753: '200996', 2754: '201034', 2755: '201185', 2756: '201386', 2757: '201466', 2758: '201468', 2759: '201550', 2760: '201577', 2761: '201582', 2762: '201635', 2763: '20168', 2764: '201690', 2765: '201734', 2766: '201745', 2767: '20183', 2768: '201878', 2769: '201954', 2770: '201958', 2771: '20199', 2772: '202006', 2773: '202007', 2774: '202086', 2775: '202154', 2776: '202166', 2777: '202201', 2778: '202245', 2779: '20225', 2780: '202264', 2781: '202340', 2782: '202367', 2783: '202368', 2784: '202397', 2785: '202414', 2786: '202422', 2787: '202429', 2788: '202475', 2789: '202513', 2790: '202540', 2791: '202541', 2792: '202553', 2793: '202557', 2794: '202605', 2795: '202696', 2796: '202786', 2797: '202808', 2798: '202880', 2799: '2029', 2800: '202945', 2801: '202981', 2802: '202982', 2803: '202985', 2804: '20364', 2805: '20366', 2806: '20370', 2807: '20371', 2808: '20421', 2809: '20423', 2810: '20440', 2811: '20444', 2812: '20470', 2813: '20537', 2814: '20556', 2815: '20580', 2816: '20666', 2817: '20677', 2818: '20746', 2819: '20759', 2820: '20780', 2821: '20797', 2822: '20808', 2823: '20898', 2824: '20964', 2825: '21015', 2826: '21058', 2827: '21152', 2828: '2118', 2829: '21189', 2830: '21195', 2831: '21232', 2832: '21238', 2833: '21283', 2834: '21321', 2835: '2134', 2836: '21358', 2837: '21395', 2838: '21428', 2839: '21441', 2840: '21455', 2841: '2149', 2842: '2150', 2843: '21627', 2844: '21629', 2845: '21664', 2846: '21678', 2847: '21756', 2848: '21757', 2849: '21800', 2850: '21808', 2851: '21836', 2852: '21855', 2853: '21875', 2854: '21963', 2855: '2198', 2856: '22031', 2857: '22084', 2858: '22218', 2859: '22263', 2860: '22318', 2861: '22346', 2862: '22353', 2863: '22376', 2864: '22402', 2865: '22420', 2866: '22508', 2867: '22518', 2868: '22535', 2869: '22591', 2870: '22596', 2871: '22677', 2872: '22687', 2873: '22710', 2874: '22716', 2875: '22725', 2876: '22726', 2877: '2286', 2878: '22897', 2879: '22926', 2880: '22934', 2881: '22941', 2882: '22957', 2883: '22961', 2884: '22965', 2885: '22984', 2886: '23', 2887: '23080', 2888: '23081', 2889: '23093', 2890: '2312', 2891: '23152', 2892: '23159', 2893: '23177', 2894: '23214', 2895: '23235', 2896: '23245', 2897: '23288', 2898: '23465', 2899: '23469', 2900: '23472', 2901: '23485', 2902: '23515', 2903: '23532', 2904: '23566', 2905: '23596', 2906: '23613', 2907: '23616', 2908: '23627', 2909: '23706', 2910: '2372', 2911: '23743', 2912: '23749', 2913: '23800', 2914: '23834', 2915: '23858', 2916: '23859', 2917: '23874', 2918: '23903', 2919: '23931', 2920: '23943', 2921: '23959', 2922: '23960', 2923: '23966', 2924: '24019', 2925: '24173', 2926: '24198', 2927: '2423', 2928: '24243', 2929: '24378', 2930: '24479', 2931: '2450', 2932: '24550', 2933: '24561', 2934: '24567', 2935: '24595', 2936: '24602', 2937: '24606', 2938: '24638', 2939: '24658', 2940: '24693', 2941: '24711', 2942: '24735', 2943: '24813', 2944: '24849', 2945: '24879', 2946: '24978', 2947: '24986', 2948: '24991', 2949: '24998', 2950: '2510', 2951: '25127', 2952: '25159', 2953: '25190', 2954: '25202', 2955: '25218', 2956: '25241', 2957: '25257', 2958: '25281', 2959: '25322', 2960: '25333', 2961: '25397', 2962: '25405', 2963: '2546', 2964: '25470', 2965: '25567', 2966: '2557', 2967: '25599', 2968: '25683', 2969: '25701', 2970: '25814', 2971: '25855', 2972: '25901', 2973: '25905', 2974: '25918', 2975: '25974', 2976: '260', 2977: '26003', 2978: '26040', 2979: '26041', 2980: '26076', 2981: '26080', 2982: '26098', 2983: '26156', 2984: '26186', 2985: '2623', 2986: '26387', 2987: '26395', 2988: '26402', 2989: '26494', 2990: '26500', 2991: '2651', 2992: '26521', 2993: '26552', 2994: '26603', 2995: '26613', 2996: '26666', 2997: '26701', 2998: '2671', 2999: '26719', 3000: '26813', 3001: '26815', 3002: '26855', 3003: '26887', 3004: '26961', 3005: '27036', 3006: '27099', 3007: '27120', 3008: '27139', 3009: '27143', 3010: '27239', 3011: '27267', 3012: '2727', 3013: '27333', 3014: '27362', 3015: '27405', 3016: '27415', 3017: '27451', 3018: '2749', 3019: '27585', 3020: '27602', 3021: '27606', 3022: '27640', 3023: '27712', 3024: '27720', 3025: '27805', 3026: '27842', 3027: '27851', 3028: '27880', 3029: '28056', 3030: '28057', 3031: '28087', 3032: '28147', 3033: '28159', 3034: '28175', 3035: '28225', 3036: '28229', 3037: '2823', 3038: '28233', 3039: '28257', 3040: '28263', 3041: '28279', 3042: '28292', 3043: '28293', 3044: '28338', 3045: '28421', 3046: '28454', 3047: '28516', 3048: '28537', 3049: '28549', 3050: '28556', 3051: '28591', 3052: '28618', 3053: '28630', 3054: '28637', 3055: '28651', 3056: '28666', 3057: '28679', 3058: '28733', 3059: '2878', 3060: '28900', 3061: '28901', 3062: '28970', 3063: '29004', 3064: '29019', 3065: '29042', 3066: '29128', 3067: '29156', 3068: '29218', 3069: '29222', 3070: '29245', 3071: '29262', 3072: '29399', 3073: '29516', 3074: '29604', 3075: '29605', 3076: '29626', 3077: '29683', 3078: '29695', 3079: '29741', 3080: '29766', 3081: '29774', 3082: '29845', 3083: '29858', 3084: '29893', 3085: '2992', 3086: '2995', 3087: '30006', 3088: '30016', 3089: '30025', 3090: '30036', 3091: '30081', 3092: '30114', 3093: '30127', 3094: '30149', 3095: '30216', 3096: '30221', 3097: '30263', 3098: '30276', 3099: '30295', 3100: '30303', 3101: '3032', 3102: '30322', 3103: '30350', 3104: '30361', 3105: '30367', 3106: '30412', 3107: '30416', 3108: '30518', 3109: '30536', 3110: '30591', 3111: '30605', 3112: '30699', 3113: '30718', 3114: '30734', 3115: '30773', 3116: '30815', 3117: '30884', 3118: '3094', 3119: '31020', 3120: '31052', 3121: '31058', 3122: '31103', 3123: '31130', 3124: '31232', 3125: '31252', 3126: '31316', 3127: '31337', 3128: '31375', 3129: '3140', 3130: '31464', 3131: '31489', 3132: '3151', 3133: '31541', 3134: '31619', 3135: '31629', 3136: '31659', 3137: '31712', 3138: '31781', 3139: '318', 3140: '31848', 3141: '31887', 3142: '3190', 3143: '31993', 3144: '31996', 3145: '32017', 3146: '32034', 3147: '32084', 3148: '32110', 3149: '32166', 3150: '32228', 3151: '32246', 3152: '32275', 3153: '32360', 3154: '32362', 3155: '32366', 3156: '32510', 3157: '32519', 3158: '32537', 3159: '32546', 3160: '3268', 3161: '32742', 3162: '32743', 3163: '32810', 3164: '32886', 3165: '32976', 3166: '3298', 3167: '33023', 3168: '33043', 3169: '33059', 3170: '33073', 3171: '3308', 3172: '33081', 3173: '33082', 3174: '33086', 3175: '33089', 3176: '33140', 3177: '33173', 3178: '33182', 3179: '33212', 3180: '33216', 3181: '33229', 3182: '33251', 3183: '33268', 3184: '33283', 3185: '3330', 3186: '33318', 3187: '33320', 3188: '33355', 3189: '33399', 3190: '33438', 3191: '33440', 3192: '33449', 3193: '33451', 3194: '33472', 3195: '33492', 3196: '33503', 3197: '3351', 3198: '33596', 3199: '33626', 3200: '33674', 3201: '33686', 3202: '33714', 3203: '33859', 3204: '3389', 3205: '33913', 3206: '33944', 3207: '33981', 3208: '34011', 3209: '34023', 3210: '34029', 3211: '34036', 3212: '34075', 3213: '34126', 3214: '34143', 3215: '34149', 3216: '3415', 3217: '34171', 3218: '34201', 3219: '34237', 3220: '34239', 3221: '34243', 3222: '34258', 3223: '34278', 3224: '34282', 3225: '34306', 3226: '34335', 3227: '34338', 3228: '34377', 3229: '34399', 3230: '344', 3231: '34429', 3232: '34436', 3233: '34469', 3234: '34470', 3235: '34480', 3236: '34486', 3237: '34559', 3238: '34626', 3239: '347', 3240: '34703', 3241: '34729', 3242: '34741', 3243: '34754', 3244: '3479', 3245: '34820', 3246: '34972', 3247: '35061', 3248: '35073', 3249: '35139', 3250: '35155', 3251: '35156', 3252: '35243', 3253: '35273', 3254: '35328', 3255: '3533', 3256: '3537', 3257: '35385', 3258: '35403', 3259: '35450', 3260: '35455', 3261: '35491', 3262: '35500', 3263: '35503', 3264: '35563', 3265: '35618', 3266: '35647', 3267: '35809', 3268: '35812', 3269: '35836', 3270: '35839', 3271: '35879', 3272: '35897', 3273: '35924', 3274: '36002', 3275: '36017', 3276: '36019', 3277: '36024', 3278: '36069', 3279: '36091', 3280: '36130', 3281: '3616', 3282: '36207', 3283: '36262', 3284: '3633', 3285: '36333', 3286: '36355', 3287: '36430', 3288: '36454', 3289: '36456', 3290: '36460', 3291: '36514', 3292: '36568', 3293: '36671', 3294: '36687', 3295: '36736', 3296: '36741', 3297: '3676', 3298: '36773', 3299: '36790', 3300: '36866', 3301: '37004', 3302: '37056', 3303: '37158', 3304: '37239', 3305: '37278', 3306: '37338', 3307: '37377', 3308: '37390', 3309: '37428', 3310: '37436', 3311: '37492', 3312: '37537', 3313: '3757', 3314: '37633', 3315: '37658', 3316: '3767', 3317: '37688', 3318: '37713', 3319: '37726', 3320: '37761', 3321: '37769', 3322: '37832', 3323: '37858', 3324: '37893', 3325: '3794', 3326: '37991', 3327: '38002', 3328: '38036', 3329: '38049', 3330: '38084', 3331: '38096', 3332: '3811', 3333: '38110', 3334: '38215', 3335: '38226', 3336: '38314', 3337: '38315', 3338: '38337', 3339: '38350', 3340: '38389', 3341: '38465', 3342: '38560', 3343: '38563', 3344: '38576', 3345: '38637', 3346: '38646', 3347: '38651', 3348: '38702', 3349: '38734', 3350: '38743', 3351: '38746', 3352: '38785', 3353: '38821', 3354: '38876', 3355: '38918', 3356: '39060', 3357: '39122', 3358: '39134', 3359: '392', 3360: '39212', 3361: '39218', 3362: '39366', 3363: '39389', 3364: '39399', 3365: '39497', 3366: '3952', 3367: '3958', 3368: '3964', 3369: '39697', 3370: '39732', 3371: '39739', 3372: '39786', 3373: '39792', 3374: '39841', 3375: '39894', 3376: '39947', 3377: '39984', 3378: '40009', 3379: '40014', 3380: '40185', 3381: '40208', 3382: '40219', 3383: '40245', 3384: '40277', 3385: '40305', 3386: '4033', 3387: '40348', 3388: '40444', 3389: '40477', 3390: '40488', 3391: '4056', 3392: '40568', 3393: '4070', 3394: '40708', 3395: '40839', 3396: '40858', 3397: '40907', 3398: '40989', 3399: '41049', 3400: '41076', 3401: '41079', 3402: '41091', 3403: '41157', 3404: '41178', 3405: '41219', 3406: '41256', 3407: '41299', 3408: '41340', 3409: '41344', 3410: '41379', 3411: '41381', 3412: '41408', 3413: '41409', 3414: '41494', 3415: '41499', 3416: '41532', 3417: '41594', 3418: '41621', 3419: '41629', 3420: '41681', 3421: '41687', 3422: '41695', 3423: '41699', 3424: '41716', 3425: '41761', 3426: '41829', 3427: '41888', 3428: '41913', 3429: '41934', 3430: '41937', 3431: '42002', 3432: '42024', 3433: '42056', 3434: '42085', 3435: '42090', 3436: '42127', 3437: '42268', 3438: '42275', 3439: '42315', 3440: '42332', 3441: '42346', 3442: '4236', 3443: '42384', 3444: '42445', 3445: '42458', 3446: '42476', 3447: '42497', 3448: '42535', 3449: '42549', 3450: '42602', 3451: '42604', 3452: '42618', 3453: '42620', 3454: '4266', 3455: '42669', 3456: '42744', 3457: '42750', 3458: '42754', 3459: '42792', 3460: '42811', 3461: '42935', 3462: '42958', 3463: '42971', 3464: '42992', 3465: '43057', 3466: '43144', 3467: '43145', 3468: '43203', 3469: '43293', 3470: '43343', 3471: '43441', 3472: '43467', 3473: '43547', 3474: '43594', 3475: '43706', 3476: '43719', 3477: '43733', 3478: '43759', 3479: '43769', 3480: '43770', 3481: '43820', 3482: '43826', 3483: '43846', 3484: '43855', 3485: '44139', 3486: '44154', 3487: '44169', 3488: '44176', 3489: '44200', 3490: '44408', 3491: '4445', 3492: '44477', 3493: '44555', 3494: '44792', 3495: '44797', 3496: '44847', 3497: '44895', 3498: '44942', 3499: '44981', 3500: '44982', 3501: '45006', 3502: '45011', 3503: '45122', 3504: '45148', 3505: '45215', 3506: '45236', 3507: '45279', 3508: '4531', 3509: '4534', 3510: '45346', 3511: '45406', 3512: '45411', 3513: '45414', 3514: '45454', 3515: '45463', 3516: '45533', 3517: '45591', 3518: '45635', 3519: '4565', 3520: '45674', 3521: '45725', 3522: '45737', 3523: '45747', 3524: '45775', 3525: '45785', 3526: '45806', 3527: '45858', 3528: '45911', 3529: '46048', 3530: '46066', 3531: '46069', 3532: '46071', 3533: '46113', 3534: '46117', 3535: '46177', 3536: '46275', 3537: '46289', 3538: '46334', 3539: '46355', 3540: '464', 3541: '46408', 3542: '46410', 3543: '46442', 3544: '46444', 3545: '46538', 3546: '46551', 3547: '46597', 3548: '46625', 3549: '46626', 3550: '46653', 3551: '46731', 3552: '46750', 3553: '468', 3554: '46824', 3555: '46875', 3556: '46883', 3557: '46950', 3558: '47000', 3559: '47025', 3560: '47071', 3561: '471', 3562: '47102', 3563: '47129', 3564: '47227', 3565: '47255', 3566: '47271', 3567: '47288', 3568: '47335', 3569: '47382', 3570: '47423', 3571: '47436', 3572: '47456', 3573: '47464', 3574: '47490', 3575: '47550', 3576: '47553', 3577: '47576', 3578: '47581', 3579: '47653', 3580: '47657', 3581: '47747', 3582: '47754', 3583: '47888', 3584: '47893', 3585: '47911', 3586: '47921', 3587: '47938', 3588: '48140', 3589: '48148', 3590: '48153', 3591: '48169', 3592: '4819', 3593: '48232', 3594: '48281', 3595: '48299', 3596: '48329', 3597: '48450', 3598: '48454', 3599: '48526', 3600: '4856', 3601: '48630', 3602: '48667', 3603: '48673', 3604: '48691', 3605: '48707', 3606: '48717', 3607: '48758', 3608: '48777', 3609: '48786', 3610: '48794', 3611: '4881', 3612: '48870', 3613: '48884', 3614: '48996', 3615: '48999', 3616: '49037', 3617: '49044', 3618: '49068', 3619: '49076', 3620: '49096', 3621: '4911', 3622: '49144', 3623: '49180', 3624: '49181', 3625: '49188', 3626: '49213', 3627: '49236', 3628: '49339', 3629: '49364', 3630: '49369', 3631: '49420', 3632: '49441', 3633: '49520', 3634: '49565', 3635: '49583', 3636: '49608', 3637: '49620', 3638: '49634', 3639: '49672', 3640: '4969', 3641: '497', 3642: '49721', 3643: '49753', 3644: '49785', 3645: '49844', 3646: '49886', 3647: '49903', 3648: '49915', 3649: '49990', 3650: '50035', 3651: '50126', 3652: '50132', 3653: '50163', 3654: '50184', 3655: '50200', 3656: '50308', 3657: '50353', 3658: '50404', 3659: '50411', 3660: '50425', 3661: '50459', 3662: '50627', 3663: '50678', 3664: '50719', 3665: '50728', 3666: '50807', 3667: '5081', 3668: '50817', 3669: '50839', 3670: '50880', 3671: '50933', 3672: '50934', 3673: '50943', 3674: '50948', 3675: '50975', 3676: '50988', 3677: '51034', 3678: '51062', 3679: '51064', 3680: '51110', 3681: '51279', 3682: '51281', 3683: '51301', 3684: '51431', 3685: '51578', 3686: '51584', 3687: '51597', 3688: '51652', 3689: '51678', 3690: '51726', 3691: '51801', 3692: '51807', 3693: '5181', 3694: '51830', 3695: '51838', 3696: '5189', 3697: '51930', 3698: '5194', 3699: '51975', 3700: '52049', 3701: '52149', 3702: '52175', 3703: '52184', 3704: '52286', 3705: '5231', 3706: '52348', 3707: '52377', 3708: '52434', 3709: '52437', 3710: '52446', 3711: '52504', 3712: '52508', 3713: '52543', 3714: '52562', 3715: '5262', 3716: '5267', 3717: '52674', 3718: '52722', 3719: '52750', 3720: '52807', 3721: '52823', 3722: '52993', 3723: '5300', 3724: '53104', 3725: '5312', 3726: '53200', 3727: '53231', 3728: '53319', 3729: '53326', 3730: '53330', 3731: '53357', 3732: '53382', 3733: '53397', 3734: '53405', 3735: '53417', 3736: '53447', 3737: '5349', 3738: '53530', 3739: '53539', 3740: '53643', 3741: '53679', 3742: '53700', 3743: '53721', 3744: '5373', 3745: '53748', 3746: '53771', 3747: '53801', 3748: '53836', 3749: '53919', 3750: '5393', 3751: '54002', 3752: '54014', 3753: '54036', 3754: '54044', 3755: '54047', 3756: '54092', 3757: '54187', 3758: '54191', 3759: '54312', 3760: '54347', 3761: '54397', 3762: '54428', 3763: '54515', 3764: '5454', 3765: '54584', 3766: '54586', 3767: '54797', 3768: '5480', 3769: '54810', 3770: '54855', 3771: '54912', 3772: '54913', 3773: '54957', 3774: '54966', 3775: '54967', 3776: '54988', 3777: '55016', 3778: '55018', 3779: '55058', 3780: '55073', 3781: '55096', 3782: '55142', 3783: '55163', 3784: '55209', 3785: '5523', 3786: '55263', 3787: '55343', 3788: '55648', 3789: '55655', 3790: '55664', 3791: '55718', 3792: '55787', 3793: '5581', 3794: '55851', 3795: '55874', 3796: '5590', 3797: '55910', 3798: '55920', 3799: '55928', 3800: '55988', 3801: '56002', 3802: '56065', 3803: '56070', 3804: '56194', 3805: '56250', 3806: '56256', 3807: '56284', 3808: '56307', 3809: '56356', 3810: '56401', 3811: '5644', 3812: '5646', 3813: '56517', 3814: '56555', 3815: '56575', 3816: '56593', 3817: '56627', 3818: '568', 3819: '5683', 3820: '56913', 3821: '56936', 3822: '56964', 3823: '57089', 3824: '57170', 3825: '5738', 3826: '57397', 3827: '5746', 3828: '57529', 3829: '57606', 3830: '57643', 3831: '57679', 3832: '57694', 3833: '57718', 3834: '57738', 3835: '57864', 3836: '57880', 3837: '57881', 3838: '57890', 3839: '5808', 3840: '58084', 3841: '58098', 3842: '5813', 3843: '58134', 3844: '58140', 3845: '58232', 3846: '58344', 3847: '58349', 3848: '58363', 3849: '58378', 3850: '5839', 3851: '58415', 3852: '58444', 3853: '58480', 3854: '58515', 3855: '58567', 3856: '58568', 3857: '58612', 3858: '58629', 3859: '58664', 3860: '58668', 3861: '5867', 3862: '58781', 3863: '58797', 3864: '58830', 3865: '58856', 3866: '5890', 3867: '58953', 3868: '58979', 3869: '59108', 3870: '59235', 3871: '59282', 3872: '59442', 3873: '59454', 3874: '59468', 3875: '59473', 3876: '59526', 3877: '59538', 3878: '59593', 3879: '59630', 3880: '59707', 3881: '59719', 3882: '59785', 3883: '5988', 3884: '59943', 3885: '59957', 3886: '59979', 3887: '6020', 3888: '60216', 3889: '60218', 3890: '60229', 3891: '60331', 3892: '60382', 3893: '60392', 3894: '60442', 3895: '60480', 3896: '60543', 3897: '6057', 3898: '60576', 3899: '60628', 3900: '60634', 3901: '60681', 3902: '60698', 3903: '60717', 3904: '60754', 3905: '60758', 3906: '60760', 3907: '60936', 3908: '60941', 3909: '60950', 3910: '60985', 3911: '61000', 3912: '61069', 3913: '61120', 3914: '61204', 3915: '61215', 3916: '61239', 3917: '61301', 3918: '61368', 3919: '61380', 3920: '61434', 3921: '61481', 3922: '61631', 3923: '61668', 3924: '61693', 3925: '61723', 3926: '61728', 3927: '61791', 3928: '61805', 3929: '61846', 3930: '61851', 3931: '61972', 3932: '6206', 3933: '62075', 3934: '62078', 3935: '62097', 3936: '62212', 3937: '62232', 3938: '62329', 3939: '62330', 3940: '62349', 3941: '62418', 3942: '62437', 3943: '62447', 3944: '6245', 3945: '62554', 3946: '62579', 3947: '62581', 3948: '62615', 3949: '62659', 3950: '62662', 3951: '62669', 3952: '62679', 3953: '62717', 3954: '62766', 3955: '62781', 3956: '62803', 3957: '62940', 3958: '62946', 3959: '62985', 3960: '6301', 3961: '63041', 3962: '63050', 3963: '63083', 3964: '63147', 3965: '63166', 3966: '63190', 3967: '633', 3968: '63318', 3969: '63321', 3970: '63365', 3971: '63404', 3972: '63409', 3973: '63439', 3974: '63484', 3975: '63551', 3976: '63571', 3977: '63588', 3978: '63616', 3979: '6362', 3980: '63640', 3981: '63780', 3982: '63798', 3983: '63838', 3984: '63913', 3985: '64036', 3986: '64076', 3987: '64088', 3988: '64138', 3989: '64140', 3990: '64199', 3991: '64219', 3992: '64223', 3993: '64263', 3994: '64278', 3995: '64335', 3996: '64367', 3997: '64377', 3998: '64432', 3999: '64442', 4000: '64478', 4001: '64480', 4002: '64515', 4003: '64522', 4004: '64568', 4005: '64632', 4006: '64665', 4007: '6473', 4008: '64761', 4009: '64764', 4010: '64765', 4011: '64783', 4012: '6482', 4013: '64844', 4014: '64860', 4015: '64887', 4016: '64922', 4017: '64938', 4018: '64966', 4019: '65015', 4020: '65028', 4021: '65112', 4022: '65212', 4023: '65247', 4024: '65440', 4025: '65497', 4026: '65516', 4027: '6553', 4028: '65563', 4029: '65581', 4030: '65617', 4031: '65628', 4032: '6568', 4033: '65741', 4034: '65745', 4035: '6578', 4036: '65786', 4037: '65808', 4038: '65856', 4039: '6587', 4040: '65898', 4041: '65900', 4042: '65981', 4043: '66010', 4044: '66029', 4045: '66065', 4046: '66074', 4047: '66115', 4048: '6619', 4049: '66210', 4050: '66228', 4051: '66288', 4052: '66317', 4053: '66326', 4054: '66345', 4055: '66370', 4056: '66430', 4057: '66467', 4058: '66524', 4059: '66552', 4060: '66564', 4061: '66621', 4062: '6669', 4063: '66700', 4064: '66723', 4065: '66741', 4066: '66789', 4067: '66838', 4068: '66854', 4069: '66866', 4070: '6692', 4071: '66934', 4072: '66962', 4073: '67001', 4074: '67061', 4075: '67122', 4076: '67130', 4077: '67132', 4078: '67154', 4079: '67163', 4080: '6721', 4081: '67258', 4082: '67259', 4083: '67413', 4084: '67463', 4085: '67476', 4086: '67483', 4087: '67499', 4088: '67611', 4089: '67614', 4090: '67635', 4091: '67662', 4092: '67702', 4093: '67868', 4094: '68032', 4095: '68061', 4096: '68072', 4097: '68084', 4098: '68092', 4099: '68109', 4100: '68120', 4101: '68137', 4102: '68141', 4103: '68150', 4104: '68173', 4105: '68194', 4106: '68215', 4107: '68227', 4108: '68278', 4109: '68319', 4110: '68335', 4111: '68353', 4112: '68360', 4113: '68382', 4114: '68412', 4115: '68493', 4116: '68499', 4117: '68553', 4118: '68572', 4119: '68700', 4120: '68747', 4121: '6877', 4122: '68786', 4123: '68831', 4124: '68925', 4125: '68932', 4126: '68938', 4127: '68941', 4128: '68968', 4129: '68988', 4130: '69021', 4131: '69025', 4132: '69032', 4133: '69095', 4134: '69108', 4135: '69115', 4136: '69147', 4137: '69162', 4138: '69169', 4139: '69178', 4140: '69203', 4141: '69220', 4142: '69226', 4143: '69248', 4144: '69250', 4145: '69272', 4146: '69342', 4147: '69370', 4148: '69388', 4149: '6942', 4150: '69509', 4151: '69527', 4152: '6956', 4153: '69575', 4154: '69605', 4155: '69610', 4156: '69671', 4157: '6968', 4158: '6972', 4159: '69753', 4160: '69773', 4161: '69809', 4162: '69846', 4163: '69858', 4164: '69874', 4165: '69954', 4166: '69980', 4167: '70007', 4168: '70036', 4169: '7004', 4170: '7021', 4171: '70254', 4172: '7027', 4173: '7029', 4174: '70291', 4175: '70297', 4176: '70491', 4177: '70611', 4178: '70756', 4179: '70806', 4180: '70940', 4181: '71061', 4182: '7117', 4183: '71268', 4184: '71381', 4185: '71419', 4186: '7148', 4187: '71560', 4188: '71579', 4189: '71584', 4190: '71586', 4191: '71614', 4192: '71621', 4193: '71625', 4194: '71645', 4195: '71726', 4196: '71759', 4197: '71791', 4198: '71827', 4199: '71838', 4200: '71845', 4201: '7186', 4202: '7190', 4203: '71914', 4204: '71939', 4205: '72037', 4206: '72067', 4207: '72078', 4208: '72125', 4209: '72126', 4210: '72232', 4211: '72295', 4212: '7235', 4213: '72408', 4214: '72440', 4215: '72478', 4216: '72508', 4217: '72539', 4218: '7254', 4219: '72556', 4220: '72689', 4221: '72741', 4222: '72766', 4223: '73000', 4224: '73023', 4225: '73039', 4226: '73115', 4227: '73169', 4228: '73204', 4229: '73255', 4230: '73285', 4231: '73303', 4232: '73308', 4233: '73340', 4234: '73392', 4235: '73395', 4236: '73428', 4237: '73513', 4238: '73526', 4239: '73552', 4240: '73731', 4241: '73763', 4242: '73833', 4243: '73902', 4244: '73960', 4245: '74010', 4246: '74122', 4247: '74160', 4248: '74221', 4249: '74268', 4250: '74388', 4251: '74417', 4252: '74435', 4253: '74447', 4254: '74489', 4255: '74530', 4256: '74552', 4257: '74564', 4258: '74595', 4259: '74602', 4260: '74777', 4261: '74813', 4262: '74877', 4263: '74909', 4264: '7492', 4265: '74961', 4266: '74984', 4267: '75039', 4268: '75116', 4269: '75265', 4270: '75268', 4271: '75291', 4272: '75384', 4273: '75581', 4274: '756', 4275: '75614', 4276: '75634', 4277: '75648', 4278: '75700', 4279: '75738', 4280: '75769', 4281: '75854', 4282: '7589', 4283: '75922', 4284: '75959', 4285: '76009', 4286: '76089', 4287: '76188', 4288: '76203', 4289: '7621', 4290: '76234', 4291: '76244', 4292: '76251', 4293: '76254', 4294: '76336', 4295: '76342', 4296: '76449', 4297: '76477', 4298: '76496', 4299: '76510', 4300: '76567', 4301: '76580', 4302: '76624', 4303: '76652', 4304: '76711', 4305: '76725', 4306: '76731', 4307: '76752', 4308: '76767', 4309: '76776', 4310: '76809', 4311: '76827', 4312: '76858', 4313: '76894', 4314: '76895', 4315: '77026', 4316: '77082', 4317: '77093', 4318: '77181', 4319: '77309', 4320: '77338', 4321: '77354', 4322: '77446', 4323: '77463', 4324: '77469', 4325: '7747', 4326: '77515', 4327: '77623', 4328: '77667', 4329: '77736', 4330: '77773', 4331: '77866', 4332: '77894', 4333: '77895', 4334: '77899', 4335: '77909', 4336: '77913', 4337: '77944', 4338: '77960', 4339: '77983', 4340: '78034', 4341: '78039', 4342: '7805', 4343: '78051', 4344: '78108', 4345: '7812', 4346: '78166', 4347: '78174', 4348: '78199', 4349: '78208', 4350: '78229', 4351: '7824', 4352: '78243', 4353: '78306', 4354: '78389', 4355: '78427', 4356: '7848', 4357: '78587', 4358: '78610', 4359: '78643', 4360: '78653', 4361: '78686', 4362: '78778', 4363: '78799', 4364: '78837', 4365: '78974', 4366: '79070', 4367: '79097', 4368: '79106', 4369: '79112', 4370: '79172', 4371: '79221', 4372: '79227', 4373: '79311', 4374: '79328', 4375: '7940', 4376: '79406', 4377: '79498', 4378: '79539', 4379: '79562', 4380: '79563', 4381: '79571', 4382: '79623', 4383: '7963', 4384: '79682', 4385: '79786', 4386: '79798', 4387: '79870', 4388: '799', 4389: '79906', 4390: '79932', 4391: '79945', 4392: '80008', 4393: '80045', 4394: '80079', 4395: '80080', 4396: '8010', 4397: '80104', 4398: '80105', 4399: '80156', 4400: '80194', 4401: '8023', 4402: '80265', 4403: '80344', 4404: '80384', 4405: '80412', 4406: '80425', 4407: '80458', 4408: '80512', 4409: '80517', 4410: '80558', 4411: '80664', 4412: '80686', 4413: '80721', 4414: '80732', 4415: '80763', 4416: '8079', 4417: '80796', 4418: '80845', 4419: '80935', 4420: '80953', 4421: '80979', 4422: '81023', 4423: '81044', 4424: '81085', 4425: '81100', 4426: '81107', 4427: '8114', 4428: '81346', 4429: '81366', 4430: '81373', 4431: '81387', 4432: '81398', 4433: '81525', 4434: '81568', 4435: '81574', 4436: '81594', 4437: '81621', 4438: '81638', 4439: '81714', 4440: '81717', 4441: '81798', 4442: '81807', 4443: '81919', 4444: '81999', 4445: '82014', 4446: '82021', 4447: '82040', 4448: '82054', 4449: '82135', 4450: '8216', 4451: '8217', 4452: '82180', 4453: '82188', 4454: '82263', 4455: '82375', 4456: '82396', 4457: '82429', 4458: '82435', 4459: '8255', 4460: '82670', 4461: '82671', 4462: '82687', 4463: '82703', 4464: '82745', 4465: '82775', 4466: '82803', 4467: '82823', 4468: '82908', 4469: '82920', 4470: '82962', 4471: '82985', 4472: '82996', 4473: '83001', 4474: '83013', 4475: '83036', 4476: '83050', 4477: '83058', 4478: '83067', 4479: '8309', 4480: '83117', 4481: '83153', 4482: '832', 4483: '83220', 4484: '83280', 4485: '83332', 4486: '83536', 4487: '83591', 4488: '8374', 4489: '83756', 4490: '83798', 4491: '83807', 4492: '83852', 4493: '83959', 4494: '83975', 4495: '84041', 4496: '84086', 4497: '84243', 4498: '84280', 4499: '84302', 4500: '84304', 4501: '84318', 4502: '84354', 4503: '84372', 4504: '84386', 4505: '84401', 4506: '84440', 4507: '84547', 4508: '84576', 4509: '84603', 4510: '84620', 4511: '8466', 4512: '84690', 4513: '84817', 4514: '84978', 4515: '8507', 4516: '85079', 4517: '8512', 4518: '85122', 4519: '85181', 4520: '85193', 4521: '85209', 4522: '85267', 4523: '85301', 4524: '85309', 4525: '85338', 4526: '85346', 4527: '85367', 4528: '85444', 4529: '85538', 4530: '85580', 4531: '8560', 4532: '85636', 4533: '8564', 4534: '85666', 4535: '85668', 4536: '85772', 4537: '85773', 4538: '85815', 4539: '8583', 4540: '85851', 4541: '85875', 4542: '85983', 4543: '85997', 4544: '86079', 4545: '86115', 4546: '86116', 4547: '86124', 4548: '8620', 4549: '86262', 4550: '86281', 4551: '86433', 4552: '86457', 4553: '86507', 4554: '86535', 4555: '86631', 4556: '86674', 4557: '86725', 4558: '86765', 4559: '86783', 4560: '8688', 4561: '86893', 4562: '86957', 4563: '86978', 4564: '87019', 4565: '87031', 4566: '8706', 4567: '87060', 4568: '87071', 4569: '87106', 4570: '87147', 4571: '87148', 4572: '87224', 4573: '87278', 4574: '87327', 4575: '87341', 4576: '87359', 4577: '87410', 4578: '87441', 4579: '87496', 4580: '87530', 4581: '87542', 4582: '87546', 4583: '87567', 4584: '87673', 4585: '8769', 4586: '87703', 4587: '8771', 4588: '87839', 4589: '87848', 4590: '87911', 4591: '87943', 4592: '88050', 4593: '88053', 4594: '88101', 4595: '88180', 4596: '88369', 4597: '88394', 4598: '88453', 4599: '88459', 4600: '88520', 4601: '88531', 4602: '88557', 4603: '88583', 4604: '88603', 4605: '88628', 4606: '88687', 4607: '88689', 4608: '88720', 4609: '88807', 4610: '88859', 4611: '88875', 4612: '88880', 4613: '88921', 4614: '88926', 4615: '88936', 4616: '88946', 4617: '88963', 4618: '89026', 4619: '89052', 4620: '89090', 4621: '8912', 4622: '8913', 4623: '89386', 4624: '89391', 4625: '89430', 4626: '89456', 4627: '89495', 4628: '89538', 4629: '89646', 4630: '89652', 4631: '89791', 4632: '89793', 4633: '89812', 4634: '89895', 4635: '9015', 4636: '90185', 4637: '90258', 4638: '90312', 4639: '9035', 4640: '9041', 4641: '90413', 4642: '90415', 4643: '90418', 4644: '90442', 4645: '90487', 4646: '90590', 4647: '90595', 4648: '90692', 4649: '90710', 4650: '90719', 4651: '90746', 4652: '90756', 4653: '90785', 4654: '90962', 4655: '9099', 4656: '91028', 4657: '91043', 4658: '91055', 4659: '91058', 4660: '91075', 4661: '91102', 4662: '91112', 4663: '91153', 4664: '9116', 4665: '91179', 4666: '9120', 4667: '91220', 4668: '91299', 4669: '91322', 4670: '9136', 4671: '91421', 4672: '91519', 4673: '91548', 4674: '91754', 4675: '91755', 4676: '91766', 4677: '91776', 4678: '91822', 4679: '91875', 4680: '91877', 4681: '91878', 4682: '9190', 4683: '9195', 4684: '91954', 4685: '92010', 4686: '92094', 4687: '92095', 4688: '92126', 4689: '92151', 4690: '92267', 4691: '92269', 4692: '92277', 4693: '92296', 4694: '92301', 4695: '92314', 4696: '92342', 4697: '92348', 4698: '92353', 4699: '92378', 4700: '92492', 4701: '92571', 4702: '92580', 4703: '92668', 4704: '92669', 4705: '9270', 4706: '92888', 4707: '92938', 4708: '92939', 4709: '92947', 4710: '92965', 4711: '93043', 4712: '93056', 4713: '93063', 4714: '93072', 4715: '93075', 4716: '93164', 4717: '9321', 4718: '93220', 4719: '93245', 4720: '93260', 4721: '9334', 4722: '93388', 4723: '93490', 4724: '93508', 4725: '93514', 4726: '93656', 4727: '93675', 4728: '93728', 4729: '93731', 4730: '9374', 4731: '93781', 4732: '93789', 4733: '93825', 4734: '93836', 4735: '93857', 4736: '93953', 4737: '94059', 4738: '94153', 4739: '94187', 4740: '94276', 4741: '94347', 4742: '9440', 4743: '94409', 4744: '94430', 4745: '94514', 4746: '94527', 4747: '94664', 4748: '94665', 4749: '94702', 4750: '94722', 4751: '94769', 4752: '94925', 4753: '94982', 4754: '94984', 4755: '95004', 4756: '95077', 4757: '95111', 4758: '9515', 4759: '95193', 4760: '95197', 4761: '95257', 4762: '95335', 4763: '9535', 4764: '95353', 4765: '95387', 4766: '95437', 4767: '95443', 4768: '95452', 4769: '95551', 4770: '95555', 4771: '95577', 4772: '9562', 4773: '95625', 4774: '9565', 4775: '95659', 4776: '95683', 4777: '95696', 4778: '95699', 4779: '95707', 4780: '95722', 4781: '95735', 4782: '95755', 4783: '95801', 4784: '95848', 4785: '95925', 4786: '96015', 4787: '96021', 4788: '96059', 4789: '96070', 4790: '96081', 4791: '96085', 4792: '96089', 4793: '96105', 4794: '96147', 4795: '96184', 4796: '96192', 4797: '96247', 4798: '96332', 4799: '96355', 4800: '96440', 4801: '96443', 4802: '96444', 4803: '96463', 4804: '96480', 4805: '96499', 4806: '9654', 4807: '96565', 4808: '96573', 4809: '96672', 4810: '967', 4811: '96732', 4812: '96826', 4813: '96832', 4814: '9685', 4815: '96852', 4816: '96863', 4817: '96868', 4818: '96879', 4819: '96882', 4820: '9691', 4821: '96955', 4822: '97008', 4823: '9702', 4824: '97058', 4825: '97066', 4826: '97080', 4827: '97084', 4828: '97123', 4829: '97175', 4830: '97216', 4831: '9727', 4832: '97271', 4833: '97330', 4834: '9735', 4835: '9737', 4836: '9740', 4837: '97409', 4838: '97439', 4839: '97461', 4840: '97498', 4841: '97565', 4842: '97620', 4843: '97638', 4844: '97673', 4845: '97695', 4846: '97794', 4847: '97795', 4848: '97818', 4849: '97951', 4850: '98025', 4851: '9814', 4852: '98208', 4853: '98424', 4854: '98481', 4855: '98551', 4856: '98601', 4857: '9863', 4858: '98643', 4859: '98661', 4860: '98685', 4861: '9869', 4862: '98690', 4863: '98724', 4864: '98731', 4865: '98762', 4866: '98888', 4867: '98925', 4868: '98928', 4869: '98963', 4870: '98992', 4871: '99023', 4872: '99058', 4873: '99115', 4874: '99121', 4875: '99175', 4876: '99188', 4877: '99204', 4878: '99263', 4879: '99265', 4880: '99270', 4881: '99293', 4882: '99299', 4883: '99301', 4884: '99305', 4885: '99351', 4886: '99365', 4887: '99378', 4888: '99492', 4889: '99496', 4890: '995', 4891: '9956', 4892: '9962', 4893: '99629', 4894: '99630', 4895: '9966', 4896: '99714', 4897: '99737', 4898: '99760', 4899: '99772', 4900: '998', 4901: '99868', 4902: '99902', 4903: '99912', 4904: '99953', 4905: '99966', 4906: '99978', 4907: '99996'}\n"
     ]
    }
   ],
   "source": [
    "indices_to_class_labels_dict = {value : key for key, value in train_generator.class_indices.items()}\n",
    "print(len(indices_to_class_labels_dict))\n",
    "print(indices_to_class_labels_dict)\n",
    "with open(\"group5_indices_to_class_labels_dict.json\", \"wb\") as pickle_file:\n",
    "    pickle.dump(indices_to_class_labels_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks and Fitting Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFolder = 'checkpoints_group5'\n",
    "if not os.path.exists(outputFolder):\n",
    "    os.makedirs(outputFolder)\n",
    "checkpoint_filepath=outputFolder+\"/model-{epoch:02d}-{val_acc:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath, monitor='val_acc', verbose=1, mode='max',\n",
    "    save_best_only=True, save_weights_only=True,\n",
    "    save_frequency='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-4c3d897e268e>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 8.0837 - acc: 0.0051\n",
      "Epoch 00001: val_acc improved from -inf to 0.04786, saving model to checkpoints_group5/model-01-0.05.hdf5\n",
      "1676/1676 [==============================] - 524s 312ms/step - loss: 8.0837 - acc: 0.0051 - val_loss: 6.5714 - val_acc: 0.0479\n",
      "Epoch 2/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 5.6105 - acc: 0.1219\n",
      "Epoch 00002: val_acc improved from 0.04786 to 0.23588, saving model to checkpoints_group5/model-02-0.24.hdf5\n",
      "1676/1676 [==============================] - 527s 315ms/step - loss: 5.6105 - acc: 0.1219 - val_loss: 4.7763 - val_acc: 0.2359\n",
      "Epoch 3/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 4.0996 - acc: 0.2832\n",
      "Epoch 00003: val_acc improved from 0.23588 to 0.34127, saving model to checkpoints_group5/model-03-0.34.hdf5\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 4.0996 - acc: 0.2832 - val_loss: 3.9808 - val_acc: 0.3413\n",
      "Epoch 4/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 3.3205 - acc: 0.3849\n",
      "Epoch 00004: val_acc improved from 0.34127 to 0.39947, saving model to checkpoints_group5/model-04-0.40.hdf5\n",
      "1676/1676 [==============================] - 529s 316ms/step - loss: 3.3205 - acc: 0.3849 - val_loss: 3.6105 - val_acc: 0.3995\n",
      "Epoch 5/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 2.8092 - acc: 0.4567\n",
      "Epoch 00005: val_acc improved from 0.39947 to 0.40658, saving model to checkpoints_group5/model-05-0.41.hdf5\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 2.8092 - acc: 0.4567 - val_loss: 3.6579 - val_acc: 0.4066\n",
      "Epoch 6/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 2.4259 - acc: 0.5134\n",
      "Epoch 00006: val_acc did not improve from 0.40658\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 2.4259 - acc: 0.5134 - val_loss: 3.7718 - val_acc: 0.3964\n",
      "Epoch 7/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 2.1129 - acc: 0.5638\n",
      "Epoch 00007: val_acc improved from 0.40658 to 0.46617, saving model to checkpoints_group5/model-07-0.47.hdf5\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 2.1129 - acc: 0.5638 - val_loss: 3.2989 - val_acc: 0.4662\n",
      "Epoch 8/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 1.8537 - acc: 0.6044\n",
      "Epoch 00008: val_acc improved from 0.46617 to 0.50728, saving model to checkpoints_group5/model-08-0.51.hdf5\n",
      "1676/1676 [==============================] - 527s 314ms/step - loss: 1.8537 - acc: 0.6044 - val_loss: 3.0489 - val_acc: 0.5073\n",
      "Epoch 9/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 1.6431 - acc: 0.6402\n",
      "Epoch 00009: val_acc improved from 0.50728 to 0.50868, saving model to checkpoints_group5/model-09-0.51.hdf5\n",
      "1676/1676 [==============================] - 527s 314ms/step - loss: 1.6431 - acc: 0.6402 - val_loss: 3.0602 - val_acc: 0.5087\n",
      "Epoch 10/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 1.4538 - acc: 0.6738\n",
      "Epoch 00010: val_acc did not improve from 0.50868\n",
      "1676/1676 [==============================] - 527s 314ms/step - loss: 1.4538 - acc: 0.6738 - val_loss: 3.2510 - val_acc: 0.5063\n",
      "Epoch 11/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 1.2856 - acc: 0.7031\n",
      "Epoch 00011: val_acc did not improve from 0.50868\n",
      "1676/1676 [==============================] - 530s 316ms/step - loss: 1.2856 - acc: 0.7031 - val_loss: 3.3011 - val_acc: 0.5037\n",
      "Epoch 12/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 1.1507 - acc: 0.7289\n",
      "Epoch 00012: val_acc improved from 0.50868 to 0.52801, saving model to checkpoints_group5/model-12-0.53.hdf5\n",
      "1676/1676 [==============================] - 531s 317ms/step - loss: 1.1507 - acc: 0.7289 - val_loss: 3.1382 - val_acc: 0.5280\n",
      "Epoch 13/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 1.0181 - acc: 0.7544\n",
      "Epoch 00013: val_acc did not improve from 0.52801\n",
      "1676/1676 [==============================] - 530s 317ms/step - loss: 1.0181 - acc: 0.7544 - val_loss: 3.4934 - val_acc: 0.5002\n",
      "Epoch 14/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.9227 - acc: 0.7749\n",
      "Epoch 00014: val_acc did not improve from 0.52801\n",
      "1676/1676 [==============================] - 530s 316ms/step - loss: 0.9227 - acc: 0.7749 - val_loss: 3.3393 - val_acc: 0.5138\n",
      "Epoch 15/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.8238 - acc: 0.7967\n",
      "Epoch 00015: val_acc did not improve from 0.52801\n",
      "1676/1676 [==============================] - 530s 316ms/step - loss: 0.8238 - acc: 0.7967 - val_loss: 3.5910 - val_acc: 0.4940\n",
      "Epoch 16/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.7465 - acc: 0.8110\n",
      "Epoch 00016: val_acc did not improve from 0.52801\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 0.7465 - acc: 0.8110 - val_loss: 3.3884 - val_acc: 0.5193\n",
      "Epoch 17/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.6690 - acc: 0.8296\n",
      "Epoch 00017: val_acc improved from 0.52801 to 0.53430, saving model to checkpoints_group5/model-17-0.53.hdf5\n",
      "1676/1676 [==============================] - 527s 315ms/step - loss: 0.6690 - acc: 0.8296 - val_loss: 3.4526 - val_acc: 0.5343\n",
      "Epoch 18/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.6067 - acc: 0.8430\n",
      "Epoch 00018: val_acc did not improve from 0.53430\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 0.6067 - acc: 0.8430 - val_loss: 3.7115 - val_acc: 0.5113\n",
      "Epoch 19/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.5532 - acc: 0.8570\n",
      "Epoch 00019: val_acc did not improve from 0.53430\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 0.5532 - acc: 0.8570 - val_loss: 3.6564 - val_acc: 0.5181\n",
      "Epoch 20/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.5092 - acc: 0.8666\n",
      "Epoch 00020: val_acc improved from 0.53430 to 0.54186, saving model to checkpoints_group5/model-20-0.54.hdf5\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 0.5092 - acc: 0.8666 - val_loss: 3.4489 - val_acc: 0.5419\n",
      "Epoch 21/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.4722 - acc: 0.8762\n",
      "Epoch 00021: val_acc did not improve from 0.54186\n",
      "1676/1676 [==============================] - 527s 314ms/step - loss: 0.4722 - acc: 0.8762 - val_loss: 4.0670 - val_acc: 0.4704\n",
      "Epoch 22/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.4340 - acc: 0.8851\n",
      "Epoch 00022: val_acc did not improve from 0.54186\n",
      "1676/1676 [==============================] - 527s 315ms/step - loss: 0.4340 - acc: 0.8851 - val_loss: 3.8914 - val_acc: 0.5111\n",
      "Epoch 23/100\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.3994 - acc: 0.8943\n",
      "Epoch 00023: val_acc improved from 0.54186 to 0.55506, saving model to checkpoints_group5/model-23-0.56.hdf5\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 0.3994 - acc: 0.8943 - val_loss: 3.5467 - val_acc: 0.5551\n",
      "Epoch 24/100\n",
      "1591/1676 [===========================>..] - ETA: 24s - loss: 0.3735 - acc: 0.9005"
     ]
    }
   ],
   "source": [
    "# history = model.fit_generator(\n",
    "#     generator = train_generator,\n",
    "#     steps_per_epoch = train_generator.n // batch_size,\n",
    "#     validation_data = valid_generator,\n",
    "#     validation_steps = valid_generator.n // batch_size,\n",
    "#     callbacks=[checkpoint_callback],\n",
    "#     epochs = 100,\n",
    "#     workers = 8,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"checkpoints_group5/model-23-0.56.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-535389678ccd>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.4141 - acc: 0.8877\n",
      "Epoch 00001: val_acc improved from -inf to 0.50751, saving model to checkpoints_group5/model-01-0.51.hdf5\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.4141 - acc: 0.8877 - val_loss: 4.0620 - val_acc: 0.5075\n",
      "Epoch 2/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.3599 - acc: 0.9022\n",
      "Epoch 00002: val_acc improved from 0.50751 to 0.52498, saving model to checkpoints_group5/model-02-0.52.hdf5\n",
      "1676/1676 [==============================] - 530s 316ms/step - loss: 0.3599 - acc: 0.9022 - val_loss: 3.9398 - val_acc: 0.5250\n",
      "Epoch 3/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.3367 - acc: 0.9096\n",
      "Epoch 00003: val_acc improved from 0.52498 to 0.55030, saving model to checkpoints_group5/model-03-0.55.hdf5\n",
      "1676/1676 [==============================] - 530s 316ms/step - loss: 0.3367 - acc: 0.9096 - val_loss: 3.7822 - val_acc: 0.5503\n",
      "Epoch 4/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.3208 - acc: 0.9126\n",
      "Epoch 00004: val_acc did not improve from 0.55030\n",
      "1676/1676 [==============================] - 529s 316ms/step - loss: 0.3208 - acc: 0.9126 - val_loss: 3.7057 - val_acc: 0.5458\n",
      "Epoch 5/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2992 - acc: 0.9191\n",
      "Epoch 00005: val_acc did not improve from 0.55030\n",
      "1676/1676 [==============================] - 527s 315ms/step - loss: 0.2992 - acc: 0.9191 - val_loss: 3.8547 - val_acc: 0.5491\n",
      "Epoch 6/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2795 - acc: 0.9236\n",
      "Epoch 00006: val_acc did not improve from 0.55030\n",
      "1676/1676 [==============================] - 525s 314ms/step - loss: 0.2795 - acc: 0.9236 - val_loss: 3.9786 - val_acc: 0.5198\n",
      "Epoch 7/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2660 - acc: 0.9282\n",
      "Epoch 00007: val_acc did not improve from 0.55030\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.2660 - acc: 0.9282 - val_loss: 4.0902 - val_acc: 0.5163\n",
      "Epoch 8/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2566 - acc: 0.9302\n",
      "Epoch 00008: val_acc improved from 0.55030 to 0.56112, saving model to checkpoints_group5/model-08-0.56.hdf5\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.2566 - acc: 0.9302 - val_loss: 3.8156 - val_acc: 0.5611\n",
      "Epoch 9/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2405 - acc: 0.9345\n",
      "Epoch 00009: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.2405 - acc: 0.9345 - val_loss: 4.5785 - val_acc: 0.4970\n",
      "Epoch 10/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2334 - acc: 0.9360\n",
      "Epoch 00010: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.2334 - acc: 0.9360 - val_loss: 4.2504 - val_acc: 0.5340\n",
      "Epoch 11/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2256 - acc: 0.9392\n",
      "Epoch 00011: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.2256 - acc: 0.9392 - val_loss: 3.9881 - val_acc: 0.5532\n",
      "Epoch 12/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2154 - acc: 0.9412\n",
      "Epoch 00012: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.2154 - acc: 0.9412 - val_loss: 4.1320 - val_acc: 0.5238\n",
      "Epoch 13/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2062 - acc: 0.9434\n",
      "Epoch 00013: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 525s 314ms/step - loss: 0.2062 - acc: 0.9434 - val_loss: 4.2244 - val_acc: 0.5396\n",
      "Epoch 14/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1985 - acc: 0.9460\n",
      "Epoch 00014: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.1985 - acc: 0.9460 - val_loss: 3.9176 - val_acc: 0.5529\n",
      "Epoch 15/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1935 - acc: 0.9467\n",
      "Epoch 00015: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.1935 - acc: 0.9467 - val_loss: 4.4363 - val_acc: 0.5025\n",
      "Epoch 16/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1840 - acc: 0.9495\n",
      "Epoch 00016: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.1840 - acc: 0.9495 - val_loss: 4.1806 - val_acc: 0.5415\n",
      "Epoch 17/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1795 - acc: 0.9503\n",
      "Epoch 00017: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.1795 - acc: 0.9503 - val_loss: 4.1447 - val_acc: 0.5535\n",
      "Epoch 18/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1733 - acc: 0.9525\n",
      "Epoch 00018: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 0.1733 - acc: 0.9525 - val_loss: 4.2391 - val_acc: 0.5401\n",
      "Epoch 19/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1684 - acc: 0.9536\n",
      "Epoch 00019: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 530s 316ms/step - loss: 0.1684 - acc: 0.9536 - val_loss: 4.3352 - val_acc: 0.5377\n",
      "Epoch 20/76\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1636 - acc: 0.9553\n",
      "Epoch 00020: val_acc did not improve from 0.56112\n",
      "1676/1676 [==============================] - 530s 316ms/step - loss: 0.1636 - acc: 0.9553 - val_loss: 4.3350 - val_acc: 0.5382\n",
      "Epoch 21/76\n",
      "1194/1676 [====================>.........] - ETA: 2:19 - loss: 0.1551 - acc: 0.9575"
     ]
    }
   ],
   "source": [
    "# history = model.fit_generator(\n",
    "#     generator = train_generator,\n",
    "#     steps_per_epoch = train_generator.n // batch_size,\n",
    "#     validation_data = valid_generator,\n",
    "#     validation_steps = valid_generator.n // batch_size,\n",
    "#     callbacks=[checkpoint_callback],\n",
    "#     epochs = 76,\n",
    "#     workers = 8,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"checkpoints_group5/model-08-0.56.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-670cd2fe64cd>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2695 - acc: 0.9264\n",
      "Epoch 00001: val_acc improved from -inf to 0.51872, saving model to checkpoints_group5/model-01-0.52.hdf5\n",
      "1676/1676 [==============================] - 521s 311ms/step - loss: 0.2695 - acc: 0.9264 - val_loss: 4.4058 - val_acc: 0.5187\n",
      "Epoch 2/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2353 - acc: 0.9355\n",
      "Epoch 00002: val_acc improved from 0.51872 to 0.55649, saving model to checkpoints_group5/model-02-0.56.hdf5\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.2353 - acc: 0.9355 - val_loss: 4.0190 - val_acc: 0.5565\n",
      "Epoch 3/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2286 - acc: 0.9370\n",
      "Epoch 00003: val_acc did not improve from 0.55649\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.2286 - acc: 0.9370 - val_loss: 4.2318 - val_acc: 0.5184\n",
      "Epoch 4/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2207 - acc: 0.9394\n",
      "Epoch 00004: val_acc did not improve from 0.55649\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.2207 - acc: 0.9394 - val_loss: 4.2439 - val_acc: 0.5134\n",
      "Epoch 5/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2090 - acc: 0.9422\n",
      "Epoch 00005: val_acc did not improve from 0.55649\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.2090 - acc: 0.9422 - val_loss: 4.0973 - val_acc: 0.5557\n",
      "Epoch 6/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.2020 - acc: 0.9449\n",
      "Epoch 00006: val_acc did not improve from 0.55649\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.2020 - acc: 0.9449 - val_loss: 4.0923 - val_acc: 0.5379\n",
      "Epoch 7/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1954 - acc: 0.9460\n",
      "Epoch 00007: val_acc did not improve from 0.55649\n",
      "1676/1676 [==============================] - 522s 312ms/step - loss: 0.1954 - acc: 0.9460 - val_loss: 4.7806 - val_acc: 0.5058\n",
      "Epoch 8/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1895 - acc: 0.9475\n",
      "Epoch 00008: val_acc did not improve from 0.55649\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.1895 - acc: 0.9475 - val_loss: 4.0468 - val_acc: 0.5495\n",
      "Epoch 9/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1811 - acc: 0.9501\n",
      "Epoch 00009: val_acc improved from 0.55649 to 0.56027, saving model to checkpoints_group5/model-09-0.56.hdf5\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.1811 - acc: 0.9501 - val_loss: 3.9384 - val_acc: 0.5603\n",
      "Epoch 10/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1739 - acc: 0.9521\n",
      "Epoch 00010: val_acc did not improve from 0.56027\n",
      "1676/1676 [==============================] - 522s 312ms/step - loss: 0.1739 - acc: 0.9521 - val_loss: 3.9766 - val_acc: 0.5518\n",
      "Epoch 11/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1703 - acc: 0.9527\n",
      "Epoch 00011: val_acc did not improve from 0.56027\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.1703 - acc: 0.9527 - val_loss: 4.2891 - val_acc: 0.5196\n",
      "Epoch 12/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1683 - acc: 0.9538\n",
      "Epoch 00012: val_acc did not improve from 0.56027\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1683 - acc: 0.9538 - val_loss: 4.3730 - val_acc: 0.5361\n",
      "Epoch 13/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1602 - acc: 0.9557\n",
      "Epoch 00013: val_acc improved from 0.56027 to 0.56293, saving model to checkpoints_group5/model-13-0.56.hdf5\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.1602 - acc: 0.9557 - val_loss: 4.1464 - val_acc: 0.5629\n",
      "Epoch 14/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1574 - acc: 0.9553\n",
      "Epoch 00014: val_acc did not improve from 0.56293\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.1574 - acc: 0.9553 - val_loss: 4.0485 - val_acc: 0.5602\n",
      "Epoch 15/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1477 - acc: 0.9592\n",
      "Epoch 00015: val_acc did not improve from 0.56293\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.1477 - acc: 0.9592 - val_loss: 4.3658 - val_acc: 0.5417\n",
      "Epoch 16/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1468 - acc: 0.9597\n",
      "Epoch 00016: val_acc did not improve from 0.56293\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.1468 - acc: 0.9597 - val_loss: 4.2589 - val_acc: 0.5592\n",
      "Epoch 17/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1429 - acc: 0.9608\n",
      "Epoch 00017: val_acc did not improve from 0.56293\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.1429 - acc: 0.9608 - val_loss: 4.5173 - val_acc: 0.5417\n",
      "Epoch 18/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1414 - acc: 0.9610\n",
      "Epoch 00018: val_acc did not improve from 0.56293\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.1414 - acc: 0.9610 - val_loss: 4.3419 - val_acc: 0.5575\n",
      "Epoch 19/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1380 - acc: 0.9613\n",
      "Epoch 00019: val_acc did not improve from 0.56293\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.1380 - acc: 0.9613 - val_loss: 4.2829 - val_acc: 0.5462\n",
      "Epoch 20/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1316 - acc: 0.9641\n",
      "Epoch 00020: val_acc improved from 0.56293 to 0.56418, saving model to checkpoints_group5/model-20-0.56.hdf5\n",
      "1676/1676 [==============================] - 526s 314ms/step - loss: 0.1316 - acc: 0.9641 - val_loss: 4.2170 - val_acc: 0.5642\n",
      "Epoch 21/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1299 - acc: 0.9642\n",
      "Epoch 00021: val_acc did not improve from 0.56418\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.1299 - acc: 0.9642 - val_loss: 4.3459 - val_acc: 0.5426\n",
      "Epoch 22/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1274 - acc: 0.9645\n",
      "Epoch 00022: val_acc did not improve from 0.56418\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.1274 - acc: 0.9645 - val_loss: 4.2568 - val_acc: 0.5525\n",
      "Epoch 23/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1233 - acc: 0.9656\n",
      "Epoch 00023: val_acc did not improve from 0.56418\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.1233 - acc: 0.9656 - val_loss: 4.1087 - val_acc: 0.5584\n",
      "Epoch 24/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1199 - acc: 0.9669\n",
      "Epoch 00024: val_acc did not improve from 0.56418\n",
      "1676/1676 [==============================] - 524s 312ms/step - loss: 0.1199 - acc: 0.9669 - val_loss: 4.7472 - val_acc: 0.5410\n",
      "Epoch 25/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1216 - acc: 0.9662\n",
      "Epoch 00025: val_acc did not improve from 0.56418\n",
      "1676/1676 [==============================] - 524s 312ms/step - loss: 0.1216 - acc: 0.9662 - val_loss: 4.5398 - val_acc: 0.5403\n",
      "Epoch 26/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1158 - acc: 0.9675\n",
      "Epoch 00026: val_acc improved from 0.56418 to 0.57007, saving model to checkpoints_group5/model-26-0.57.hdf5\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1158 - acc: 0.9675 - val_loss: 4.4176 - val_acc: 0.5701\n",
      "Epoch 27/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1128 - acc: 0.9689\n",
      "Epoch 00027: val_acc did not improve from 0.57007\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1128 - acc: 0.9689 - val_loss: 4.6078 - val_acc: 0.5475\n",
      "Epoch 28/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1122 - acc: 0.9692\n",
      "Epoch 00028: val_acc improved from 0.57007 to 0.57473, saving model to checkpoints_group5/model-28-0.57.hdf5\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1122 - acc: 0.9692 - val_loss: 4.2009 - val_acc: 0.5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1098 - acc: 0.9700\n",
      "Epoch 00029: val_acc did not improve from 0.57473\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1098 - acc: 0.9700 - val_loss: 4.5453 - val_acc: 0.5510\n",
      "Epoch 30/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1118 - acc: 0.9685\n",
      "Epoch 00030: val_acc did not improve from 0.57473\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1118 - acc: 0.9685 - val_loss: 4.4129 - val_acc: 0.5672\n",
      "Epoch 31/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1010 - acc: 0.9722\n",
      "Epoch 00031: val_acc did not improve from 0.57473\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1010 - acc: 0.9722 - val_loss: 4.4251 - val_acc: 0.5669\n",
      "Epoch 32/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1042 - acc: 0.9702\n",
      "Epoch 00032: val_acc did not improve from 0.57473\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1042 - acc: 0.9702 - val_loss: 4.5528 - val_acc: 0.5477\n",
      "Epoch 33/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1029 - acc: 0.9708\n",
      "Epoch 00033: val_acc improved from 0.57473 to 0.57531, saving model to checkpoints_group5/model-33-0.58.hdf5\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1029 - acc: 0.9708 - val_loss: 4.2519 - val_acc: 0.5753\n",
      "Epoch 34/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1007 - acc: 0.9714\n",
      "Epoch 00034: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1007 - acc: 0.9714 - val_loss: 4.2885 - val_acc: 0.5649\n",
      "Epoch 35/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0957 - acc: 0.9733\n",
      "Epoch 00035: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 524s 312ms/step - loss: 0.0957 - acc: 0.9733 - val_loss: 4.6079 - val_acc: 0.5617\n",
      "Epoch 36/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.1012 - acc: 0.9716\n",
      "Epoch 00036: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.1012 - acc: 0.9716 - val_loss: 4.7207 - val_acc: 0.5454\n",
      "Epoch 37/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0966 - acc: 0.9733\n",
      "Epoch 00037: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.0966 - acc: 0.9733 - val_loss: 4.7569 - val_acc: 0.5387\n",
      "Epoch 38/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0896 - acc: 0.9745\n",
      "Epoch 00038: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 524s 312ms/step - loss: 0.0896 - acc: 0.9745 - val_loss: 5.0255 - val_acc: 0.5424\n",
      "Epoch 39/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0911 - acc: 0.9741\n",
      "Epoch 00039: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 523s 312ms/step - loss: 0.0911 - acc: 0.9741 - val_loss: 4.5248 - val_acc: 0.5535\n",
      "Epoch 40/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0901 - acc: 0.9746\n",
      "Epoch 00040: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 0.0901 - acc: 0.9746 - val_loss: 4.4602 - val_acc: 0.5705\n",
      "Epoch 41/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0915 - acc: 0.9742\n",
      "Epoch 00041: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 527s 314ms/step - loss: 0.0915 - acc: 0.9742 - val_loss: 5.0900 - val_acc: 0.5227\n",
      "Epoch 42/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0867 - acc: 0.9756\n",
      "Epoch 00042: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 527s 315ms/step - loss: 0.0867 - acc: 0.9756 - val_loss: 4.4333 - val_acc: 0.5746\n",
      "Epoch 43/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0865 - acc: 0.9753\n",
      "Epoch 00043: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 527s 315ms/step - loss: 0.0865 - acc: 0.9753 - val_loss: 5.7122 - val_acc: 0.4857\n",
      "Epoch 44/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0842 - acc: 0.9766\n",
      "Epoch 00044: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 527s 315ms/step - loss: 0.0842 - acc: 0.9766 - val_loss: 4.4072 - val_acc: 0.5735\n",
      "Epoch 45/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0848 - acc: 0.9760\n",
      "Epoch 00045: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 525s 314ms/step - loss: 0.0848 - acc: 0.9760 - val_loss: 4.9605 - val_acc: 0.5465\n",
      "Epoch 46/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0808 - acc: 0.9769\n",
      "Epoch 00046: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 524s 312ms/step - loss: 0.0808 - acc: 0.9769 - val_loss: 4.8977 - val_acc: 0.5390\n",
      "Epoch 47/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0808 - acc: 0.9769\n",
      "Epoch 00047: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 524s 313ms/step - loss: 0.0808 - acc: 0.9769 - val_loss: 5.4490 - val_acc: 0.5080\n",
      "Epoch 48/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0790 - acc: 0.9776\n",
      "Epoch 00048: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 525s 313ms/step - loss: 0.0790 - acc: 0.9776 - val_loss: 4.6105 - val_acc: 0.5687\n",
      "Epoch 49/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0780 - acc: 0.9779\n",
      "Epoch 00049: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 528s 315ms/step - loss: 0.0780 - acc: 0.9779 - val_loss: 5.0063 - val_acc: 0.5566\n",
      "Epoch 50/50\n",
      "1676/1676 [==============================] - ETA: 0s - loss: 0.0793 - acc: 0.9773\n",
      "Epoch 00050: val_acc did not improve from 0.57531\n",
      "1676/1676 [==============================] - 527s 315ms/step - loss: 0.0793 - acc: 0.9773 - val_loss: 4.5480 - val_acc: 0.5745\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch = train_generator.n // batch_size,\n",
    "    validation_data = valid_generator,\n",
    "    validation_steps = valid_generator.n // batch_size,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    epochs = 50,\n",
    "    workers = 4,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training accuracy')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsedrfgdffg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
