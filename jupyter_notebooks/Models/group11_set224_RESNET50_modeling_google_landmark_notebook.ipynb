{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50_MODEL=tf.keras.applications.ResNet50(input_shape=(224,224,3),\n",
    "                                               include_top=False,\n",
    "                                               weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ResNet50_MODEL.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv2_block1_0_bn'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNet50_MODEL.layers[15].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in ResNet50_MODEL.layers:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ResNet50_MODEL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "l1_factor = 0.0001\n",
    "l2_factor = 0.001\n",
    "dropout = 0.5\n",
    "learning_rate = 0.0001\n",
    "target_size_image_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "Dropout_Regularization1 (Dro (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4040)              8277960   \n",
      "=================================================================\n",
      "Total params: 31,865,672\n",
      "Trainable params: 31,812,552\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "                                  ResNet50_MODEL,\n",
    "                                  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                  tf.keras.layers.Dropout(dropout, name='Dropout_Regularization1'),\n",
    "                                  tf.keras.layers.Dense(4040, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "train_data_dir = '../datasets/group11_set_224/set_224/train/'\n",
    "valid_data_dir = '../datasets/group11_set_224/set_224/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.25,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 323131 images belonging to 4040 classes.\n"
     ]
    }
   ],
   "source": [
    "#flow training images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=target_size_image_shape,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80800 images belonging to 4040 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=target_size_image_shape,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFolder = 'checkpoints'\n",
    "if not os.path.exists(outputFolder):\n",
    "    os.makedirs(outputFolder)\n",
    "filepath=outputFolder+\"/model-{epoch:02d}-{val_acc:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_acc', verbose=1, mode='max',\n",
    "    save_best_only=True, save_weights_only=False,\n",
    "    save_frequency='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='logs', histogram_freq=1, write_graph=True, write_images=True,\n",
    "    update_freq='epoch', profile_batch=2, embeddings_freq=1,\n",
    "    embeddings_metadata=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-53ee69445caf>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 6.4249 - acc: 0.0746\n",
      "Epoch 00001: val_acc improved from -inf to 0.24037, saving model to checkpoints/model-01-0.24.hdf5\n",
      "5048/5048 [==============================] - 1583s 314ms/step - loss: 6.4249 - acc: 0.0746 - val_loss: 4.4448 - val_acc: 0.2404\n",
      "Epoch 2/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 4.2598 - acc: 0.2554\n",
      "Epoch 00002: val_acc improved from 0.24037 to 0.34421, saving model to checkpoints/model-02-0.34.hdf5\n",
      "5048/5048 [==============================] - 1584s 314ms/step - loss: 4.2598 - acc: 0.2554 - val_loss: 3.7194 - val_acc: 0.3442\n",
      "Epoch 3/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 3.5334 - acc: 0.3498\n",
      "Epoch 00003: val_acc improved from 0.34421 to 0.41134, saving model to checkpoints/model-03-0.41.hdf5\n",
      "5048/5048 [==============================] - 1583s 314ms/step - loss: 3.5334 - acc: 0.3498 - val_loss: 3.1990 - val_acc: 0.4113\n",
      "Epoch 4/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 3.0852 - acc: 0.4119\n",
      "Epoch 00004: val_acc improved from 0.41134 to 0.42868, saving model to checkpoints/model-04-0.43.hdf5\n",
      "5048/5048 [==============================] - 1580s 313ms/step - loss: 3.0852 - acc: 0.4119 - val_loss: 3.1251 - val_acc: 0.4287\n",
      "Epoch 5/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 2.7637 - acc: 0.4587\n",
      "Epoch 00005: val_acc improved from 0.42868 to 0.46501, saving model to checkpoints/model-05-0.47.hdf5\n",
      "5048/5048 [==============================] - 1585s 314ms/step - loss: 2.7637 - acc: 0.4587 - val_loss: 2.9060 - val_acc: 0.4650\n",
      "Epoch 6/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 2.5140 - acc: 0.4960\n",
      "Epoch 00006: val_acc improved from 0.46501 to 0.47758, saving model to checkpoints/model-06-0.48.hdf5\n",
      "5048/5048 [==============================] - 1584s 314ms/step - loss: 2.5140 - acc: 0.4960 - val_loss: 2.8659 - val_acc: 0.4776\n",
      "Epoch 7/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 2.3061 - acc: 0.5276\n",
      "Epoch 00007: val_acc improved from 0.47758 to 0.50159, saving model to checkpoints/model-07-0.50.hdf5\n",
      "5048/5048 [==============================] - 1580s 313ms/step - loss: 2.3061 - acc: 0.5276 - val_loss: 2.7332 - val_acc: 0.5016\n",
      "Epoch 8/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 2.1344 - acc: 0.5557\n",
      "Epoch 00008: val_acc improved from 0.50159 to 0.51779, saving model to checkpoints/model-08-0.52.hdf5\n",
      "5048/5048 [==============================] - 1572s 311ms/step - loss: 2.1344 - acc: 0.5557 - val_loss: 2.6427 - val_acc: 0.5178\n",
      "Epoch 9/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.9831 - acc: 0.5799\n",
      "Epoch 00009: val_acc improved from 0.51779 to 0.53985, saving model to checkpoints/model-09-0.54.hdf5\n",
      "5048/5048 [==============================] - 1567s 310ms/step - loss: 1.9831 - acc: 0.5799 - val_loss: 2.5797 - val_acc: 0.5398\n",
      "Epoch 10/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.8525 - acc: 0.6014\n",
      "Epoch 00010: val_acc did not improve from 0.53985\n",
      "5048/5048 [==============================] - 1566s 310ms/step - loss: 1.8525 - acc: 0.6014 - val_loss: 2.8102 - val_acc: 0.5030\n",
      "Epoch 11/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.7345 - acc: 0.6204\n",
      "Epoch 00011: val_acc improved from 0.53985 to 0.54182, saving model to checkpoints/model-11-0.54.hdf5\n",
      "5048/5048 [==============================] - 1565s 310ms/step - loss: 1.7345 - acc: 0.6204 - val_loss: 2.5609 - val_acc: 0.5418\n",
      "Epoch 12/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.6299 - acc: 0.6385\n",
      "Epoch 00012: val_acc did not improve from 0.54182\n",
      "5048/5048 [==============================] - 1565s 310ms/step - loss: 1.6299 - acc: 0.6385 - val_loss: 2.9147 - val_acc: 0.4975\n",
      "Epoch 13/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.5356 - acc: 0.6545\n",
      "Epoch 00013: val_acc improved from 0.54182 to 0.55055, saving model to checkpoints/model-13-0.55.hdf5\n",
      "5048/5048 [==============================] - 1565s 310ms/step - loss: 1.5356 - acc: 0.6545 - val_loss: 2.5337 - val_acc: 0.5505\n",
      "Epoch 14/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.4430 - acc: 0.6700\n",
      "Epoch 00014: val_acc improved from 0.55055 to 0.55504, saving model to checkpoints/model-14-0.56.hdf5\n",
      "5048/5048 [==============================] - 1565s 310ms/step - loss: 1.4430 - acc: 0.6700 - val_loss: 2.5944 - val_acc: 0.5550\n",
      "Epoch 15/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.3658 - acc: 0.6847\n",
      "Epoch 00015: val_acc did not improve from 0.55504\n",
      "5048/5048 [==============================] - 1564s 310ms/step - loss: 1.3658 - acc: 0.6847 - val_loss: 2.7916 - val_acc: 0.5312\n",
      "Epoch 16/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.2893 - acc: 0.6979\n",
      "Epoch 00016: val_acc did not improve from 0.55504\n",
      "5048/5048 [==============================] - 1564s 310ms/step - loss: 1.2893 - acc: 0.6979 - val_loss: 2.9442 - val_acc: 0.5256\n",
      "Epoch 17/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.2213 - acc: 0.7100\n",
      "Epoch 00017: val_acc improved from 0.55504 to 0.56544, saving model to checkpoints/model-17-0.57.hdf5\n",
      "5048/5048 [==============================] - 1563s 310ms/step - loss: 1.2213 - acc: 0.7100 - val_loss: 2.5659 - val_acc: 0.5654\n",
      "Epoch 18/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.1556 - acc: 0.7229\n",
      "Epoch 00018: val_acc improved from 0.56544 to 0.56935, saving model to checkpoints/model-18-0.57.hdf5\n",
      "5048/5048 [==============================] - 1563s 310ms/step - loss: 1.1556 - acc: 0.7229 - val_loss: 2.6652 - val_acc: 0.5693\n",
      "Epoch 19/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.0963 - acc: 0.7327\n",
      "Epoch 00019: val_acc did not improve from 0.56935\n",
      "5048/5048 [==============================] - 1562s 309ms/step - loss: 1.0963 - acc: 0.7327 - val_loss: 2.6763 - val_acc: 0.5526\n",
      "Epoch 20/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 1.0467 - acc: 0.7431\n",
      "Epoch 00020: val_acc did not improve from 0.56935\n",
      "5048/5048 [==============================] - 1561s 309ms/step - loss: 1.0467 - acc: 0.7431 - val_loss: 2.9044 - val_acc: 0.5448\n",
      "Epoch 21/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.9932 - acc: 0.7535\n",
      "Epoch 00021: val_acc did not improve from 0.56935\n",
      "5048/5048 [==============================] - 1561s 309ms/step - loss: 0.9932 - acc: 0.7535 - val_loss: 2.6874 - val_acc: 0.5583\n",
      "Epoch 22/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.9467 - acc: 0.7629\n",
      "Epoch 00022: val_acc did not improve from 0.56935\n",
      "5048/5048 [==============================] - 1562s 309ms/step - loss: 0.9467 - acc: 0.7629 - val_loss: 3.0134 - val_acc: 0.5434\n",
      "Epoch 23/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.9053 - acc: 0.7710\n",
      "Epoch 00023: val_acc did not improve from 0.56935\n",
      "5048/5048 [==============================] - 1563s 310ms/step - loss: 0.9053 - acc: 0.7710 - val_loss: 2.8714 - val_acc: 0.5404\n",
      "Epoch 24/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.8636 - acc: 0.7791\n",
      "Epoch 00024: val_acc did not improve from 0.56935\n",
      "5048/5048 [==============================] - 1564s 310ms/step - loss: 0.8636 - acc: 0.7791 - val_loss: 3.0372 - val_acc: 0.5442\n",
      "Epoch 25/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.8209 - acc: 0.7879\n",
      "Epoch 00025: val_acc improved from 0.56935 to 0.58591, saving model to checkpoints/model-25-0.59.hdf5\n",
      "5048/5048 [==============================] - 1568s 311ms/step - loss: 0.8209 - acc: 0.7879 - val_loss: 2.7189 - val_acc: 0.5859\n",
      "Epoch 26/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.7886 - acc: 0.7947\n",
      "Epoch 00026: val_acc did not improve from 0.58591\n",
      "5048/5048 [==============================] - 1581s 313ms/step - loss: 0.7886 - acc: 0.7947 - val_loss: 2.8858 - val_acc: 0.5722\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5048/5048 [==============================] - ETA: 0s - loss: 0.7523 - acc: 0.8021\n",
      "Epoch 00027: val_acc did not improve from 0.58591\n",
      "5048/5048 [==============================] - 1575s 312ms/step - loss: 0.7523 - acc: 0.8021 - val_loss: 2.9174 - val_acc: 0.5749\n",
      "Epoch 28/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.7222 - acc: 0.8087\n",
      "Epoch 00028: val_acc did not improve from 0.58591\n",
      "5048/5048 [==============================] - 1584s 314ms/step - loss: 0.7222 - acc: 0.8087 - val_loss: 2.7977 - val_acc: 0.5772\n",
      "Epoch 29/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.6963 - acc: 0.8149\n",
      "Epoch 00029: val_acc did not improve from 0.58591\n",
      "5048/5048 [==============================] - 1586s 314ms/step - loss: 0.6963 - acc: 0.8149 - val_loss: 2.8460 - val_acc: 0.5699\n",
      "Epoch 30/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.6649 - acc: 0.8219\n",
      "Epoch 00030: val_acc did not improve from 0.58591\n",
      "5048/5048 [==============================] - 1576s 312ms/step - loss: 0.6649 - acc: 0.8219 - val_loss: 2.7693 - val_acc: 0.5754\n",
      "Epoch 31/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.6407 - acc: 0.8266\n",
      "Epoch 00031: val_acc improved from 0.58591 to 0.59682, saving model to checkpoints/model-31-0.60.hdf5\n",
      "5048/5048 [==============================] - 1581s 313ms/step - loss: 0.6407 - acc: 0.8266 - val_loss: 2.7377 - val_acc: 0.5968\n",
      "Epoch 32/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.6174 - acc: 0.8323\n",
      "Epoch 00032: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1586s 314ms/step - loss: 0.6174 - acc: 0.8323 - val_loss: 2.9541 - val_acc: 0.5774\n",
      "Epoch 33/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.5911 - acc: 0.8389\n",
      "Epoch 00033: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1588s 315ms/step - loss: 0.5911 - acc: 0.8389 - val_loss: 3.2783 - val_acc: 0.5502\n",
      "Epoch 34/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.5706 - acc: 0.8433\n",
      "Epoch 00034: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.5706 - acc: 0.8433 - val_loss: 2.9444 - val_acc: 0.5935\n",
      "Epoch 35/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.5509 - acc: 0.8485\n",
      "Epoch 00035: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1584s 314ms/step - loss: 0.5509 - acc: 0.8485 - val_loss: 3.0593 - val_acc: 0.5714\n",
      "Epoch 36/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.5313 - acc: 0.8533\n",
      "Epoch 00036: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1592s 315ms/step - loss: 0.5313 - acc: 0.8533 - val_loss: 3.1558 - val_acc: 0.5712\n",
      "Epoch 37/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.5170 - acc: 0.8571\n",
      "Epoch 00037: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1590s 315ms/step - loss: 0.5170 - acc: 0.8571 - val_loss: 3.0855 - val_acc: 0.5803\n",
      "Epoch 38/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.4983 - acc: 0.8607\n",
      "Epoch 00038: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1592s 315ms/step - loss: 0.4983 - acc: 0.8607 - val_loss: 2.9390 - val_acc: 0.5885\n",
      "Epoch 39/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.4805 - acc: 0.8653\n",
      "Epoch 00039: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1593s 316ms/step - loss: 0.4805 - acc: 0.8653 - val_loss: 3.1711 - val_acc: 0.5852\n",
      "Epoch 40/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.4656 - acc: 0.8690\n",
      "Epoch 00040: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1594s 316ms/step - loss: 0.4656 - acc: 0.8690 - val_loss: 3.2741 - val_acc: 0.5729\n",
      "Epoch 41/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.4544 - acc: 0.8719\n",
      "Epoch 00041: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1594s 316ms/step - loss: 0.4544 - acc: 0.8719 - val_loss: 3.6073 - val_acc: 0.5388\n",
      "Epoch 42/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.4423 - acc: 0.8759\n",
      "Epoch 00042: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1588s 315ms/step - loss: 0.4423 - acc: 0.8759 - val_loss: 3.2996 - val_acc: 0.5805\n",
      "Epoch 43/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.4264 - acc: 0.8796\n",
      "Epoch 00043: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.4264 - acc: 0.8796 - val_loss: 3.4219 - val_acc: 0.5557\n",
      "Epoch 44/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.4157 - acc: 0.8819\n",
      "Epoch 00044: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.4157 - acc: 0.8819 - val_loss: 3.4339 - val_acc: 0.5735\n",
      "Epoch 45/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.4038 - acc: 0.8845\n",
      "Epoch 00045: val_acc did not improve from 0.59682\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.4038 - acc: 0.8845 - val_loss: 3.3835 - val_acc: 0.5841\n",
      "Epoch 46/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3932 - acc: 0.8885\n",
      "Epoch 00046: val_acc improved from 0.59682 to 0.60946, saving model to checkpoints/model-46-0.61.hdf5\n",
      "5048/5048 [==============================] - 1583s 314ms/step - loss: 0.3932 - acc: 0.8885 - val_loss: 3.1912 - val_acc: 0.6095\n",
      "Epoch 47/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3852 - acc: 0.8899\n",
      "Epoch 00047: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1583s 314ms/step - loss: 0.3852 - acc: 0.8899 - val_loss: 3.3692 - val_acc: 0.5705\n",
      "Epoch 48/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3760 - acc: 0.8924\n",
      "Epoch 00048: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.3760 - acc: 0.8924 - val_loss: 3.4468 - val_acc: 0.5821\n",
      "Epoch 49/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3676 - acc: 0.8949\n",
      "Epoch 00049: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1583s 314ms/step - loss: 0.3676 - acc: 0.8949 - val_loss: 3.1262 - val_acc: 0.6000\n",
      "Epoch 50/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3586 - acc: 0.8969\n",
      "Epoch 00050: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1583s 314ms/step - loss: 0.3586 - acc: 0.8969 - val_loss: 3.5264 - val_acc: 0.5675\n",
      "Epoch 51/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3483 - acc: 0.9008\n",
      "Epoch 00051: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1583s 314ms/step - loss: 0.3483 - acc: 0.9008 - val_loss: 3.6138 - val_acc: 0.5681\n",
      "Epoch 52/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3407 - acc: 0.9018\n",
      "Epoch 00052: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1588s 315ms/step - loss: 0.3407 - acc: 0.9018 - val_loss: 3.3480 - val_acc: 0.5944\n",
      "Epoch 53/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3351 - acc: 0.9037\n",
      "Epoch 00053: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1592s 315ms/step - loss: 0.3351 - acc: 0.9037 - val_loss: 3.5668 - val_acc: 0.5771\n",
      "Epoch 54/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3245 - acc: 0.9063\n",
      "Epoch 00054: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1583s 313ms/step - loss: 0.3245 - acc: 0.9063 - val_loss: 3.5179 - val_acc: 0.5812\n",
      "Epoch 55/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3186 - acc: 0.9082\n",
      "Epoch 00055: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.3186 - acc: 0.9082 - val_loss: 3.3049 - val_acc: 0.5966\n",
      "Epoch 56/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3131 - acc: 0.9096\n",
      "Epoch 00056: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1581s 313ms/step - loss: 0.3131 - acc: 0.9096 - val_loss: 3.7064 - val_acc: 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3076 - acc: 0.9108\n",
      "Epoch 00057: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1587s 314ms/step - loss: 0.3076 - acc: 0.9108 - val_loss: 3.6127 - val_acc: 0.5774\n",
      "Epoch 58/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.3015 - acc: 0.9120\n",
      "Epoch 00058: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1579s 313ms/step - loss: 0.3015 - acc: 0.9120 - val_loss: 3.7877 - val_acc: 0.5727\n",
      "Epoch 59/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2954 - acc: 0.9139\n",
      "Epoch 00059: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1580s 313ms/step - loss: 0.2954 - acc: 0.9139 - val_loss: 3.4116 - val_acc: 0.5897\n",
      "Epoch 60/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2877 - acc: 0.9166\n",
      "Epoch 00060: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1590s 315ms/step - loss: 0.2877 - acc: 0.9166 - val_loss: 3.7780 - val_acc: 0.5575\n",
      "Epoch 61/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2834 - acc: 0.9178\n",
      "Epoch 00061: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1589s 315ms/step - loss: 0.2834 - acc: 0.9178 - val_loss: 3.5361 - val_acc: 0.5941\n",
      "Epoch 62/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2797 - acc: 0.9180\n",
      "Epoch 00062: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1580s 313ms/step - loss: 0.2797 - acc: 0.9180 - val_loss: 3.6253 - val_acc: 0.5767\n",
      "Epoch 63/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2743 - acc: 0.9201\n",
      "Epoch 00063: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1575s 312ms/step - loss: 0.2743 - acc: 0.9201 - val_loss: 3.2938 - val_acc: 0.6050\n",
      "Epoch 64/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2685 - acc: 0.9215\n",
      "Epoch 00064: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1573s 312ms/step - loss: 0.2685 - acc: 0.9215 - val_loss: 3.4396 - val_acc: 0.6003\n",
      "Epoch 65/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2620 - acc: 0.9232\n",
      "Epoch 00065: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1574s 312ms/step - loss: 0.2620 - acc: 0.9232 - val_loss: 3.5284 - val_acc: 0.5849\n",
      "Epoch 66/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2606 - acc: 0.9236\n",
      "Epoch 00066: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1571s 311ms/step - loss: 0.2606 - acc: 0.9236 - val_loss: 3.8592 - val_acc: 0.5655\n",
      "Epoch 67/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2569 - acc: 0.9249\n",
      "Epoch 00067: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1571s 311ms/step - loss: 0.2569 - acc: 0.9249 - val_loss: 4.0340 - val_acc: 0.5572\n",
      "Epoch 68/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2533 - acc: 0.9263\n",
      "Epoch 00068: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1571s 311ms/step - loss: 0.2533 - acc: 0.9263 - val_loss: 3.4457 - val_acc: 0.6039\n",
      "Epoch 69/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2473 - acc: 0.9274\n",
      "Epoch 00069: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1571s 311ms/step - loss: 0.2473 - acc: 0.9274 - val_loss: 3.3587 - val_acc: 0.6080\n",
      "Epoch 70/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2421 - acc: 0.9288\n",
      "Epoch 00070: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1572s 311ms/step - loss: 0.2421 - acc: 0.9288 - val_loss: 3.9580 - val_acc: 0.5795\n",
      "Epoch 71/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2393 - acc: 0.9299\n",
      "Epoch 00071: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1570s 311ms/step - loss: 0.2393 - acc: 0.9299 - val_loss: 4.1313 - val_acc: 0.5595\n",
      "Epoch 72/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2358 - acc: 0.9311\n",
      "Epoch 00072: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1571s 311ms/step - loss: 0.2358 - acc: 0.9311 - val_loss: 3.5451 - val_acc: 0.6071\n",
      "Epoch 73/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2320 - acc: 0.9326\n",
      "Epoch 00073: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1571s 311ms/step - loss: 0.2320 - acc: 0.9326 - val_loss: 4.0894 - val_acc: 0.5638\n",
      "Epoch 74/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2268 - acc: 0.9331\n",
      "Epoch 00074: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1570s 311ms/step - loss: 0.2268 - acc: 0.9331 - val_loss: 4.0331 - val_acc: 0.5560\n",
      "Epoch 75/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2279 - acc: 0.9333\n",
      "Epoch 00075: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1569s 311ms/step - loss: 0.2279 - acc: 0.9333 - val_loss: 3.6877 - val_acc: 0.6029\n",
      "Epoch 76/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2222 - acc: 0.9350\n",
      "Epoch 00076: val_acc did not improve from 0.60946\n",
      "5048/5048 [==============================] - 1570s 311ms/step - loss: 0.2222 - acc: 0.9350 - val_loss: 3.9448 - val_acc: 0.5778\n",
      "Epoch 77/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2213 - acc: 0.9348\n",
      "Epoch 00077: val_acc improved from 0.60946 to 0.61088, saving model to checkpoints/model-77-0.61.hdf5\n",
      "5048/5048 [==============================] - 1572s 311ms/step - loss: 0.2213 - acc: 0.9348 - val_loss: 3.5987 - val_acc: 0.6109\n",
      "Epoch 78/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2153 - acc: 0.9367\n",
      "Epoch 00078: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1572s 311ms/step - loss: 0.2153 - acc: 0.9367 - val_loss: 3.4781 - val_acc: 0.6062\n",
      "Epoch 79/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2128 - acc: 0.9371\n",
      "Epoch 00079: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1573s 312ms/step - loss: 0.2128 - acc: 0.9371 - val_loss: 3.8642 - val_acc: 0.5851\n",
      "Epoch 80/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2101 - acc: 0.9384\n",
      "Epoch 00080: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1584s 314ms/step - loss: 0.2101 - acc: 0.9384 - val_loss: 4.2178 - val_acc: 0.5626\n",
      "Epoch 81/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2086 - acc: 0.9389\n",
      "Epoch 00081: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1590s 315ms/step - loss: 0.2086 - acc: 0.9389 - val_loss: 3.6036 - val_acc: 0.5949\n",
      "Epoch 82/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2064 - acc: 0.9392\n",
      "Epoch 00082: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1581s 313ms/step - loss: 0.2064 - acc: 0.9392 - val_loss: 3.5632 - val_acc: 0.6093\n",
      "Epoch 83/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2046 - acc: 0.9399\n",
      "Epoch 00083: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1579s 313ms/step - loss: 0.2046 - acc: 0.9399 - val_loss: 3.7019 - val_acc: 0.5964\n",
      "Epoch 84/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.2023 - acc: 0.9405\n",
      "Epoch 00084: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1580s 313ms/step - loss: 0.2023 - acc: 0.9405 - val_loss: 4.0170 - val_acc: 0.5793\n",
      "Epoch 85/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1968 - acc: 0.9418\n",
      "Epoch 00085: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1581s 313ms/step - loss: 0.1968 - acc: 0.9418 - val_loss: 3.8570 - val_acc: 0.5856\n",
      "Epoch 86/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1948 - acc: 0.9431\n",
      "Epoch 00086: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1581s 313ms/step - loss: 0.1948 - acc: 0.9431 - val_loss: 3.6397 - val_acc: 0.5976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1937 - acc: 0.9428\n",
      "Epoch 00087: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1581s 313ms/step - loss: 0.1937 - acc: 0.9428 - val_loss: 3.7903 - val_acc: 0.5819\n",
      "Epoch 88/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1913 - acc: 0.9434\n",
      "Epoch 00088: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.1913 - acc: 0.9434 - val_loss: 3.9202 - val_acc: 0.5979\n",
      "Epoch 89/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1892 - acc: 0.9442\n",
      "Epoch 00089: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.1892 - acc: 0.9442 - val_loss: 4.1572 - val_acc: 0.5781\n",
      "Epoch 90/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1879 - acc: 0.9446\n",
      "Epoch 00090: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1582s 313ms/step - loss: 0.1879 - acc: 0.9446 - val_loss: 4.2559 - val_acc: 0.5613\n",
      "Epoch 91/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1850 - acc: 0.9453\n",
      "Epoch 00091: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1583s 314ms/step - loss: 0.1850 - acc: 0.9453 - val_loss: 4.2690 - val_acc: 0.5680\n",
      "Epoch 92/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1831 - acc: 0.9455\n",
      "Epoch 00092: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1584s 314ms/step - loss: 0.1831 - acc: 0.9455 - val_loss: 3.9971 - val_acc: 0.5889\n",
      "Epoch 93/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1801 - acc: 0.9468\n",
      "Epoch 00093: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1584s 314ms/step - loss: 0.1801 - acc: 0.9468 - val_loss: 4.0841 - val_acc: 0.5834\n",
      "Epoch 94/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1784 - acc: 0.9476\n",
      "Epoch 00094: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1586s 314ms/step - loss: 0.1784 - acc: 0.9476 - val_loss: 3.5933 - val_acc: 0.6026\n",
      "Epoch 95/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1771 - acc: 0.9477\n",
      "Epoch 00095: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1589s 315ms/step - loss: 0.1771 - acc: 0.9477 - val_loss: 3.8939 - val_acc: 0.5939\n",
      "Epoch 96/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1759 - acc: 0.9477\n",
      "Epoch 00096: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1588s 315ms/step - loss: 0.1759 - acc: 0.9477 - val_loss: 4.1783 - val_acc: 0.5891\n",
      "Epoch 97/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1736 - acc: 0.9490\n",
      "Epoch 00097: val_acc did not improve from 0.61088\n",
      "5048/5048 [==============================] - 1589s 315ms/step - loss: 0.1736 - acc: 0.9490 - val_loss: 4.1358 - val_acc: 0.5802\n",
      "Epoch 98/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1724 - acc: 0.9487\n",
      "Epoch 00098: val_acc improved from 0.61088 to 0.61836, saving model to checkpoints/model-98-0.62.hdf5\n",
      "5048/5048 [==============================] - 1590s 315ms/step - loss: 0.1724 - acc: 0.9487 - val_loss: 3.7866 - val_acc: 0.6184\n",
      "Epoch 99/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1675 - acc: 0.9505\n",
      "Epoch 00099: val_acc did not improve from 0.61836\n",
      "5048/5048 [==============================] - 1589s 315ms/step - loss: 0.1675 - acc: 0.9505 - val_loss: 3.8604 - val_acc: 0.6035\n",
      "Epoch 100/100\n",
      "5048/5048 [==============================] - ETA: 0s - loss: 0.1655 - acc: 0.9505\n",
      "Epoch 00100: val_acc did not improve from 0.61836\n",
      "5048/5048 [==============================] - 1593s 316ms/step - loss: 0.1655 - acc: 0.9505 - val_loss: 4.3477 - val_acc: 0.5845\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch = train_generator.n // batch_size,\n",
    "    validation_data = valid_generator,\n",
    "    validation_steps = valid_generator.n // batch_size,\n",
    "    callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "    epochs = 100,\n",
    "    workers = 8,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfsdfsdsdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(history.model, 'group11_set224_resnet50_07202020.hdf5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training accuracy')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsedrfgdffg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_class_labels_dict = {value : key for key, value in train_generator.class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040\n",
      "{0: '100028', 1: '100082', 2: '100192', 3: '100230', 4: '100372', 5: '100407', 6: '100484', 7: '100505', 8: '100533', 9: '100614', 10: '100631', 11: '100674', 12: '100740', 13: '100826', 14: '100895', 15: '10090', 16: '100951', 17: '100955', 18: '100966', 19: '100982', 20: '101043', 21: '101056', 22: '101112', 23: '101128', 24: '101138', 25: '101176', 26: '101241', 27: '101271', 28: '101318', 29: '101339', 30: '101399', 31: '101400', 32: '101457', 33: '101708', 34: '101727', 35: '101857', 36: '101881', 37: '101940', 38: '101946', 39: '102002', 40: '102108', 41: '102146', 42: '102154', 43: '10217', 44: '102178', 45: '102206', 46: '102320', 47: '102329', 48: '102375', 49: '102402', 50: '102416', 51: '10248', 52: '102488', 53: '102511', 54: '102544', 55: '102554', 56: '102566', 57: '102572', 58: '102598', 59: '102621', 60: '102664', 61: '102680', 62: '102709', 63: '102712', 64: '102850', 65: '102904', 66: '102917', 67: '102942', 68: '102943', 69: '102984', 70: '103049', 71: '103082', 72: '103119', 73: '103134', 74: '103143', 75: '103168', 76: '103182', 77: '103232', 78: '103235', 79: '103257', 80: '103295', 81: '103306', 82: '10331', 83: '103311', 84: '103530', 85: '103531', 86: '103532', 87: '103652', 88: '103669', 89: '103709', 90: '103827', 91: '103844', 92: '103848', 93: '103899', 94: '103950', 95: '103963', 96: '104006', 97: '104027', 98: '104030', 99: '104036', 100: '10411', 101: '10412', 102: '104152', 103: '104169', 104: '10419', 105: '104245', 106: '104246', 107: '104330', 108: '104335', 109: '104341', 110: '104373', 111: '104406', 112: '104458', 113: '10447', 114: '104605', 115: '104691', 116: '104696', 117: '104744', 118: '104757', 119: '104792', 120: '104858', 121: '104906', 122: '104916', 123: '105017', 124: '105020', 125: '105024', 126: '105071', 127: '10514', 128: '105153', 129: '105294', 130: '105333', 131: '105337', 132: '105354', 133: '10538', 134: '105384', 135: '105447', 136: '105471', 137: '105496', 138: '105512', 139: '105523', 140: '105526', 141: '105608', 142: '105623', 143: '105627', 144: '105676', 145: '105688', 146: '105694', 147: '105719', 148: '105784', 149: '105858', 150: '10592', 151: '105924', 152: '10598', 153: '106030', 154: '106086', 155: '106126', 156: '106130', 157: '106150', 158: '10640', 159: '106404', 160: '106521', 161: '10656', 162: '106716', 163: '106735', 164: '106761', 165: '106793', 166: '106811', 167: '106852', 168: '106924', 169: '106969', 170: '106989', 171: '107063', 172: '107135', 173: '107164', 174: '107219', 175: '107323', 176: '107332', 177: '107344', 178: '10742', 179: '107463', 180: '107488', 181: '107494', 182: '107511', 183: '107513', 184: '107593', 185: '10762', 186: '107627', 187: '107668', 188: '107673', 189: '107706', 190: '107718', 191: '107750', 192: '107801', 193: '107813', 194: '107854', 195: '107874', 196: '107917', 197: '107973', 198: '108028', 199: '10811', 200: '108129', 201: '108146', 202: '108218', 203: '108283', 204: '108291', 205: '10831', 206: '108327', 207: '108392', 208: '108472', 209: '108502', 210: '108505', 211: '108517', 212: '108591', 213: '108815', 214: '108824', 215: '108862', 216: '108972', 217: '108983', 218: '109013', 219: '109018', 220: '10903', 221: '109034', 222: '109082', 223: '109092', 224: '109153', 225: '109159', 226: '109169', 227: '109220', 228: '109349', 229: '109350', 230: '109382', 231: '109406', 232: '10943', 233: '109455', 234: '109482', 235: '109522', 236: '109544', 237: '109572', 238: '109703', 239: '109743', 240: '109814', 241: '109928', 242: '109963', 243: '109969', 244: '110047', 245: '110098', 246: '1101', 247: '110131', 248: '110153', 249: '110216', 250: '110276', 251: '110283', 252: '110320', 253: '110334', 254: '1104', 255: '110444', 256: '110496', 257: '110504', 258: '110505', 259: '110536', 260: '110560', 261: '110605', 262: '110708', 263: '110873', 264: '110908', 265: '110929', 266: '110986', 267: '111096', 268: '111100', 269: '111105', 270: '111222', 271: '111224', 272: '111255', 273: '111289', 274: '111291', 275: '111306', 276: '111311', 277: '111347', 278: '111382', 279: '111455', 280: '111529', 281: '11153', 282: '111608', 283: '111661', 284: '111699', 285: '111705', 286: '11171', 287: '111747', 288: '111774', 289: '11183', 290: '111834', 291: '112009', 292: '112037', 293: '112095', 294: '112099', 295: '112135', 296: '112189', 297: '112200', 298: '112203', 299: '11230', 300: '112308', 301: '11231', 302: '112481', 303: '112536', 304: '112552', 305: '112565', 306: '112605', 307: '112699', 308: '1127', 309: '112832', 310: '112873', 311: '11289', 312: '112907', 313: '112954', 314: '112969', 315: '113010', 316: '113145', 317: '113162', 318: '113189', 319: '113209', 320: '113253', 321: '113259', 322: '113312', 323: '113396', 324: '113462', 325: '11354', 326: '113557', 327: '113590', 328: '113596', 329: '113630', 330: '113636', 331: '113657', 332: '113693', 333: '113747', 334: '113750', 335: '113769', 336: '113815', 337: '113838', 338: '113846', 339: '113868', 340: '114044', 341: '114046', 342: '114058', 343: '114082', 344: '114090', 345: '114157', 346: '114282', 347: '114289', 348: '114327', 349: '114565', 350: '114600', 351: '114727', 352: '114745', 353: '11477', 354: '114791', 355: '114946', 356: '114957', 357: '115000', 358: '115091', 359: '115348', 360: '115391', 361: '115419', 362: '11544', 363: '115543', 364: '115570', 365: '115598', 366: '115637', 367: '115643', 368: '115658', 369: '115820', 370: '115821', 371: '115836', 372: '115919', 373: '11595', 374: '115979', 375: '115995', 376: '116097', 377: '116118', 378: '116124', 379: '116186', 380: '116255', 381: '116265', 382: '116321', 383: '116359', 384: '116462', 385: '116591', 386: '116601', 387: '116722', 388: '116744', 389: '116800', 390: '117041', 391: '117063', 392: '117093', 393: '117206', 394: '117336', 395: '1174', 396: '117425', 397: '117510', 398: '117563', 399: '117635', 400: '117641', 401: '117783', 402: '117860', 403: '117861', 404: '117874', 405: '117885', 406: '117919', 407: '118008', 408: '118091', 409: '118171', 410: '118353', 411: '118514', 412: '118652', 413: '118661', 414: '118671', 415: '118783', 416: '118789', 417: '118856', 418: '118886', 419: '1189', 420: '118906', 421: '118925', 422: '118985', 423: '119109', 424: '119199', 425: '119216', 426: '119223', 427: '119288', 428: '119481', 429: '119570', 430: '119571', 431: '119585', 432: '119615', 433: '119645', 434: '11971', 435: '11984', 436: '119877', 437: '119915', 438: '119918', 439: '120023', 440: '120035', 441: '120041', 442: '120053', 443: '120075', 444: '120144', 445: '120148', 446: '120213', 447: '120278', 448: '120383', 449: '120442', 450: '120518', 451: '120541', 452: '120558', 453: '120585', 454: '120588', 455: '120680', 456: '120734', 457: '120768', 458: '120848', 459: '120856', 460: '120885', 461: '120891', 462: '120981', 463: '121049', 464: '121054', 465: '121062', 466: '121146', 467: '121165', 468: '121316', 469: '121394', 470: '121417', 471: '121449', 472: '121478', 473: '121494', 474: '121572', 475: '121665', 476: '121703', 477: '121745', 478: '121759', 479: '1218', 480: '12186', 481: '121864', 482: '121873', 483: '121887', 484: '121991', 485: '122022', 486: '122142', 487: '122149', 488: '122228', 489: '122234', 490: '122246', 491: '12226', 492: '122284', 493: '12229', 494: '122367', 495: '122400', 496: '122418', 497: '12244', 498: '122482', 499: '122574', 500: '122610', 501: '122613', 502: '122658', 503: '122737', 504: '122752', 505: '122791', 506: '122800', 507: '122808', 508: '122827', 509: '122852', 510: '122853', 511: '122884', 512: '12291', 513: '123079', 514: '123159', 515: '123225', 516: '123309', 517: '123315', 518: '123411', 519: '123454', 520: '123509', 521: '123558', 522: '1236', 523: '123611', 524: '123698', 525: '123731', 526: '123735', 527: '123742', 528: '123874', 529: '123970', 530: '124046', 531: '124101', 532: '124128', 533: '124183', 534: '124194', 535: '124279', 536: '124406', 537: '124430', 538: '124455', 539: '124460', 540: '124527', 541: '124587', 542: '124620', 543: '124675', 544: '124708', 545: '124838', 546: '124913', 547: '124923', 548: '124924', 549: '124928', 550: '124984', 551: '125020', 552: '125036', 553: '125092', 554: '125099', 555: '12517', 556: '125194', 557: '125236', 558: '125250', 559: '125425', 560: '125483', 561: '125547', 562: '125596', 563: '125631', 564: '125706', 565: '125734', 566: '125765', 567: '125786', 568: '125808', 569: '125831', 570: '125866', 571: '125985', 572: '12600', 573: '126014', 574: '126046', 575: '126061', 576: '126100', 577: '12614', 578: '126159', 579: '126241', 580: '126267', 581: '126285', 582: '126382', 583: '126401', 584: '126465', 585: '126467', 586: '126494', 587: '126501', 588: '126599', 589: '126619', 590: '126637', 591: '126721', 592: '126724', 593: '126729', 594: '126746', 595: '126780', 596: '12679', 597: '126811', 598: '127014', 599: '127020', 600: '127047', 601: '127120', 602: '127155', 603: '127157', 604: '127255', 605: '127272', 606: '127358', 607: '127442', 608: '127496', 609: '127514', 610: '127516', 611: '127523', 612: '127677', 613: '127687', 614: '127699', 615: '127765', 616: '127789', 617: '127894', 618: '127989', 619: '128069', 620: '128151', 621: '128234', 622: '128268', 623: '128314', 624: '128320', 625: '128357', 626: '128383', 627: '128432', 628: '128435', 629: '128599', 630: '128628', 631: '128697', 632: '12877', 633: '128818', 634: '128847', 635: '128875', 636: '128938', 637: '129164', 638: '129191', 639: '129200', 640: '129256', 641: '12927', 642: '129318', 643: '129428', 644: '129459', 645: '129472', 646: '129475', 647: '129495', 648: '129529', 649: '12957', 650: '129609', 651: '129610', 652: '129633', 653: '129722', 654: '129769', 655: '129781', 656: '129851', 657: '129984', 658: '130022', 659: '130039', 660: '13006', 661: '130097', 662: '130102', 663: '1302', 664: '130282', 665: '130330', 666: '130363', 667: '130398', 668: '13047', 669: '130505', 670: '130533', 671: '130537', 672: '13054', 673: '130588', 674: '130614', 675: '130643', 676: '130702', 677: '130834', 678: '130871', 679: '130876', 680: '130906', 681: '130960', 682: '131030', 683: '131050', 684: '131095', 685: '131107', 686: '131112', 687: '13112', 688: '131174', 689: '131176', 690: '131213', 691: '131355', 692: '131429', 693: '131437', 694: '131452', 695: '131503', 696: '131580', 697: '131669', 698: '131748', 699: '131755', 700: '131812', 701: '131820', 702: '132033', 703: '132053', 704: '132075', 705: '132172', 706: '132215', 707: '132497', 708: '132505', 709: '132559', 710: '132567', 711: '132574', 712: '132614', 713: '132623', 714: '132685', 715: '132775', 716: '132787', 717: '132842', 718: '132860', 719: '132871', 720: '132911', 721: '132969', 722: '132978', 723: '133003', 724: '133041', 725: '133066', 726: '133147', 727: '133179', 728: '133226', 729: '13328', 730: '133433', 731: '133454', 732: '133530', 733: '133540', 734: '133550', 735: '133575', 736: '133576', 737: '133586', 738: '133634', 739: '133681', 740: '133734', 741: '133800', 742: '133933', 743: '134012', 744: '134019', 745: '13410', 746: '134125', 747: '134193', 748: '134202', 749: '134210', 750: '134217', 751: '134235', 752: '134262', 753: '134263', 754: '134276', 755: '134303', 756: '134309', 757: '134311', 758: '134316', 759: '134456', 760: '134466', 761: '1345', 762: '134501', 763: '134578', 764: '134587', 765: '1346', 766: '134676', 767: '13471', 768: '134783', 769: '134844', 770: '134892', 771: '135169', 772: '135203', 773: '135239', 774: '13524', 775: '135240', 776: '135304', 777: '135432', 778: '135467', 779: '135503', 780: '135533', 781: '135560', 782: '135591', 783: '135601', 784: '135613', 785: '135648', 786: '135653', 787: '135663', 788: '135677', 789: '135747', 790: '135753', 791: '135767', 792: '135798', 793: '135855', 794: '135993', 795: '136', 796: '136000', 797: '136080', 798: '136090', 799: '136093', 800: '136212', 801: '136221', 802: '136228', 803: '136253', 804: '136302', 805: '136409', 806: '136462', 807: '136468', 808: '13665', 809: '136675', 810: '136698', 811: '136736', 812: '136752', 813: '136755', 814: '136776', 815: '136801', 816: '136821', 817: '13684', 818: '136847', 819: '13685', 820: '136886', 821: '136906', 822: '137043', 823: '137117', 824: '137184', 825: '137203', 826: '137270', 827: '137295', 828: '13738', 829: '137412', 830: '137447', 831: '13755', 832: '137558', 833: '137560', 834: '137574', 835: '137624', 836: '137728', 837: '137751', 838: '137783', 839: '137830', 840: '137845', 841: '137910', 842: '137999', 843: '138033', 844: '138037', 845: '138055', 846: '13807', 847: '138132', 848: '138140', 849: '138176', 850: '138208', 851: '138247', 852: '138270', 853: '138395', 854: '138420', 855: '138471', 856: '138518', 857: '138526', 858: '13853', 859: '138599', 860: '138602', 861: '138627', 862: '138635', 863: '13866', 864: '138662', 865: '138734', 866: '138736', 867: '138775', 868: '13878', 869: '138853', 870: '138947', 871: '138974', 872: '139002', 873: '139018', 874: '139038', 875: '139139', 876: '139157', 877: '139159', 878: '139186', 879: '139245', 880: '139256', 881: '1393', 882: '13935', 883: '139350', 884: '139351', 885: '13943', 886: '139457', 887: '13954', 888: '139557', 889: '139565', 890: '139667', 891: '139671', 892: '139707', 893: '139721', 894: '13978', 895: '139785', 896: '139859', 897: '139873', 898: '139894', 899: '139911', 900: '139916', 901: '139945', 902: '139977', 903: '139980', 904: '139982', 905: '140067', 906: '140192', 907: '140248', 908: '140278', 909: '140326', 910: '14051', 911: '14052', 912: '140579', 913: '140608', 914: '140763', 915: '140815', 916: '140818', 917: '140841', 918: '140927', 919: '140973', 920: '140998', 921: '141089', 922: '141101', 923: '141158', 924: '141327', 925: '141333', 926: '141336', 927: '14135', 928: '141388', 929: '14150', 930: '141521', 931: '14160', 932: '141611', 933: '14162', 934: '141705', 935: '141793', 936: '141843', 937: '141860', 938: '141926', 939: '142026', 940: '142147', 941: '142225', 942: '142236', 943: '142288', 944: '142296', 945: '142341', 946: '142366', 947: '142413', 948: '142558', 949: '142644', 950: '142674', 951: '142704', 952: '1428', 953: '142809', 954: '142855', 955: '14286', 956: '142889', 957: '14291', 958: '142910', 959: '142920', 960: '142948', 961: '142951', 962: '142971', 963: '143001', 964: '143024', 965: '14307', 966: '14308', 967: '143131', 968: '143203', 969: '143221', 970: '143227', 971: '143324', 972: '143373', 973: '143513', 974: '143521', 975: '143533', 976: '143574', 977: '143640', 978: '143667', 979: '143710', 980: '143729', 981: '143857', 982: '143863', 983: '14387', 984: '143881', 985: '143954', 986: '14401', 987: '144036', 988: '144099', 989: '144191', 990: '144201', 991: '144355', 992: '144376', 993: '144404', 994: '144430', 995: '144501', 996: '144580', 997: '144588', 998: '144654', 999: '144685', 1000: '144734', 1001: '14478', 1002: '144881', 1003: '14490', 1004: '144922', 1005: '145018', 1006: '145069', 1007: '145110', 1008: '145134', 1009: '145137', 1010: '145310', 1011: '145325', 1012: '145429', 1013: '145474', 1014: '14550', 1015: '145543', 1016: '14559', 1017: '145735', 1018: '145768', 1019: '145785', 1020: '14587', 1021: '145934', 1022: '145949', 1023: '145978', 1024: '145990', 1025: '146058', 1026: '146146', 1027: '146184', 1028: '146187', 1029: '146250', 1030: '146277', 1031: '146359', 1032: '146370', 1033: '14641', 1034: '146497', 1035: '14650', 1036: '146562', 1037: '146595', 1038: '146624', 1039: '146629', 1040: '146661', 1041: '146749', 1042: '146866', 1043: '146950', 1044: '146981', 1045: '14699', 1046: '147047', 1047: '147091', 1048: '147106', 1049: '147120', 1050: '147122', 1051: '147151', 1052: '147235', 1053: '147237', 1054: '147302', 1055: '14733', 1056: '147332', 1057: '147373', 1058: '147404', 1059: '147434', 1060: '147446', 1061: '147486', 1062: '147513', 1063: '147683', 1064: '147704', 1065: '147745', 1066: '14780', 1067: '147815', 1068: '147828', 1069: '147897', 1070: '147998', 1071: '148045', 1072: '148244', 1073: '148300', 1074: '148314', 1075: '148331', 1076: '148332', 1077: '148365', 1078: '148481', 1079: '148585', 1080: '148657', 1081: '148801', 1082: '148806', 1083: '148841', 1084: '148872', 1085: '148906', 1086: '148938', 1087: '148975', 1088: '148979', 1089: '14900', 1090: '149007', 1091: '149091', 1092: '14915', 1093: '149180', 1094: '14923', 1095: '149328', 1096: '149361', 1097: '149362', 1098: '149385', 1099: '149399', 1100: '149403', 1101: '149425', 1102: '149519', 1103: '149554', 1104: '149555', 1105: '149565', 1106: '149649', 1107: '149659', 1108: '14972', 1109: '149736', 1110: '149737', 1111: '149805', 1112: '149807', 1113: '149956', 1114: '149978', 1115: '149980', 1116: '149995', 1117: '150026', 1118: '150056', 1119: '150077', 1120: '150140', 1121: '150179', 1122: '150183', 1123: '150196', 1124: '150198', 1125: '150227', 1126: '150237', 1127: '150256', 1128: '150389', 1129: '150478', 1130: '150502', 1131: '150591', 1132: '150596', 1133: '150597', 1134: '15060', 1135: '150611', 1136: '150660', 1137: '150703', 1138: '150733', 1139: '150737', 1140: '150778', 1141: '150831', 1142: '150996', 1143: '151025', 1144: '151054', 1145: '151205', 1146: '15124', 1147: '151254', 1148: '151271', 1149: '151343', 1150: '151358', 1151: '15138', 1152: '151408', 1153: '151508', 1154: '151561', 1155: '151586', 1156: '15162', 1157: '151635', 1158: '151676', 1159: '151682', 1160: '151684', 1161: '151791', 1162: '151903', 1163: '151942', 1164: '151945', 1165: '152003', 1166: '15209', 1167: '152181', 1168: '152182', 1169: '152227', 1170: '152254', 1171: '152306', 1172: '15231', 1173: '152331', 1174: '152409', 1175: '152418', 1176: '152423', 1177: '152527', 1178: '152673', 1179: '152708', 1180: '152730', 1181: '152795', 1182: '152827', 1183: '152837', 1184: '152918', 1185: '152950', 1186: '152966', 1187: '15298', 1188: '153034', 1189: '153069', 1190: '153104', 1191: '153105', 1192: '153165', 1193: '15317', 1194: '153364', 1195: '153399', 1196: '153401', 1197: '153412', 1198: '153471', 1199: '153529', 1200: '153652', 1201: '153668', 1202: '153700', 1203: '153766', 1204: '153803', 1205: '153865', 1206: '153890', 1207: '153945', 1208: '153998', 1209: '154034', 1210: '154054', 1211: '154079', 1212: '15409', 1213: '154243', 1214: '15427', 1215: '15445', 1216: '154476', 1217: '154481', 1218: '154583', 1219: '154695', 1220: '154752', 1221: '154778', 1222: '154820', 1223: '154821', 1224: '15484', 1225: '154916', 1226: '154959', 1227: '154960', 1228: '155007', 1229: '155087', 1230: '155104', 1231: '15515', 1232: '155194', 1233: '155217', 1234: '155303', 1235: '15531', 1236: '155413', 1237: '155421', 1238: '155479', 1239: '155510', 1240: '155699', 1241: '155715', 1242: '155750', 1243: '155751', 1244: '155785', 1245: '155818', 1246: '155873', 1247: '155910', 1248: '155981', 1249: '155983', 1250: '156009', 1251: '156021', 1252: '156045', 1253: '156108', 1254: '156134', 1255: '156214', 1256: '156232', 1257: '156259', 1258: '156278', 1259: '156308', 1260: '156356', 1261: '156358', 1262: '156363', 1263: '15646', 1264: '156460', 1265: '156479', 1266: '156483', 1267: '156626', 1268: '156770', 1269: '156914', 1270: '157043', 1271: '157072', 1272: '15709', 1273: '157116', 1274: '157120', 1275: '15720', 1276: '157352', 1277: '15741', 1278: '157472', 1279: '157502', 1280: '157511', 1281: '157538', 1282: '157630', 1283: '157756', 1284: '157791', 1285: '157981', 1286: '158025', 1287: '158062', 1288: '158155', 1289: '158169', 1290: '158232', 1291: '158296', 1292: '158317', 1293: '158344', 1294: '158345', 1295: '158443', 1296: '158498', 1297: '1585', 1298: '158522', 1299: '158524', 1300: '158629', 1301: '158753', 1302: '158818', 1303: '158844', 1304: '158921', 1305: '15895', 1306: '158966', 1307: '158991', 1308: '159006', 1309: '159019', 1310: '159036', 1311: '159101', 1312: '1592', 1313: '159300', 1314: '159334', 1315: '159342', 1316: '159406', 1317: '159412', 1318: '159476', 1319: '159539', 1320: '159546', 1321: '159623', 1322: '15968', 1323: '159727', 1324: '159765', 1325: '160043', 1326: '16014', 1327: '160230', 1328: '160240', 1329: '160396', 1330: '160421', 1331: '160466', 1332: '1605', 1333: '160557', 1334: '160572', 1335: '1607', 1336: '160826', 1337: '160847', 1338: '160860', 1339: '160876', 1340: '160944', 1341: '160964', 1342: '161093', 1343: '161139', 1344: '161163', 1345: '161181', 1346: '161213', 1347: '16124', 1348: '161284', 1349: '161287', 1350: '161315', 1351: '161409', 1352: '161512', 1353: '161545', 1354: '161592', 1355: '161668', 1356: '161720', 1357: '161728', 1358: '16177', 1359: '161893', 1360: '161902', 1361: '161905', 1362: '161941', 1363: '162016', 1364: '162032', 1365: '162042', 1366: '162110', 1367: '162131', 1368: '162141', 1369: '16217', 1370: '162213', 1371: '162220', 1372: '162345', 1373: '162403', 1374: '162486', 1375: '162490', 1376: '162547', 1377: '162569', 1378: '162590', 1379: '162600', 1380: '162604', 1381: '162605', 1382: '162620', 1383: '162634', 1384: '162751', 1385: '162777', 1386: '162823', 1387: '162830', 1388: '162833', 1389: '162849', 1390: '162874', 1391: '162959', 1392: '16307', 1393: '163072', 1394: '163105', 1395: '163120', 1396: '163179', 1397: '163227', 1398: '163261', 1399: '163321', 1400: '163381', 1401: '163404', 1402: '163604', 1403: '163682', 1404: '163706', 1405: '163805', 1406: '163896', 1407: '163919', 1408: '16392', 1409: '163944', 1410: '164019', 1411: '164020', 1412: '164046', 1413: '164062', 1414: '164072', 1415: '164083', 1416: '164103', 1417: '164154', 1418: '164191', 1419: '164195', 1420: '164273', 1421: '164406', 1422: '164423', 1423: '164428', 1424: '164468', 1425: '164562', 1426: '164565', 1427: '164580', 1428: '164661', 1429: '164705', 1430: '164734', 1431: '164773', 1432: '164779', 1433: '164845', 1434: '164862', 1435: '165011', 1436: '165045', 1437: '165247', 1438: '165376', 1439: '165415', 1440: '165421', 1441: '165500', 1442: '165569', 1443: '165577', 1444: '165596', 1445: '165617', 1446: '16563', 1447: '165771', 1448: '165812', 1449: '165900', 1450: '165985', 1451: '166032', 1452: '166072', 1453: '166098', 1454: '16610', 1455: '166107', 1456: '166198', 1457: '166269', 1458: '166293', 1459: '166418', 1460: '16658', 1461: '166651', 1462: '166705', 1463: '166759', 1464: '166810', 1465: '166812', 1466: '166820', 1467: '166854', 1468: '166934', 1469: '166964', 1470: '166978', 1471: '166986', 1472: '167030', 1473: '167037', 1474: '167038', 1475: '167092', 1476: '167170', 1477: '167223', 1478: '16737', 1479: '167422', 1480: '167512', 1481: '16753', 1482: '167548', 1483: '167616', 1484: '167620', 1485: '167673', 1486: '167692', 1487: '167702', 1488: '167800', 1489: '16789', 1490: '167929', 1491: '167942', 1492: '167988', 1493: '168071', 1494: '168098', 1495: '168106', 1496: '168123', 1497: '168148', 1498: '16820', 1499: '168311', 1500: '168338', 1501: '16835', 1502: '168452', 1503: '168460', 1504: '168487', 1505: '1685', 1506: '168547', 1507: '168638', 1508: '168645', 1509: '168669', 1510: '168675', 1511: '168701', 1512: '168784', 1513: '168847', 1514: '168856', 1515: '168857', 1516: '168868', 1517: '168879', 1518: '168923', 1519: '168981', 1520: '168988', 1521: '169021', 1522: '169053', 1523: '169182', 1524: '169189', 1525: '169217', 1526: '169233', 1527: '16925', 1528: '169256', 1529: '169271', 1530: '169318', 1531: '169336', 1532: '169349', 1533: '169358', 1534: '169413', 1535: '169422', 1536: '169439', 1537: '169484', 1538: '169557', 1539: '169725', 1540: '169748', 1541: '169795', 1542: '169819', 1543: '169848', 1544: '169864', 1545: '169868', 1546: '169904', 1547: '169933', 1548: '169964', 1549: '169983', 1550: '169991', 1551: '170025', 1552: '170037', 1553: '170039', 1554: '170091', 1555: '170104', 1556: '170229', 1557: '170245', 1558: '170288', 1559: '170307', 1560: '170351', 1561: '170458', 1562: '170475', 1563: '170494', 1564: '170553', 1565: '17062', 1566: '170651', 1567: '170670', 1568: '170717', 1569: '170720', 1570: '170802', 1571: '170840', 1572: '170969', 1573: '170974', 1574: '171018', 1575: '171044', 1576: '171077', 1577: '171085', 1578: '171091', 1579: '171113', 1580: '171150', 1581: '17116', 1582: '171176', 1583: '171222', 1584: '171300', 1585: '171303', 1586: '171347', 1587: '171349', 1588: '171364', 1589: '171380', 1590: '171409', 1591: '171435', 1592: '171456', 1593: '171498', 1594: '171546', 1595: '171558', 1596: '171580', 1597: '1716', 1598: '171650', 1599: '171651', 1600: '171670', 1601: '171683', 1602: '171786', 1603: '171801', 1604: '171807', 1605: '171901', 1606: '171905', 1607: '171910', 1608: '171993', 1609: '172016', 1610: '17203', 1611: '172056', 1612: '172109', 1613: '172138', 1614: '172201', 1615: '172238', 1616: '172242', 1617: '172288', 1618: '172328', 1619: '172335', 1620: '172337', 1621: '17238', 1622: '172564', 1623: '172647', 1624: '172692', 1625: '172700', 1626: '172721', 1627: '172767', 1628: '172774', 1629: '172820', 1630: '172836', 1631: '172839', 1632: '172903', 1633: '172929', 1634: '172940', 1635: '172991', 1636: '17304', 1637: '173078', 1638: '173083', 1639: '173141', 1640: '173216', 1641: '173350', 1642: '173402', 1643: '173485', 1644: '173495', 1645: '173511', 1646: '173542', 1647: '17358', 1648: '173767', 1649: '173798', 1650: '173825', 1651: '173828', 1652: '173857', 1653: '173890', 1654: '17395', 1655: '17403', 1656: '174032', 1657: '174069', 1658: '174159', 1659: '174160', 1660: '174181', 1661: '174253', 1662: '174275', 1663: '174321', 1664: '174440', 1665: '174697', 1666: '174715', 1667: '174730', 1668: '174765', 1669: '174812', 1670: '174834', 1671: '174877', 1672: '174911', 1673: '174934', 1674: '175049', 1675: '175054', 1676: '175070', 1677: '175105', 1678: '175152', 1679: '175238', 1680: '17528', 1681: '175356', 1682: '175360', 1683: '175408', 1684: '175410', 1685: '175431', 1686: '175480', 1687: '175494', 1688: '1757', 1689: '175702', 1690: '175721', 1691: '175732', 1692: '175757', 1693: '175774', 1694: '175829', 1695: '175867', 1696: '175893', 1697: '17599', 1698: '176018', 1699: '176044', 1700: '176048', 1701: '17608', 1702: '176144', 1703: '176163', 1704: '176222', 1705: '176342', 1706: '176349', 1707: '17645', 1708: '176457', 1709: '176488', 1710: '176528', 1711: '176555', 1712: '176637', 1713: '176670', 1714: '176684', 1715: '176722', 1716: '17675', 1717: '176758', 1718: '176805', 1719: '176862', 1720: '176899', 1721: '176946', 1722: '176956', 1723: '177007', 1724: '177027', 1725: '177071', 1726: '17712', 1727: '177140', 1728: '17716', 1729: '177194', 1730: '177216', 1731: '177255', 1732: '177268', 1733: '177305', 1734: '177376', 1735: '177409', 1736: '177423', 1737: '177452', 1738: '177499', 1739: '177501', 1740: '17757', 1741: '177599', 1742: '1776', 1743: '177602', 1744: '177770', 1745: '177870', 1746: '177875', 1747: '177881', 1748: '177899', 1749: '178006', 1750: '178014', 1751: '178085', 1752: '178193', 1753: '178295', 1754: '17831', 1755: '178386', 1756: '178396', 1757: '178422', 1758: '178441', 1759: '178449', 1760: '178490', 1761: '178519', 1762: '178526', 1763: '178545', 1764: '178660', 1765: '178720', 1766: '178801', 1767: '178816', 1768: '178829', 1769: '178870', 1770: '178872', 1771: '178910', 1772: '178973', 1773: '179034', 1774: '179072', 1775: '179088', 1776: '1791', 1777: '179171', 1778: '179269', 1779: '179380', 1780: '179405', 1781: '179443', 1782: '179486', 1783: '179550', 1784: '179626', 1785: '179705', 1786: '179775', 1787: '179856', 1788: '179864', 1789: '17993', 1790: '17994', 1791: '179959', 1792: '180072', 1793: '18012', 1794: '180133', 1795: '180151', 1796: '180180', 1797: '180202', 1798: '180239', 1799: '180260', 1800: '180264', 1801: '180308', 1802: '180341', 1803: '180346', 1804: '180450', 1805: '180523', 1806: '180646', 1807: '180735', 1808: '180759', 1809: '180762', 1810: '180776', 1811: '180836', 1812: '180901', 1813: '180970', 1814: '180982', 1815: '180988', 1816: '181041', 1817: '181054', 1818: '181074', 1819: '181087', 1820: '181178', 1821: '181298', 1822: '181302', 1823: '181322', 1824: '181387', 1825: '181508', 1826: '18152', 1827: '181523', 1828: '181602', 1829: '181656', 1830: '181658', 1831: '181712', 1832: '181740', 1833: '181774', 1834: '181805', 1835: '181835', 1836: '181948', 1837: '181988', 1838: '182098', 1839: '182164', 1840: '182198', 1841: '18224', 1842: '182354', 1843: '182387', 1844: '182430', 1845: '18246', 1846: '182463', 1847: '182506', 1848: '182511', 1849: '182537', 1850: '182597', 1851: '182735', 1852: '182797', 1853: '182814', 1854: '182846', 1855: '182908', 1856: '182935', 1857: '182958', 1858: '182973', 1859: '183037', 1860: '183099', 1861: '183125', 1862: '183151', 1863: '183170', 1864: '183192', 1865: '183247', 1866: '18328', 1867: '183360', 1868: '183366', 1869: '183467', 1870: '183531', 1871: '183541', 1872: '183574', 1873: '183580', 1874: '183634', 1875: '183672', 1876: '183795', 1877: '183839', 1878: '18388', 1879: '183912', 1880: '18392', 1881: '183933', 1882: '183956', 1883: '184025', 1884: '184057', 1885: '18409', 1886: '184144', 1887: '1842', 1888: '184205', 1889: '18421', 1890: '18422', 1891: '184407', 1892: '184479', 1893: '184512', 1894: '18456', 1895: '18471', 1896: '18488', 1897: '184906', 1898: '184907', 1899: '184919', 1900: '184952', 1901: '185100', 1902: '185200', 1903: '185205', 1904: '185214', 1905: '185272', 1906: '185302', 1907: '185396', 1908: '185455', 1909: '185491', 1910: '185558', 1911: '18562', 1912: '185633', 1913: '185641', 1914: '185729', 1915: '185771', 1916: '185896', 1917: '185938', 1918: '185957', 1919: '186012', 1920: '186013', 1921: '186051', 1922: '18615', 1923: '18622', 1924: '186250', 1925: '186258', 1926: '186314', 1927: '186426', 1928: '186446', 1929: '18647', 1930: '186511', 1931: '186538', 1932: '186541', 1933: '186551', 1934: '186615', 1935: '186634', 1936: '186662', 1937: '186728', 1938: '18676', 1939: '18679', 1940: '186793', 1941: '186795', 1942: '186908', 1943: '186957', 1944: '186962', 1945: '187002', 1946: '187055', 1947: '187139', 1948: '187149', 1949: '187196', 1950: '187209', 1951: '187213', 1952: '187294', 1953: '1875', 1954: '187515', 1955: '187563', 1956: '187597', 1957: '187742', 1958: '187779', 1959: '187836', 1960: '18786', 1961: '187875', 1962: '187897', 1963: '187900', 1964: '187923', 1965: '187932', 1966: '188047', 1967: '188059', 1968: '188060', 1969: '188088', 1970: '188174', 1971: '188195', 1972: '188225', 1973: '188232', 1974: '188234', 1975: '188310', 1976: '188329', 1977: '188354', 1978: '188357', 1979: '188369', 1980: '188410', 1981: '188417', 1982: '188451', 1983: '18849', 1984: '188521', 1985: '18854', 1986: '18855', 1987: '188554', 1988: '18859', 1989: '188609', 1990: '188622', 1991: '188686', 1992: '188696', 1993: '188711', 1994: '188741', 1995: '188787', 1996: '188789', 1997: '188834', 1998: '188866', 1999: '188919', 2000: '188934', 2001: '188936', 2002: '188975', 2003: '188999', 2004: '189009', 2005: '189022', 2006: '189029', 2007: '189042', 2008: '189050', 2009: '18914', 2010: '189217', 2011: '189231', 2012: '18926', 2013: '18931', 2014: '189339', 2015: '189446', 2016: '189575', 2017: '189587', 2018: '189661', 2019: '189688', 2020: '189723', 2021: '189811', 2022: '189907', 2023: '189915', 2024: '189971', 2025: '189988', 2026: '190144', 2027: '190216', 2028: '190220', 2029: '190230', 2030: '190245', 2031: '190271', 2032: '190283', 2033: '190307', 2034: '190527', 2035: '190587', 2036: '190644', 2037: '190714', 2038: '190721', 2039: '190822', 2040: '190830', 2041: '190869', 2042: '190879', 2043: '190929', 2044: '190931', 2045: '190946', 2046: '19095', 2047: '190956', 2048: '190960', 2049: '190972', 2050: '191001', 2051: '191041', 2052: '191153', 2053: '191200', 2054: '191228', 2055: '191292', 2056: '19130', 2057: '191316', 2058: '191334', 2059: '191369', 2060: '191497', 2061: '191558', 2062: '191597', 2063: '191666', 2064: '191888', 2065: '191889', 2066: '191934', 2067: '192', 2068: '192021', 2069: '192024', 2070: '19205', 2071: '192065', 2072: '192101', 2073: '192160', 2074: '192191', 2075: '192233', 2076: '192235', 2077: '192317', 2078: '192349', 2079: '19238', 2080: '19239', 2081: '192398', 2082: '1924', 2083: '192488', 2084: '192583', 2085: '192605', 2086: '192625', 2087: '192662', 2088: '192683', 2089: '192701', 2090: '192761', 2091: '19282', 2092: '192870', 2093: '192903', 2094: '193102', 2095: '193106', 2096: '193164', 2097: '193225', 2098: '19328', 2099: '193402', 2100: '193505', 2101: '193550', 2102: '193580', 2103: '193603', 2104: '193765', 2105: '193781', 2106: '193833', 2107: '193938', 2108: '193962', 2109: '193973', 2110: '19402', 2111: '194027', 2112: '194037', 2113: '194047', 2114: '194130', 2115: '194135', 2116: '194198', 2117: '194309', 2118: '194337', 2119: '194406', 2120: '19449', 2121: '19451', 2122: '194521', 2123: '19455', 2124: '194571', 2125: '194692', 2126: '194699', 2127: '194721', 2128: '194838', 2129: '194889', 2130: '194896', 2131: '194914', 2132: '195008', 2133: '195071', 2134: '195139', 2135: '195301', 2136: '195341', 2137: '195361', 2138: '195366', 2139: '195382', 2140: '195396', 2141: '195412', 2142: '195432', 2143: '195477', 2144: '195510', 2145: '195562', 2146: '195584', 2147: '195660', 2148: '195811', 2149: '195814', 2150: '195862', 2151: '195900', 2152: '196001', 2153: '196031', 2154: '19605', 2155: '196117', 2156: '196241', 2157: '196358', 2158: '196387', 2159: '19643', 2160: '196454', 2161: '19647', 2162: '196598', 2163: '196634', 2164: '196640', 2165: '196668', 2166: '196682', 2167: '196692', 2168: '196699', 2169: '196722', 2170: '196873', 2171: '196877', 2172: '196886', 2173: '196942', 2174: '196990', 2175: '197068', 2176: '197069', 2177: '197272', 2178: '197321', 2179: '197341', 2180: '197353', 2181: '197519', 2182: '197585', 2183: '197645', 2184: '197884', 2185: '198047', 2186: '198059', 2187: '198120', 2188: '198208', 2189: '198229', 2190: '198232', 2191: '198351', 2192: '198396', 2193: '198626', 2194: '198657', 2195: '198674', 2196: '198680', 2197: '198711', 2198: '198792', 2199: '198837', 2200: '198838', 2201: '198848', 2202: '198880', 2203: '198912', 2204: '198916', 2205: '198945', 2206: '198955', 2207: '198958', 2208: '198963', 2209: '199044', 2210: '199188', 2211: '199209', 2212: '199222', 2213: '199285', 2214: '199312', 2215: '199318', 2216: '19937', 2217: '199450', 2218: '199455', 2219: '199457', 2220: '199506', 2221: '199507', 2222: '199532', 2223: '199577', 2224: '199606', 2225: '199735', 2226: '199748', 2227: '199815', 2228: '199890', 2229: '199904', 2230: '199936', 2231: '200000', 2232: '200100', 2233: '200127', 2234: '200144', 2235: '200154', 2236: '200305', 2237: '20044', 2238: '20064', 2239: '200644', 2240: '200688', 2241: '200696', 2242: '200827', 2243: '200842', 2244: '200858', 2245: '200887', 2246: '200931', 2247: '200954', 2248: '200976', 2249: '20102', 2250: '201041', 2251: '201051', 2252: '201061', 2253: '20120', 2254: '201267', 2255: '201304', 2256: '201306', 2257: '201311', 2258: '201438', 2259: '201462', 2260: '20149', 2261: '20153', 2262: '201554', 2263: '201589', 2264: '201676', 2265: '201709', 2266: '20179', 2267: '20180', 2268: '201840', 2269: '201874', 2270: '201953', 2271: '202055', 2272: '202085', 2273: '202182', 2274: '202288', 2275: '202342', 2276: '202440', 2277: '202464', 2278: '202508', 2279: '202548', 2280: '202723', 2281: '202779', 2282: '202814', 2283: '202857', 2284: '202886', 2285: '202892', 2286: '202915', 2287: '203007', 2288: '203071', 2289: '20409', 2290: '20481', 2291: '20512', 2292: '20522', 2293: '20579', 2294: '20651', 2295: '20652', 2296: '20740', 2297: '20752', 2298: '2079', 2299: '20823', 2300: '20863', 2301: '20864', 2302: '20885', 2303: '2093', 2304: '20981', 2305: '20994', 2306: '21013', 2307: '21016', 2308: '21084', 2309: '21137', 2310: '21279', 2311: '21353', 2312: '21416', 2313: '21422', 2314: '21447', 2315: '21497', 2316: '21546', 2317: '21576', 2318: '21603', 2319: '21613', 2320: '21635', 2321: '2167', 2322: '21680', 2323: '21703', 2324: '21738', 2325: '2174', 2326: '21794', 2327: '21838', 2328: '21843', 2329: '2185', 2330: '219', 2331: '21908', 2332: '2191', 2333: '21923', 2334: '21925', 2335: '2196', 2336: '22059', 2337: '22077', 2338: '22130', 2339: '2215', 2340: '22164', 2341: '22229', 2342: '22312', 2343: '22366', 2344: '22385', 2345: '22417', 2346: '22494', 2347: '22522', 2348: '22533', 2349: '22574', 2350: '226', 2351: '22629', 2352: '2273', 2353: '22810', 2354: '22868', 2355: '22877', 2356: '230', 2357: '23060', 2358: '2311', 2359: '23129', 2360: '23184', 2361: '23207', 2362: '23209', 2363: '23333', 2364: '23334', 2365: '23439', 2366: '23458', 2367: '23521', 2368: '23538', 2369: '23582', 2370: '2361', 2371: '23721', 2372: '23777', 2373: '23785', 2374: '24024', 2375: '24155', 2376: '24176', 2377: '24191', 2378: '24254', 2379: '24278', 2380: '244', 2381: '24409', 2382: '24448', 2383: '24669', 2384: '2474', 2385: '24852', 2386: '24958', 2387: '25047', 2388: '25054', 2389: '25093', 2390: '25296', 2391: '25398', 2392: '25457', 2393: '25529', 2394: '25602', 2395: '25651', 2396: '25704', 2397: '25731', 2398: '25762', 2399: '2581', 2400: '25847', 2401: '2586', 2402: '25954', 2403: '26012', 2404: '26028', 2405: '26064', 2406: '26066', 2407: '26136', 2408: '26143', 2409: '26169', 2410: '26205', 2411: '26266', 2412: '26318', 2413: '26386', 2414: '26391', 2415: '26434', 2416: '26440', 2417: '26515', 2418: '26617', 2419: '26648', 2420: '26653', 2421: '26689', 2422: '26724', 2423: '26801', 2424: '26849', 2425: '26903', 2426: '2695', 2427: '26967', 2428: '27', 2429: '27008', 2430: '27021', 2431: '27133', 2432: '27190', 2433: '27201', 2434: '27250', 2435: '27251', 2436: '27272', 2437: '27318', 2438: '2733', 2439: '27332', 2440: '27347', 2441: '27364', 2442: '27428', 2443: '27493', 2444: '27512', 2445: '27526', 2446: '27659', 2447: '27807', 2448: '27869', 2449: '27888', 2450: '27915', 2451: '27947', 2452: '28040', 2453: '28119', 2454: '28136', 2455: '28139', 2456: '28230', 2457: '28278', 2458: '28321', 2459: '28366', 2460: '2838', 2461: '28488', 2462: '28503', 2463: '28567', 2464: '2859', 2465: '28641', 2466: '28801', 2467: '28865', 2468: '28867', 2469: '28955', 2470: '28989', 2471: '29135', 2472: '29140', 2473: '29215', 2474: '29263', 2475: '29333', 2476: '29350', 2477: '2936', 2478: '29367', 2479: '29380', 2480: '29382', 2481: '29410', 2482: '29429', 2483: '29495', 2484: '29503', 2485: '29537', 2486: '29560', 2487: '2957', 2488: '29583', 2489: '29753', 2490: '29760', 2491: '29794', 2492: '29912', 2493: '30048', 2494: '30119', 2495: '30172', 2496: '30181', 2497: '30196', 2498: '30212', 2499: '30239', 2500: '30267', 2501: '30337', 2502: '30356', 2503: '30365', 2504: '30374', 2505: '30384', 2506: '30443', 2507: '3046', 2508: '30479', 2509: '30533', 2510: '3057', 2511: '30575', 2512: '30640', 2513: '30646', 2514: '30691', 2515: '30721', 2516: '30829', 2517: '3087', 2518: '3092', 2519: '31037', 2520: '31094', 2521: '31099', 2522: '3115', 2523: '31256', 2524: '3127', 2525: '31273', 2526: '31294', 2527: '31298', 2528: '31306', 2529: '31361', 2530: '31371', 2531: '31413', 2532: '31458', 2533: '31470', 2534: '31479', 2535: '31480', 2536: '31484', 2537: '31531', 2538: '31546', 2539: '31639', 2540: '31685', 2541: '31739', 2542: '31761', 2543: '31800', 2544: '31833', 2545: '31837', 2546: '31845', 2547: '3185', 2548: '31853', 2549: '31880', 2550: '31898', 2551: '31933', 2552: '31950', 2553: '32004', 2554: '32061', 2555: '32075', 2556: '32080', 2557: '32083', 2558: '32091', 2559: '32104', 2560: '32201', 2561: '32268', 2562: '32305', 2563: '32308', 2564: '32338', 2565: '32375', 2566: '32395', 2567: '32432', 2568: '32459', 2569: '32480', 2570: '32493', 2571: '32560', 2572: '32579', 2573: '32638', 2574: '32657', 2575: '32687', 2576: '32696', 2577: '32701', 2578: '32708', 2579: '32776', 2580: '32864', 2581: '32895', 2582: '3299', 2583: '33021', 2584: '33034', 2585: '33152', 2586: '33167', 2587: '33208', 2588: '33350', 2589: '33378', 2590: '33485', 2591: '33515', 2592: '33545', 2593: '3355', 2594: '33564', 2595: '33573', 2596: '33577', 2597: '33636', 2598: '33658', 2599: '33708', 2600: '33730', 2601: '33737', 2602: '33785', 2603: '33943', 2604: '33961', 2605: '34047', 2606: '34051', 2607: '3410', 2608: '34120', 2609: '34121', 2610: '34141', 2611: '34232', 2612: '34309', 2613: '34394', 2614: '34452', 2615: '34539', 2616: '34547', 2617: '34631', 2618: '34632', 2619: '34650', 2620: '34659', 2621: '3466', 2622: '34663', 2623: '34675', 2624: '34698', 2625: '34701', 2626: '34840', 2627: '35005', 2628: '35012', 2629: '35091', 2630: '35098', 2631: '35110', 2632: '35114', 2633: '35138', 2634: '35142', 2635: '35223', 2636: '35234', 2637: '35291', 2638: '3531', 2639: '35496', 2640: '35558', 2641: '35621', 2642: '35628', 2643: '35635', 2644: '35677', 2645: '35691', 2646: '35793', 2647: '35855', 2648: '35883', 2649: '35910', 2650: '35933', 2651: '36087', 2652: '36125', 2653: '36134', 2654: '36148', 2655: '36161', 2656: '36279', 2657: '36377', 2658: '3638', 2659: '36407', 2660: '36450', 2661: '36494', 2662: '36544', 2663: '3662', 2664: '36691', 2665: '36717', 2666: '36748', 2667: '36838', 2668: '36893', 2669: '3690', 2670: '3691', 2671: '36964', 2672: '36977', 2673: '36981', 2674: '37048', 2675: '3705', 2676: '37133', 2677: '37171', 2678: '37245', 2679: '37271', 2680: '3732', 2681: '37342', 2682: '37391', 2683: '37394', 2684: '37396', 2685: '37462', 2686: '37463', 2687: '3747', 2688: '375', 2689: '37556', 2690: '37644', 2691: '37728', 2692: '37732', 2693: '37768', 2694: '37789', 2695: '3779', 2696: '37835', 2697: '37837', 2698: '37892', 2699: '37972', 2700: '38000', 2701: '3802', 2702: '3803', 2703: '38069', 2704: '3809', 2705: '38090', 2706: '38109', 2707: '38156', 2708: '38354', 2709: '38357', 2710: '38482', 2711: '38494', 2712: '38507', 2713: '38526', 2714: '38610', 2715: '38691', 2716: '38732', 2717: '38733', 2718: '3874', 2719: '38879', 2720: '389', 2721: '38921', 2722: '38936', 2723: '38952', 2724: '38984', 2725: '39014', 2726: '39070', 2727: '39187', 2728: '39203', 2729: '39209', 2730: '39215', 2731: '39283', 2732: '39293', 2733: '39334', 2734: '39391', 2735: '39457', 2736: '39511', 2737: '39534', 2738: '39650', 2739: '39772', 2740: '39855', 2741: '39865', 2742: '39892', 2743: '39910', 2744: '39928', 2745: '39938', 2746: '39970', 2747: '39981', 2748: '40072', 2749: '40088', 2750: '4009', 2751: '40111', 2752: '40152', 2753: '40373', 2754: '40379', 2755: '40459', 2756: '40530', 2757: '40565', 2758: '40599', 2759: '40625', 2760: '40704', 2761: '40717', 2762: '40770', 2763: '40856', 2764: '40888', 2765: '4093', 2766: '40980', 2767: '41037', 2768: '4107', 2769: '41113', 2770: '41120', 2771: '41170', 2772: '41202', 2773: '41239', 2774: '41314', 2775: '4137', 2776: '4143', 2777: '41447', 2778: '41478', 2779: '41480', 2780: '4149', 2781: '41505', 2782: '41534', 2783: '41546', 2784: '41571', 2785: '41648', 2786: '41684', 2787: '4169', 2788: '41691', 2789: '41759', 2790: '41769', 2791: '41783', 2792: '41873', 2793: '41985', 2794: '42001', 2795: '42016', 2796: '42054', 2797: '4206', 2798: '42073', 2799: '42123', 2800: '42211', 2801: '42225', 2802: '42239', 2803: '4226', 2804: '42297', 2805: '42321', 2806: '42363', 2807: '42415', 2808: '42417', 2809: '42488', 2810: '42539', 2811: '42547', 2812: '42600', 2813: '42616', 2814: '42654', 2815: '42665', 2816: '42974', 2817: '43003', 2818: '43017', 2819: '43112', 2820: '43141', 2821: '43192', 2822: '43210', 2823: '43222', 2824: '43235', 2825: '43314', 2826: '4335', 2827: '43409', 2828: '43517', 2829: '43568', 2830: '43593', 2831: '43699', 2832: '43714', 2833: '43750', 2834: '43845', 2835: '43862', 2836: '43912', 2837: '43970', 2838: '44006', 2839: '44052', 2840: '44067', 2841: '4407', 2842: '44157', 2843: '44201', 2844: '44338', 2845: '44368', 2846: '44400', 2847: '44470', 2848: '44486', 2849: '44570', 2850: '44595', 2851: '4461', 2852: '44716', 2853: '44717', 2854: '44737', 2855: '44743', 2856: '4478', 2857: '44795', 2858: '44796', 2859: '44935', 2860: '44939', 2861: '44958', 2862: '44975', 2863: '44991', 2864: '45017', 2865: '45064', 2866: '45178', 2867: '45192', 2868: '45332', 2869: '45347', 2870: '454', 2871: '45428', 2872: '45600', 2873: '45752', 2874: '458', 2875: '45871', 2876: '45887', 2877: '4592', 2878: '45939', 2879: '45966', 2880: '45993', 2881: '46032', 2882: '46047', 2883: '46053', 2884: '46259', 2885: '46268', 2886: '46277', 2887: '46281', 2888: '46296', 2889: '46400', 2890: '4641', 2891: '46452', 2892: '46500', 2893: '46503', 2894: '46522', 2895: '46525', 2896: '46608', 2897: '46705', 2898: '46717', 2899: '46734', 2900: '46736', 2901: '46779', 2902: '46839', 2903: '46934', 2904: '46946', 2905: '47035', 2906: '4707', 2907: '47133', 2908: '47173', 2909: '47176', 2910: '47207', 2911: '4728', 2912: '47304', 2913: '47378', 2914: '47401', 2915: '47461', 2916: '47463', 2917: '47478', 2918: '47516', 2919: '47537', 2920: '47542', 2921: '47605', 2922: '47663', 2923: '47669', 2924: '47733', 2925: '47735', 2926: '47755', 2927: '47798', 2928: '47842', 2929: '47880', 2930: '47900', 2931: '47902', 2932: '47910', 2933: '47918', 2934: '47924', 2935: '47967', 2936: '48114', 2937: '48135', 2938: '48137', 2939: '48177', 2940: '48186', 2941: '48230', 2942: '48291', 2943: '48328', 2944: '48409', 2945: '48443', 2946: '48478', 2947: '48522', 2948: '48538', 2949: '48570', 2950: '48580', 2951: '48605', 2952: '4865', 2953: '48753', 2954: '48795', 2955: '48813', 2956: '48891', 2957: '48901', 2958: '48979', 2959: '49161', 2960: '49251', 2961: '49257', 2962: '49267', 2963: '49286', 2964: '49324', 2965: '49394', 2966: '49423', 2967: '49714', 2968: '49724', 2969: '49773', 2970: '49793', 2971: '49795', 2972: '49804', 2973: '49839', 2974: '49872', 2975: '49960', 2976: '5004', 2977: '50117', 2978: '50133', 2979: '50198', 2980: '50233', 2981: '50238', 2982: '50267', 2983: '50396', 2984: '50484', 2985: '50501', 2986: '50522', 2987: '50624', 2988: '50636', 2989: '50643', 2990: '50655', 2991: '50663', 2992: '50683', 2993: '5074', 2994: '50777', 2995: '50798', 2996: '50826', 2997: '5083', 2998: '5088', 2999: '50915', 3000: '50920', 3001: '50958', 3002: '50981', 3003: '51016', 3004: '5110', 3005: '51118', 3006: '51156', 3007: '51169', 3008: '51270', 3009: '51272', 3010: '51329', 3011: '51330', 3012: '51377', 3013: '51392', 3014: '51508', 3015: '51655', 3016: '51699', 3017: '5170', 3018: '51759', 3019: '51856', 3020: '51860', 3021: '5188', 3022: '5192', 3023: '51946', 3024: '51987', 3025: '52082', 3026: '52090', 3027: '52150', 3028: '52172', 3029: '52173', 3030: '52298', 3031: '52299', 3032: '52381', 3033: '52547', 3034: '52568', 3035: '52588', 3036: '52615', 3037: '52678', 3038: '52759', 3039: '52773', 3040: '52857', 3041: '52897', 3042: '52905', 3043: '5292', 3044: '5293', 3045: '53012', 3046: '53057', 3047: '53058', 3048: '53092', 3049: '53093', 3050: '5310', 3051: '53113', 3052: '53151', 3053: '53170', 3054: '53173', 3055: '53185', 3056: '53193', 3057: '53312', 3058: '53379', 3059: '5341', 3060: '53424', 3061: '53504', 3062: '53624', 3063: '53632', 3064: '53692', 3065: '53786', 3066: '53806', 3067: '53831', 3068: '53858', 3069: '5386', 3070: '53884', 3071: '53890', 3072: '53904', 3073: '53941', 3074: '5401', 3075: '54081', 3076: '54145', 3077: '54291', 3078: '54346', 3079: '54424', 3080: '54439', 3081: '54464', 3082: '5453', 3083: '5458', 3084: '5477', 3085: '5483', 3086: '54833', 3087: '54879', 3088: '54951', 3089: '54982', 3090: '55088', 3091: '55177', 3092: '55207', 3093: '55219', 3094: '55276', 3095: '55290', 3096: '55347', 3097: '55350', 3098: '55412', 3099: '55466', 3100: '55519', 3101: '5561', 3102: '55623', 3103: '55713', 3104: '55766', 3105: '55780', 3106: '55810', 3107: '56054', 3108: '56090', 3109: '56164', 3110: '562', 3111: '56230', 3112: '56460', 3113: '56553', 3114: '56649', 3115: '56733', 3116: '56752', 3117: '56767', 3118: '56772', 3119: '56808', 3120: '56827', 3121: '56887', 3122: '56917', 3123: '56935', 3124: '56939', 3125: '56986', 3126: '57015', 3127: '57033', 3128: '57043', 3129: '57107', 3130: '57116', 3131: '57185', 3132: '57191', 3133: '57198', 3134: '57330', 3135: '57343', 3136: '57378', 3137: '57381', 3138: '57459', 3139: '57499', 3140: '57505', 3141: '57540', 3142: '57600', 3143: '57601', 3144: '57616', 3145: '57630', 3146: '57668', 3147: '57722', 3148: '57771', 3149: '57825', 3150: '57872', 3151: '57910', 3152: '57939', 3153: '57943', 3154: '57970', 3155: '58112', 3156: '58421', 3157: '5843', 3158: '58445', 3159: '58541', 3160: '58608', 3161: '58637', 3162: '58653', 3163: '58697', 3164: '58765', 3165: '58845', 3166: '58893', 3167: '58894', 3168: '59101', 3169: '59142', 3170: '59149', 3171: '5917', 3172: '59257', 3173: '59265', 3174: '59272', 3175: '59355', 3176: '59470', 3177: '59546', 3178: '59586', 3179: '59639', 3180: '59682', 3181: '59703', 3182: '59742', 3183: '59793', 3184: '59831', 3185: '59833', 3186: '59848', 3187: '59853', 3188: '59872', 3189: '5997', 3190: '60144', 3191: '60307', 3192: '60332', 3193: '60349', 3194: '60352', 3195: '60391', 3196: '60435', 3197: '60491', 3198: '6050', 3199: '60532', 3200: '60563', 3201: '60597', 3202: '60603', 3203: '60661', 3204: '6068', 3205: '60708', 3206: '60730', 3207: '60749', 3208: '60823', 3209: '60873', 3210: '60894', 3211: '60896', 3212: '6090', 3213: '60900', 3214: '60912', 3215: '60948', 3216: '61019', 3217: '61067', 3218: '61089', 3219: '6110', 3220: '61169', 3221: '61251', 3222: '61268', 3223: '61271', 3224: '6138', 3225: '61406', 3226: '61412', 3227: '61420', 3228: '61464', 3229: '615', 3230: '61501', 3231: '61505', 3232: '61524', 3233: '61553', 3234: '61559', 3235: '61583', 3236: '6170', 3237: '61727', 3238: '61780', 3239: '61782', 3240: '61786', 3241: '61789', 3242: '61840', 3243: '61888', 3244: '61925', 3245: '62054', 3246: '6208', 3247: '6210', 3248: '62193', 3249: '62251', 3250: '62313', 3251: '624', 3252: '62533', 3253: '62568', 3254: '62575', 3255: '62604', 3256: '62702', 3257: '62785', 3258: '62792', 3259: '62843', 3260: '62850', 3261: '62864', 3262: '62894', 3263: '62996', 3264: '63044', 3265: '63122', 3266: '63157', 3267: '63316', 3268: '63334', 3269: '63344', 3270: '63366', 3271: '63443', 3272: '63473', 3273: '63529', 3274: '63584', 3275: '63593', 3276: '63597', 3277: '63630', 3278: '63817', 3279: '63840', 3280: '63894', 3281: '64042', 3282: '64118', 3283: '64146', 3284: '64179', 3285: '64183', 3286: '64233', 3287: '64267', 3288: '64378', 3289: '64412', 3290: '64422', 3291: '64424', 3292: '64433', 3293: '64503', 3294: '64520', 3295: '64531', 3296: '64625', 3297: '64792', 3298: '64813', 3299: '64877', 3300: '64915', 3301: '64976', 3302: '64997', 3303: '65068', 3304: '65109', 3305: '65141', 3306: '65178', 3307: '65199', 3308: '65218', 3309: '65268', 3310: '6529', 3311: '65297', 3312: '65321', 3313: '65433', 3314: '6545', 3315: '65502', 3316: '6551', 3317: '65526', 3318: '65528', 3319: '65586', 3320: '65592', 3321: '65625', 3322: '65658', 3323: '65675', 3324: '65731', 3325: '65766', 3326: '65805', 3327: '65818', 3328: '65855', 3329: '65906', 3330: '65971', 3331: '66016', 3332: '66119', 3333: '66146', 3334: '66151', 3335: '66181', 3336: '66264', 3337: '66275', 3338: '66355', 3339: '66445', 3340: '66478', 3341: '6650', 3342: '66539', 3343: '66774', 3344: '6679', 3345: '66807', 3346: '66929', 3347: '67013', 3348: '67027', 3349: '67043', 3350: '67045', 3351: '67109', 3352: '67274', 3353: '67302', 3354: '67335', 3355: '67352', 3356: '67353', 3357: '6737', 3358: '67406', 3359: '67409', 3360: '67454', 3361: '67480', 3362: '6749', 3363: '67496', 3364: '67497', 3365: '67580', 3366: '67599', 3367: '67610', 3368: '67619', 3369: '67685', 3370: '6774', 3371: '67771', 3372: '67842', 3373: '67884', 3374: '67929', 3375: '68033', 3376: '68087', 3377: '68091', 3378: '68106', 3379: '68122', 3380: '68168', 3381: '68177', 3382: '68192', 3383: '68226', 3384: '68255', 3385: '68256', 3386: '68264', 3387: '68339', 3388: '68429', 3389: '68445', 3390: '68486', 3391: '68495', 3392: '68501', 3393: '68563', 3394: '68581', 3395: '68631', 3396: '68632', 3397: '68924', 3398: '69027', 3399: '69038', 3400: '69172', 3401: '69184', 3402: '69193', 3403: '69366', 3404: '6939', 3405: '69444', 3406: '6946', 3407: '69463', 3408: '69483', 3409: '69499', 3410: '69576', 3411: '69611', 3412: '69672', 3413: '69743', 3414: '69835', 3415: '69837', 3416: '69905', 3417: '69915', 3418: '69960', 3419: '69973', 3420: '70024', 3421: '70050', 3422: '70088', 3423: '70111', 3424: '70255', 3425: '7033', 3426: '7038', 3427: '70414', 3428: '7042', 3429: '70441', 3430: '70514', 3431: '7053', 3432: '70644', 3433: '7065', 3434: '70669', 3435: '70686', 3436: '70716', 3437: '70801', 3438: '70816', 3439: '70825', 3440: '70909', 3441: '71003', 3442: '71024', 3443: '71037', 3444: '71077', 3445: '71090', 3446: '7112', 3447: '71197', 3448: '71336', 3449: '71405', 3450: '7145', 3451: '7150', 3452: '71555', 3453: '7159', 3454: '71615', 3455: '71692', 3456: '71731', 3457: '71756', 3458: '71852', 3459: '71977', 3460: '72024', 3461: '72039', 3462: '72106', 3463: '72162', 3464: '72220', 3465: '7223', 3466: '72263', 3467: '72301', 3468: '72303', 3469: '72335', 3470: '72365', 3471: '72389', 3472: '72446', 3473: '72475', 3474: '72541', 3475: '72615', 3476: '72686', 3477: '72694', 3478: '72823', 3479: '72836', 3480: '72859', 3481: '72946', 3482: '72949', 3483: '72978', 3484: '73018', 3485: '73064', 3486: '73072', 3487: '73092', 3488: '73211', 3489: '73220', 3490: '73231', 3491: '73266', 3492: '73300', 3493: '73304', 3494: '73319', 3495: '73327', 3496: '73406', 3497: '73455', 3498: '7357', 3499: '73647', 3500: '73652', 3501: '73727', 3502: '73750', 3503: '73772', 3504: '73954', 3505: '73967', 3506: '74050', 3507: '74052', 3508: '74053', 3509: '74063', 3510: '74186', 3511: '74244', 3512: '74264', 3513: '74274', 3514: '74283', 3515: '74339', 3516: '74510', 3517: '74513', 3518: '74518', 3519: '74556', 3520: '74559', 3521: '74662', 3522: '74707', 3523: '74812', 3524: '74821', 3525: '74834', 3526: '74858', 3527: '74874', 3528: '74879', 3529: '74910', 3530: '74913', 3531: '74955', 3532: '74973', 3533: '75005', 3534: '75009', 3535: '75036', 3536: '75112', 3537: '75149', 3538: '75213', 3539: '75263', 3540: '75285', 3541: '75286', 3542: '75319', 3543: '75324', 3544: '75478', 3545: '75501', 3546: '75666', 3547: '75805', 3548: '75930', 3549: '76038', 3550: '76110', 3551: '76147', 3552: '76200', 3553: '76245', 3554: '76253', 3555: '76259', 3556: '76303', 3557: '76343', 3558: '76344', 3559: '76393', 3560: '76590', 3561: '76656', 3562: '7666', 3563: '76871', 3564: '77002', 3565: '77024', 3566: '77172', 3567: '77174', 3568: '77190', 3569: '77203', 3570: '77218', 3571: '77230', 3572: '77288', 3573: '77342', 3574: '77344', 3575: '7738', 3576: '77386', 3577: '77395', 3578: '7753', 3579: '77563', 3580: '77568', 3581: '77571', 3582: '77666', 3583: '77677', 3584: '77727', 3585: '77757', 3586: '77855', 3587: '77856', 3588: '77943', 3589: '78006', 3590: '78156', 3591: '78197', 3592: '78267', 3593: '78304', 3594: '78405', 3595: '78426', 3596: '78430', 3597: '78471', 3598: '78691', 3599: '78791', 3600: '78801', 3601: '78880', 3602: '78901', 3603: '78907', 3604: '78914', 3605: '79140', 3606: '79148', 3607: '79155', 3608: '79162', 3609: '79184', 3610: '79298', 3611: '7930', 3612: '79358', 3613: '79534', 3614: '79585', 3615: '79688', 3616: '79747', 3617: '79754', 3618: '79764', 3619: '79777', 3620: '79787', 3621: '79836', 3622: '79838', 3623: '79844', 3624: '79867', 3625: '79922', 3626: '79973', 3627: '8006', 3628: '8009', 3629: '80133', 3630: '80138', 3631: '80146', 3632: '80147', 3633: '80205', 3634: '80221', 3635: '80235', 3636: '80249', 3637: '80272', 3638: '80299', 3639: '80402', 3640: '80470', 3641: '80488', 3642: '80600', 3643: '80740', 3644: '80801', 3645: '80811', 3646: '8089', 3647: '80942', 3648: '8098', 3649: '81020', 3650: '81049', 3651: '81117', 3652: '81133', 3653: '81187', 3654: '81209', 3655: '81247', 3656: '81281', 3657: '81351', 3658: '81380', 3659: '81405', 3660: '81410', 3661: '81419', 3662: '81428', 3663: '81471', 3664: '8149', 3665: '81564', 3666: '81572', 3667: '816', 3668: '8161', 3669: '8170', 3670: '81702', 3671: '81788', 3672: '81827', 3673: '81918', 3674: '81964', 3675: '8198', 3676: '81986', 3677: '82057', 3678: '82117', 3679: '82155', 3680: '82233', 3681: '82260', 3682: '82320', 3683: '82386', 3684: '82390', 3685: '82412', 3686: '82428', 3687: '8245', 3688: '8247', 3689: '82487', 3690: '82540', 3691: '82546', 3692: '82573', 3693: '82579', 3694: '8262', 3695: '82666', 3696: '82668', 3697: '82669', 3698: '82734', 3699: '82763', 3700: '82836', 3701: '82857', 3702: '8291', 3703: '8292', 3704: '82936', 3705: '8296', 3706: '83061', 3707: '83144', 3708: '83235', 3709: '8325', 3710: '83282', 3711: '83298', 3712: '8330', 3713: '83392', 3714: '83416', 3715: '83434', 3716: '83467', 3717: '83504', 3718: '8351', 3719: '83547', 3720: '8361', 3721: '83631', 3722: '83653', 3723: '83663', 3724: '83688', 3725: '83692', 3726: '83695', 3727: '83697', 3728: '83890', 3729: '83920', 3730: '83936', 3731: '83972', 3732: '84042', 3733: '84065', 3734: '84154', 3735: '84194', 3736: '84281', 3737: '84487', 3738: '84491', 3739: '84537', 3740: '84714', 3741: '84808', 3742: '84810', 3743: '84867', 3744: '84878', 3745: '84905', 3746: '84939', 3747: '85018', 3748: '85040', 3749: '85081', 3750: '85213', 3751: '85216', 3752: '85220', 3753: '85232', 3754: '85374', 3755: '85433', 3756: '85435', 3757: '85447', 3758: '85469', 3759: '85476', 3760: '85495', 3761: '85545', 3762: '85604', 3763: '85633', 3764: '85646', 3765: '85658', 3766: '85663', 3767: '85791', 3768: '85807', 3769: '85928', 3770: '86026', 3771: '86083', 3772: '86130', 3773: '86146', 3774: '86154', 3775: '86175', 3776: '86213', 3777: '86229', 3778: '86230', 3779: '86267', 3780: '86276', 3781: '86283', 3782: '86287', 3783: '86418', 3784: '86441', 3785: '86492', 3786: '86575', 3787: '86584', 3788: '8660', 3789: '86655', 3790: '86722', 3791: '86751', 3792: '86757', 3793: '86812', 3794: '86874', 3795: '86904', 3796: '86918', 3797: '86927', 3798: '870', 3799: '87080', 3800: '87157', 3801: '87205', 3802: '87226', 3803: '87289', 3804: '87383', 3805: '87421', 3806: '87532', 3807: '87544', 3808: '87644', 3809: '87674', 3810: '87717', 3811: '87785', 3812: '87845', 3813: '87915', 3814: '87919', 3815: '87987', 3816: '8799', 3817: '88006', 3818: '88040', 3819: '88124', 3820: '88194', 3821: '88201', 3822: '88202', 3823: '88215', 3824: '8822', 3825: '88286', 3826: '88331', 3827: '8834', 3828: '88351', 3829: '88432', 3830: '88451', 3831: '88472', 3832: '88483', 3833: '88546', 3834: '88612', 3835: '88761', 3836: '88782', 3837: '88802', 3838: '88834', 3839: '88840', 3840: '88853', 3841: '89102', 3842: '89105', 3843: '8918', 3844: '89207', 3845: '89229', 3846: '89317', 3847: '89435', 3848: '8944', 3849: '89587', 3850: '89597', 3851: '89649', 3852: '89655', 3853: '89668', 3854: '89694', 3855: '89718', 3856: '89768', 3857: '89794', 3858: '89859', 3859: '89931', 3860: '90021', 3861: '90025', 3862: '90034', 3863: '90054', 3864: '90119', 3865: '90169', 3866: '90331', 3867: '90358', 3868: '90396', 3869: '90412', 3870: '90597', 3871: '90632', 3872: '90633', 3873: '90647', 3874: '9070', 3875: '9073', 3876: '90966', 3877: '90979', 3878: '90997', 3879: '91013', 3880: '91059', 3881: '91090', 3882: '91132', 3883: '91156', 3884: '91178', 3885: '91274', 3886: '91285', 3887: '91430', 3888: '91518', 3889: '91557', 3890: '91584', 3891: '91659', 3892: '91684', 3893: '91772', 3894: '91798', 3895: '91799', 3896: '91839', 3897: '91868', 3898: '91879', 3899: '91882', 3900: '92099', 3901: '92224', 3902: '92275', 3903: '9228', 3904: '92349', 3905: '92369', 3906: '925', 3907: '92774', 3908: '92848', 3909: '92881', 3910: '92910', 3911: '92911', 3912: '9301', 3913: '93106', 3914: '93312', 3915: '93346', 3916: '93356', 3917: '93435', 3918: '93521', 3919: '93577', 3920: '93598', 3921: '93699', 3922: '93770', 3923: '93824', 3924: '93869', 3925: '93927', 3926: '93957', 3927: '93961', 3928: '93975', 3929: '93986', 3930: '93992', 3931: '94002', 3932: '94003', 3933: '94054', 3934: '94077', 3935: '94109', 3936: '94130', 3937: '94224', 3938: '94269', 3939: '94287', 3940: '94310', 3941: '94381', 3942: '94449', 3943: '9445', 3944: '94450', 3945: '94575', 3946: '94624', 3947: '94741', 3948: '94824', 3949: '94881', 3950: '94946', 3951: '95008', 3952: '95067', 3953: '95081', 3954: '95086', 3955: '951', 3956: '95194', 3957: '95326', 3958: '9534', 3959: '95356', 3960: '95573', 3961: '95585', 3962: '95628', 3963: '95728', 3964: '95885', 3965: '95905', 3966: '96182', 3967: '96209', 3968: '96224', 3969: '96325', 3970: '96331', 3971: '96412', 3972: '96420', 3973: '96431', 3974: '96455', 3975: '96471', 3976: '96520', 3977: '96653', 3978: '96663', 3979: '96667', 3980: '96680', 3981: '96711', 3982: '9673', 3983: '96842', 3984: '96945', 3985: '97038', 3986: '9706', 3987: '97125', 3988: '97142', 3989: '97256', 3990: '97283', 3991: '97465', 3992: '97507', 3993: '97643', 3994: '97668', 3995: '97717', 3996: '9772', 3997: '97744', 3998: '97765', 3999: '97791', 4000: '97834', 4001: '97854', 4002: '97901', 4003: '97989', 4004: '98003', 4005: '98038', 4006: '98075', 4007: '98143', 4008: '98179', 4009: '98230', 4010: '98280', 4011: '98304', 4012: '98335', 4013: '98385', 4014: '9846', 4015: '98460', 4016: '985', 4017: '98657', 4018: '98699', 4019: '98728', 4020: '98760', 4021: '98767', 4022: '98857', 4023: '98894', 4024: '98903', 4025: '98993', 4026: '99003', 4027: '99133', 4028: '99213', 4029: '99266', 4030: '99323', 4031: '99392', 4032: '99692', 4033: '99702', 4034: '99759', 4035: '99839', 4036: '99844', 4037: '99851', 4038: '99879', 4039: '99943'}\n"
     ]
    }
   ],
   "source": [
    "print(len(indices_to_class_labels_dict))\n",
    "print(indices_to_class_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"group11_indices_to_class_labels_dict.json\", \"wb\") as pickle_file:\n",
    "    pickle.dump(indices_to_class_labels_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(path, new_width, new_height):\n",
    "    image = Image.open(path)\n",
    "    image = ImageOps.fit(image, (new_width, new_height), Image.ANTIALIAS)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1c7201b4577c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresize_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_images\\\\e0dc68db51a90c10.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ade8f2b48e89>\u001b[0m in \u001b[0;36mresize_image\u001b[1;34m(path, new_width, new_height)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mresize_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "image = resize_image(\"test_images\\\\e0dc68db51a90c10.jpg\", 224, 224)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_image = np.array(image)\n",
    "print(np_image.shape)\n",
    "np_image = np_image/255\n",
    "image = np.expand_dims(np_image, axis=0)\n",
    "result = history.model.predict(image)\n",
    "predicted_class = indices_to_class_labels_dict[np.argmax(result)]\n",
    "print(\"Identified landmark for image is : {}\".format(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
