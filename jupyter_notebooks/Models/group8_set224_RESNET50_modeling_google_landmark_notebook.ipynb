{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50_MODEL=tf.keras.applications.ResNet50(input_shape=(224,224,3),\n",
    "                                               include_top=False,\n",
    "                                               weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ResNet50_MODEL.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv2_block1_0_bn'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNet50_MODEL.layers[15].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in ResNet50_MODEL.layers:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ResNet50_MODEL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "l1_factor = 0.0001\n",
    "l2_factor = 0.001\n",
    "dropout = 0.5\n",
    "learning_rate = 0.0001\n",
    "target_size_image_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "Dropout_Regularization1 (Dro (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2699)              5530251   \n",
      "=================================================================\n",
      "Total params: 29,117,963\n",
      "Trainable params: 29,064,843\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "                                  ResNet50_MODEL,\n",
    "                                  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                  tf.keras.layers.Dropout(dropout, name='Dropout_Regularization1'),\n",
    "                                  tf.keras.layers.Dense(2699, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "train_data_dir = '../datasets/group8_set_224/set_224/train/'\n",
    "valid_data_dir = '../datasets/group8_set_224/set_224/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.25,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116561 images belonging to 2699 classes.\n"
     ]
    }
   ],
   "source": [
    "#flow training images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=target_size_image_shape,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32388 images belonging to 2699 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=target_size_image_shape,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Labels in  Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2699\n",
      "{0: '100036', 1: '100043', 2: '100073', 3: '100150', 4: '100151', 5: '100153', 6: '100207', 7: '10022', 8: '100228', 9: '100249', 10: '100287', 11: '100344', 12: '100402', 13: '100418', 14: '100450', 15: '100523', 16: '100587', 17: '100600', 18: '100730', 19: '1009', 20: '101098', 21: '101115', 22: '101131', 23: '101507', 24: '101710', 25: '101718', 26: '101763', 27: '101797', 28: '101858', 29: '10188', 30: '101936', 31: '101952', 32: '102090', 33: '102277', 34: '102405', 35: '102472', 36: '102571', 37: '102575', 38: '102745', 39: '102924', 40: '10295', 41: '103016', 42: '103035', 43: '103050', 44: '103124', 45: '103158', 46: '103498', 47: '103583', 48: '103588', 49: '103734', 50: '103951', 51: '104070', 52: '104216', 53: '10422', 54: '104498', 55: '10450', 56: '104502', 57: '104559', 58: '104584', 59: '104633', 60: '104657', 61: '104681', 62: '104685', 63: '104749', 64: '104762', 65: '104766', 66: '104932', 67: '104958', 68: '104979', 69: '105120', 70: '105394', 71: '105485', 72: '105505', 73: '105562', 74: '105572', 75: '1056', 76: '105620', 77: '105732', 78: '105802', 79: '105812', 80: '105873', 81: '105961', 82: '105992', 83: '106134', 84: '106147', 85: '106245', 86: '106278', 87: '106298', 88: '106445', 89: '106456', 90: '106599', 91: '106657', 92: '106792', 93: '106890', 94: '107163', 95: '107269', 96: '107303', 97: '107536', 98: '107554', 99: '107630', 100: '107700', 101: '107866', 102: '107972', 103: '108008', 104: '108093', 105: '10810', 106: '108199', 107: '10821', 108: '108328', 109: '108470', 110: '108528', 111: '108574', 112: '10863', 113: '108683', 114: '108706', 115: '108808', 116: '109048', 117: '109108', 118: '10916', 119: '109201', 120: '109296', 121: '109364', 122: '109491', 123: '109720', 124: '109779', 125: '10979', 126: '109819', 127: '109832', 128: '109881', 129: '109897', 130: '109917', 131: '109999', 132: '110068', 133: '110117', 134: '11031', 135: '110329', 136: '11053', 137: '110557', 138: '11062', 139: '11066', 140: '110717', 141: '110749', 142: '110839', 143: '110955', 144: '111196', 145: '111220', 146: '111333', 147: '11135', 148: '111384', 149: '11143', 150: '111545', 151: '111739', 152: '111764', 153: '111783', 154: '11186', 155: '111927', 156: '111943', 157: '112036', 158: '112076', 159: '112103', 160: '11224', 161: '112288', 162: '112457', 163: '112498', 164: '112677', 165: '112680', 166: '112863', 167: '112906', 168: '112971', 169: '112997', 170: '113170', 171: '113271', 172: '113428', 173: '113455', 174: '113514', 175: '113518', 176: '11374', 177: '113781', 178: '11385', 179: '113889', 180: '113964', 181: '114148', 182: '114170', 183: '114172', 184: '114232', 185: '114293', 186: '114345', 187: '11437', 188: '114546', 189: '114581', 190: '1146', 191: '11471', 192: '114724', 193: '114762', 194: '114795', 195: '114832', 196: '114917', 197: '114997', 198: '115213', 199: '115263', 200: '11528', 201: '11529', 202: '115397', 203: '115472', 204: '115474', 205: '115482', 206: '1155', 207: '115674', 208: '115702', 209: '115742', 210: '115743', 211: '115922', 212: '115983', 213: '115984', 214: '116071', 215: '116104', 216: '11622', 217: '11623', 218: '116573', 219: '116713', 220: '116780', 221: '116781', 222: '116848', 223: '116933', 224: '117017', 225: '117021', 226: '117076', 227: '117089', 228: '117187', 229: '117231', 230: '117264', 231: '117283', 232: '117310', 233: '117405', 234: '117447', 235: '117659', 236: '117766', 237: '117772', 238: '117808', 239: '118097', 240: '118278', 241: '118279', 242: '118344', 243: '118385', 244: '118430', 245: '11844', 246: '118499', 247: '118547', 248: '118574', 249: '118589', 250: '118593', 251: '118594', 252: '118712', 253: '1188', 254: '118803', 255: '118822', 256: '118885', 257: '118952', 258: '11900', 259: '119096', 260: '119196', 261: '119417', 262: '119446', 263: '119503', 264: '119514', 265: '119525', 266: '119718', 267: '119783', 268: '119908', 269: '119921', 270: '119966', 271: '120052', 272: '120055', 273: '120119', 274: '120136', 275: '12014', 276: '12017', 277: '120365', 278: '120453', 279: '120466', 280: '120504', 281: '120782', 282: '120807', 283: '12084', 284: '120841', 285: '120889', 286: '120892', 287: '120933', 288: '120954', 289: '120974', 290: '121005', 291: '12108', 292: '121222', 293: '121267', 294: '121371', 295: '121444', 296: '121452', 297: '121484', 298: '121575', 299: '121737', 300: '121769', 301: '122017', 302: '122145', 303: '122230', 304: '122283', 305: '122355', 306: '122432', 307: '122533', 308: '12256', 309: '122571', 310: '12267', 311: '122688', 312: '122742', 313: '122858', 314: '123116', 315: '123123', 316: '123230', 317: '123381', 318: '123437', 319: '123449', 320: '123485', 321: '12356', 322: '12363', 323: '123631', 324: '123661', 325: '123702', 326: '123783', 327: '123852', 328: '124000', 329: '124109', 330: '124164', 331: '124218', 332: '124270', 333: '124292', 334: '124393', 335: '12440', 336: '124445', 337: '124477', 338: '124478', 339: '124515', 340: '124617', 341: '124687', 342: '124722', 343: '124788', 344: '124836', 345: '12486', 346: '124998', 347: '125037', 348: '125170', 349: '12531', 350: '125420', 351: '125426', 352: '125430', 353: '125459', 354: '125558', 355: '125603', 356: '125724', 357: '125760', 358: '125833', 359: '125957', 360: '125964', 361: '126088', 362: '126135', 363: '126148', 364: '126149', 365: '126254', 366: '126256', 367: '12634', 368: '126560', 369: '126728', 370: '126773', 371: '126881', 372: '126902', 373: '12693', 374: '126933', 375: '127182', 376: '127252', 377: '127457', 378: '127484', 379: '127492', 380: '127548', 381: '127649', 382: '127664', 383: '127706', 384: '127738', 385: '127747', 386: '127787', 387: '127799', 388: '127831', 389: '127860', 390: '127883', 391: '12793', 392: '12800', 393: '128034', 394: '128161', 395: '128205', 396: '128273', 397: '128509', 398: '128754', 399: '128770', 400: '128771', 401: '128794', 402: '12885', 403: '128855', 404: '128988', 405: '129043', 406: '129131', 407: '129155', 408: '129220', 409: '129300', 410: '129422', 411: '129741', 412: '129746', 413: '129770', 414: '129835', 415: '129867', 416: '129925', 417: '130004', 418: '130047', 419: '130089', 420: '130116', 421: '130118', 422: '130218', 423: '130283', 424: '130305', 425: '130353', 426: '130418', 427: '130422', 428: '130576', 429: '130842', 430: '130881', 431: '131014', 432: '13115', 433: '131172', 434: '131215', 435: '131265', 436: '131276', 437: '131409', 438: '131466', 439: '131532', 440: '131613', 441: '131623', 442: '131680', 443: '131722', 444: '131834', 445: '131853', 446: '131880', 447: '13193', 448: '131977', 449: '132051', 450: '132184', 451: '132213', 452: '1323', 453: '132397', 454: '132575', 455: '132624', 456: '132635', 457: '132671', 458: '132802', 459: '132914', 460: '133002', 461: '133007', 462: '133042', 463: '133090', 464: '133091', 465: '133122', 466: '133130', 467: '133131', 468: '133228', 469: '133258', 470: '133315', 471: '133335', 472: '13361', 473: '133636', 474: '133730', 475: '133736', 476: '133772', 477: '133792', 478: '133794', 479: '133822', 480: '13383', 481: '133843', 482: '133893', 483: '133976', 484: '134161', 485: '134216', 486: '134265', 487: '134322', 488: '134337', 489: '134339', 490: '134345', 491: '134489', 492: '13457', 493: '134598', 494: '134629', 495: '1347', 496: '134708', 497: '134846', 498: '134874', 499: '134877', 500: '134900', 501: '135061', 502: '13515', 503: '135160', 504: '135522', 505: '135527', 506: '135585', 507: '135706', 508: '135891', 509: '135943', 510: '136017', 511: '136164', 512: '136405', 513: '136599', 514: '136700', 515: '136721', 516: '136809', 517: '136842', 518: '136844', 519: '136929', 520: '136991', 521: '137011', 522: '137013', 523: '13705', 524: '137078', 525: '137099', 526: '137317', 527: '137411', 528: '137465', 529: '137477', 530: '137579', 531: '137671', 532: '137708', 533: '137763', 534: '137832', 535: '137901', 536: '138014', 537: '138084', 538: '138146', 539: '138293', 540: '138295', 541: '138346', 542: '138497', 543: '138498', 544: '138625', 545: '138626', 546: '138689', 547: '138760', 548: '1388', 549: '139052', 550: '139117', 551: '139143', 552: '139366', 553: '139380', 554: '139404', 555: '139584', 556: '139620', 557: '139624', 558: '139633', 559: '13976', 560: '139784', 561: '139830', 562: '139875', 563: '139895', 564: '13993', 565: '13994', 566: '139989', 567: '140010', 568: '140138', 569: '140263', 570: '140267', 571: '140276', 572: '140336', 573: '140371', 574: '14042', 575: '140431', 576: '140469', 577: '140680', 578: '140801', 579: '140844', 580: '140931', 581: '141015', 582: '141060', 583: '141076', 584: '141195', 585: '14127', 586: '141366', 587: '141443', 588: '141498', 589: '141598', 590: '141755', 591: '141777', 592: '141778', 593: '141852', 594: '141858', 595: '141910', 596: '142000', 597: '142016', 598: '142022', 599: '14216', 600: '142178', 601: '142193', 602: '142210', 603: '142276', 604: '142342', 605: '14266', 606: '142714', 607: '142907', 608: '143050', 609: '143174', 610: '143338', 611: '143359', 612: '143362', 613: '143412', 614: '143457', 615: '143463', 616: '143475', 617: '143508', 618: '143600', 619: '143693', 620: '143773', 621: '143964', 622: '144026', 623: '144051', 624: '144106', 625: '14412', 626: '14414', 627: '144229', 628: '144290', 629: '144322', 630: '144576', 631: '144590', 632: '144633', 633: '144720', 634: '144744', 635: '144979', 636: '145051', 637: '145100', 638: '145296', 639: '145352', 640: '145357', 641: '145596', 642: '145775', 643: '145807', 644: '145826', 645: '145909', 646: '145920', 647: '145939', 648: '14599', 649: '14603', 650: '146053', 651: '146209', 652: '146343', 653: '146460', 654: '146494', 655: '146555', 656: '146622', 657: '146626', 658: '14663', 659: '146675', 660: '146711', 661: '146872', 662: '146928', 663: '147009', 664: '147159', 665: '147227', 666: '147336', 667: '147369', 668: '147392', 669: '147592', 670: '147628', 671: '147824', 672: '147834', 673: '147843', 674: '147854', 675: '148091', 676: '148144', 677: '148219', 678: '148295', 679: '148310', 680: '148474', 681: '148494', 682: '148580', 683: '148729', 684: '148730', 685: '148850', 686: '148895', 687: '14905', 688: '149097', 689: '149151', 690: '149257', 691: '149267', 692: '149341', 693: '149395', 694: '149477', 695: '149683', 696: '149769', 697: '149770', 698: '149800', 699: '149862', 700: '149866', 701: '149957', 702: '150024', 703: '150053', 704: '150069', 705: '15016', 706: '150233', 707: '150251', 708: '15030', 709: '150433', 710: '150474', 711: '150498', 712: '150540', 713: '150619', 714: '150709', 715: '150782', 716: '150842', 717: '151108', 718: '151124', 719: '151215', 720: '151299', 721: '151448', 722: '151475', 723: '151587', 724: '151636', 725: '151654', 726: '151754', 727: '15178', 728: '151838', 729: '151839', 730: '152020', 731: '152061', 732: '152141', 733: '152234', 734: '152242', 735: '152266', 736: '152279', 737: '15229', 738: '15249', 739: '152789', 740: '15280', 741: '152855', 742: '152974', 743: '153061', 744: '153078', 745: '153121', 746: '153135', 747: '153328', 748: '153604', 749: '153654', 750: '153685', 751: '153775', 752: '153858', 753: '153943', 754: '15400', 755: '154031', 756: '154115', 757: '154129', 758: '154163', 759: '154179', 760: '154335', 761: '154376', 762: '154427', 763: '15453', 764: '154538', 765: '154568', 766: '154590', 767: '154659', 768: '154693', 769: '154694', 770: '154713', 771: '154931', 772: '154935', 773: '154975', 774: '154997', 775: '155', 776: '155028', 777: '155074', 778: '155142', 779: '155145', 780: '155149', 781: '155212', 782: '155244', 783: '155300', 784: '155340', 785: '155350', 786: '155403', 787: '155411', 788: '155503', 789: '155610', 790: '155621', 791: '155756', 792: '155815', 793: '155864', 794: '155918', 795: '155956', 796: '155970', 797: '15600', 798: '156160', 799: '156303', 800: '156380', 801: '156408', 802: '156440', 803: '15648', 804: '156547', 805: '156677', 806: '156784', 807: '156821', 808: '156839', 809: '156851', 810: '156861', 811: '156896', 812: '156911', 813: '157019', 814: '157079', 815: '157136', 816: '157305', 817: '157374', 818: '157393', 819: '157399', 820: '157454', 821: '157501', 822: '157635', 823: '157787', 824: '157849', 825: '157893', 826: '157913', 827: '157924', 828: '157982', 829: '15802', 830: '158076', 831: '158080', 832: '158307', 833: '158414', 834: '158466', 835: '158525', 836: '158618', 837: '158641', 838: '158663', 839: '158703', 840: '158713', 841: '158785', 842: '158817', 843: '158820', 844: '158953', 845: '159118', 846: '159148', 847: '159390', 848: '159402', 849: '159483', 850: '159521', 851: '159575', 852: '159625', 853: '159835', 854: '15985', 855: '159925', 856: '159952', 857: '160198', 858: '160237', 859: '160243', 860: '160244', 861: '160308', 862: '160333', 863: '160349', 864: '160379', 865: '160446', 866: '160489', 867: '160540', 868: '160713', 869: '160721', 870: '160764', 871: '160853', 872: '160896', 873: '161031', 874: '161087', 875: '161201', 876: '161218', 877: '161255', 878: '161297', 879: '161425', 880: '161427', 881: '161479', 882: '161482', 883: '161488', 884: '161542', 885: '161564', 886: '161669', 887: '161700', 888: '161804', 889: '161939', 890: '16202', 891: '162072', 892: '162094', 893: '162142', 894: '16232', 895: '162324', 896: '16244', 897: '162527', 898: '162545', 899: '162552', 900: '162722', 901: '162759', 902: '162800', 903: '162952', 904: '162966', 905: '163119', 906: '163147', 907: '163184', 908: '16319', 909: '16321', 910: '163217', 911: '16323', 912: '163399', 913: '163415', 914: '16344', 915: '163458', 916: '163519', 917: '163529', 918: '163602', 919: '163625', 920: '163873', 921: '163985', 922: '1640', 923: '164006', 924: '164077', 925: '164193', 926: '1642', 927: '164353', 928: '164552', 929: '16460', 930: '164815', 931: '164843', 932: '164849', 933: '164962', 934: '164966', 935: '164975', 936: '164989', 937: '164996', 938: '164999', 939: '165077', 940: '165083', 941: '165112', 942: '165152', 943: '165178', 944: '165210', 945: '165244', 946: '165287', 947: '165346', 948: '165369', 949: '165468', 950: '165690', 951: '165729', 952: '165840', 953: '165996', 954: '166007', 955: '166123', 956: '166128', 957: '166179', 958: '166185', 959: '166191', 960: '16625', 961: '166314', 962: '16636', 963: '166397', 964: '166432', 965: '166557', 966: '166615', 967: '16667', 968: '166683', 969: '16669', 970: '166720', 971: '1668', 972: '16691', 973: '167014', 974: '167080', 975: '167129', 976: '167187', 977: '167266', 978: '167275', 979: '167297', 980: '167301', 981: '167315', 982: '167332', 983: '167384', 984: '167390', 985: '167393', 986: '16748', 987: '167510', 988: '167582', 989: '167766', 990: '167776', 991: '168016', 992: '168116', 993: '168154', 994: '168173', 995: '168371', 996: '168511', 997: '168541', 998: '168542', 999: '168765', 1000: '168793', 1001: '168873', 1002: '168962', 1003: '169148', 1004: '169163', 1005: '169176', 1006: '169201', 1007: '169364', 1008: '169384', 1009: '169572', 1010: '169611', 1011: '16963', 1012: '169683', 1013: '169787', 1014: '169837', 1015: '170033', 1016: '170070', 1017: '170121', 1018: '17017', 1019: '170238', 1020: '170338', 1021: '170421', 1022: '170618', 1023: '170619', 1024: '17064', 1025: '170709', 1026: '170716', 1027: '170782', 1028: '170881', 1029: '171006', 1030: '171067', 1031: '171140', 1032: '171147', 1033: '171163', 1034: '17120', 1035: '171249', 1036: '17134', 1037: '171351', 1038: '171407', 1039: '171418', 1040: '171429', 1041: '171445', 1042: '171493', 1043: '171675', 1044: '171883', 1045: '171999', 1046: '172063', 1047: '172164', 1048: '172188', 1049: '172276', 1050: '17234', 1051: '172359', 1052: '172383', 1053: '172461', 1054: '172504', 1055: '172651', 1056: '172796', 1057: '172802', 1058: '172843', 1059: '172864', 1060: '172988', 1061: '173144', 1062: '173163', 1063: '173170', 1064: '173236', 1065: '173682', 1066: '17373', 1067: '173747', 1068: '17380', 1069: '173879', 1070: '173906', 1071: '173974', 1072: '174081', 1073: '174112', 1074: '174165', 1075: '174281', 1076: '174303', 1077: '174351', 1078: '174453', 1079: '174502', 1080: '174512', 1081: '174667', 1082: '174671', 1083: '174731', 1084: '174735', 1085: '174814', 1086: '174879', 1087: '174885', 1088: '174942', 1089: '174986', 1090: '175036', 1091: '175042', 1092: '175047', 1093: '175100', 1094: '175149', 1095: '175187', 1096: '17523', 1097: '175389', 1098: '175411', 1099: '175463', 1100: '175544', 1101: '175756', 1102: '175781', 1103: '175786', 1104: '175929', 1105: '175998', 1106: '176043', 1107: '176089', 1108: '176137', 1109: '176171', 1110: '176276', 1111: '176328', 1112: '176381', 1113: '17642', 1114: '176439', 1115: '176454', 1116: '176493', 1117: '17655', 1118: '176575', 1119: '176606', 1120: '176700', 1121: '176719', 1122: '176764', 1123: '176807', 1124: '176881', 1125: '176958', 1126: '17701', 1127: '177044', 1128: '17721', 1129: '177380', 1130: '177424', 1131: '177529', 1132: '177564', 1133: '177590', 1134: '177679', 1135: '17777', 1136: '177780', 1137: '177789', 1138: '177818', 1139: '177947', 1140: '177986', 1141: '178113', 1142: '17820', 1143: '178271', 1144: '178344', 1145: '178459', 1146: '17848', 1147: '178591', 1148: '178625', 1149: '178648', 1150: '178722', 1151: '178758', 1152: '178815', 1153: '178979', 1154: '179015', 1155: '179061', 1156: '179135', 1157: '179283', 1158: '179326', 1159: '179341', 1160: '179409', 1161: '179423', 1162: '179425', 1163: '179515', 1164: '179579', 1165: '179599', 1166: '179655', 1167: '17984', 1168: '179884', 1169: '179964', 1170: '180005', 1171: '180042', 1172: '180087', 1173: '180114', 1174: '180125', 1175: '180238', 1176: '180352', 1177: '180479', 1178: '180492', 1179: '180584', 1180: '18059', 1181: '180592', 1182: '180617', 1183: '180637', 1184: '180656', 1185: '180773', 1186: '180784', 1187: '180866', 1188: '181078', 1189: '181101', 1190: '181108', 1191: '181150', 1192: '181236', 1193: '181291', 1194: '18134', 1195: '181345', 1196: '181402', 1197: '181494', 1198: '18158', 1199: '181674', 1200: '181696', 1201: '181804', 1202: '181931', 1203: '181994', 1204: '182165', 1205: '182228', 1206: '182379', 1207: '182500', 1208: '182502', 1209: '182523', 1210: '182549', 1211: '18256', 1212: '182591', 1213: '182609', 1214: '18266', 1215: '182680', 1216: '182746', 1217: '182906', 1218: '182923', 1219: '182977', 1220: '183061', 1221: '183374', 1222: '183493', 1223: '183495', 1224: '183515', 1225: '183551', 1226: '183555', 1227: '183588', 1228: '183612', 1229: '183668', 1230: '183714', 1231: '183718', 1232: '18380', 1233: '183810', 1234: '183876', 1235: '183917', 1236: '184005', 1237: '184028', 1238: '184203', 1239: '184263', 1240: '184326', 1241: '184408', 1242: '184452', 1243: '184501', 1244: '184517', 1245: '184581', 1246: '184614', 1247: '184640', 1248: '184719', 1249: '184849', 1250: '185076', 1251: '185077', 1252: '18511', 1253: '185160', 1254: '185170', 1255: '185274', 1256: '185363', 1257: '185409', 1258: '185414', 1259: '185432', 1260: '185487', 1261: '185492', 1262: '185599', 1263: '185631', 1264: '185634', 1265: '185804', 1266: '185985', 1267: '186064', 1268: '186087', 1269: '186094', 1270: '18620', 1271: '186286', 1272: '186325', 1273: '186326', 1274: '186360', 1275: '186363', 1276: '186527', 1277: '186585', 1278: '186632', 1279: '186683', 1280: '186724', 1281: '186790', 1282: '186859', 1283: '187032', 1284: '187045', 1285: '187057', 1286: '187081', 1287: '187089', 1288: '187095', 1289: '187197', 1290: '187299', 1291: '187403', 1292: '187625', 1293: '187627', 1294: '187706', 1295: '187734', 1296: '187870', 1297: '187877', 1298: '187903', 1299: '187945', 1300: '187951', 1301: '187997', 1302: '18802', 1303: '188455', 1304: '188474', 1305: '188549', 1306: '188564', 1307: '188660', 1308: '188691', 1309: '18871', 1310: '188791', 1311: '189', 1312: '189011', 1313: '189019', 1314: '189072', 1315: '189109', 1316: '189277', 1317: '189407', 1318: '189410', 1319: '189441', 1320: '189496', 1321: '189514', 1322: '189623', 1323: '189683', 1324: '189796', 1325: '189799', 1326: '189896', 1327: '189921', 1328: '190000', 1329: '190044', 1330: '19026', 1331: '190281', 1332: '190480', 1333: '191008', 1334: '191141', 1335: '191184', 1336: '191291', 1337: '191453', 1338: '191552', 1339: '191570', 1340: '191621', 1341: '19173', 1342: '191745', 1343: '191750', 1344: '191806', 1345: '191812', 1346: '191868', 1347: '191941', 1348: '191956', 1349: '192123', 1350: '192212', 1351: '192385', 1352: '192428', 1353: '192551', 1354: '192619', 1355: '192720', 1356: '19279', 1357: '192796', 1358: '192831', 1359: '192854', 1360: '192861', 1361: '192866', 1362: '19303', 1363: '193035', 1364: '193038', 1365: '193041', 1366: '193077', 1367: '193098', 1368: '193184', 1369: '193239', 1370: '193319', 1371: '193403', 1372: '193474', 1373: '193635', 1374: '193708', 1375: '193984', 1376: '194014', 1377: '194043', 1378: '194151', 1379: '194178', 1380: '194223', 1381: '194265', 1382: '194396', 1383: '19475', 1384: '194781', 1385: '194851', 1386: '19507', 1387: '195083', 1388: '195106', 1389: '195233', 1390: '195269', 1391: '195377', 1392: '195411', 1393: '195421', 1394: '195439', 1395: '19546', 1396: '195532', 1397: '195628', 1398: '195708', 1399: '195730', 1400: '195815', 1401: '195851', 1402: '195856', 1403: '195866', 1404: '195954', 1405: '196003', 1406: '196103', 1407: '196122', 1408: '196208', 1409: '196309', 1410: '196396', 1411: '196497', 1412: '196574', 1413: '196600', 1414: '196618', 1415: '196735', 1416: '196826', 1417: '196846', 1418: '196916', 1419: '196950', 1420: '196958', 1421: '197020', 1422: '197047', 1423: '197160', 1424: '197173', 1425: '19732', 1426: '197337', 1427: '197361', 1428: '197408', 1429: '197428', 1430: '197456', 1431: '197543', 1432: '197572', 1433: '197650', 1434: '197654', 1435: '197699', 1436: '197915', 1437: '198093', 1438: '198130', 1439: '198170', 1440: '198217', 1441: '198247', 1442: '198341', 1443: '198412', 1444: '198420', 1445: '198459', 1446: '198555', 1447: '198655', 1448: '198658', 1449: '198714', 1450: '198911', 1451: '198927', 1452: '19907', 1453: '199106', 1454: '199363', 1455: '199493', 1456: '199585', 1457: '199623', 1458: '199626', 1459: '199698', 1460: '199826', 1461: '199899', 1462: '199986', 1463: '200101', 1464: '200253', 1465: '200263', 1466: '200286', 1467: '200355', 1468: '200375', 1469: '200386', 1470: '200395', 1471: '200474', 1472: '200548', 1473: '200551', 1474: '200708', 1475: '200860', 1476: '200916', 1477: '200994', 1478: '200995', 1479: '201011', 1480: '2011', 1481: '201421', 1482: '201440', 1483: '201478', 1484: '201536', 1485: '201662', 1486: '201899', 1487: '201971', 1488: '201984', 1489: '201998', 1490: '202080', 1491: '202106', 1492: '202118', 1493: '202188', 1494: '20220', 1495: '202316', 1496: '202354', 1497: '202441', 1498: '20246', 1499: '202489', 1500: '20249', 1501: '202543', 1502: '202567', 1503: '202596', 1504: '20274', 1505: '202795', 1506: '20282', 1507: '202852', 1508: '202866', 1509: '202875', 1510: '202902', 1511: '202938', 1512: '203034', 1513: '203062', 1514: '20337', 1515: '2046', 1516: '20469', 1517: '20493', 1518: '20536', 1519: '20614', 1520: '20819', 1521: '209', 1522: '20926', 1523: '21001', 1524: '21018', 1525: '21021', 1526: '21089', 1527: '21115', 1528: '21147', 1529: '21329', 1530: '21410', 1531: '21456', 1532: '21460', 1533: '21482', 1534: '21519', 1535: '21739', 1536: '21743', 1537: '21758', 1538: '21770', 1539: '22118', 1540: '22119', 1541: '22148', 1542: '2220', 1543: '22403', 1544: '22415', 1545: '22419', 1546: '22458', 1547: '22511', 1548: '22513', 1549: '22527', 1550: '22534', 1551: '2260', 1552: '22664', 1553: '22704', 1554: '22742', 1555: '22778', 1556: '22813', 1557: '22886', 1558: '22890', 1559: '22912', 1560: '23006', 1561: '23100', 1562: '23143', 1563: '23155', 1564: '23199', 1565: '23258', 1566: '23363', 1567: '23372', 1568: '23491', 1569: '23530', 1570: '23578', 1571: '23585', 1572: '23641', 1573: '23747', 1574: '23809', 1575: '23887', 1576: '24094', 1577: '24110', 1578: '24148', 1579: '24199', 1580: '24332', 1581: '24371', 1582: '24380', 1583: '24395', 1584: '24442', 1585: '24464', 1586: '24494', 1587: '2462', 1588: '24634', 1589: '24902', 1590: '24996', 1591: '25108', 1592: '25111', 1593: '25174', 1594: '25226', 1595: '25249', 1596: '25290', 1597: '25354', 1598: '25624', 1599: '25648', 1600: '25668', 1601: '25786', 1602: '25922', 1603: '2593', 1604: '2602', 1605: '26024', 1606: '26217', 1607: '26322', 1608: '26410', 1609: '26469', 1610: '2653', 1611: '26642', 1612: '26737', 1613: '26768', 1614: '26907', 1615: '26927', 1616: '27026', 1617: '27047', 1618: '27209', 1619: '27304', 1620: '27390', 1621: '27429', 1622: '27435', 1623: '27607', 1624: '27615', 1625: '27660', 1626: '27698', 1627: '27744', 1628: '27897', 1629: '28032', 1630: '28131', 1631: '28299', 1632: '28300', 1633: '28301', 1634: '28327', 1635: '28378', 1636: '28425', 1637: '2849', 1638: '28531', 1639: '28539', 1640: '28592', 1641: '28593', 1642: '28619', 1643: '28730', 1644: '28833', 1645: '28843', 1646: '28868', 1647: '28919', 1648: '28946', 1649: '29240', 1650: '29251', 1651: '29375', 1652: '29426', 1653: '29446', 1654: '29469', 1655: '29496', 1656: '29506', 1657: '29520', 1658: '2955', 1659: '29588', 1660: '29713', 1661: '29731', 1662: '29761', 1663: '2985', 1664: '29868', 1665: '29952', 1666: '30059', 1667: '30108', 1668: '30173', 1669: '30203', 1670: '3027', 1671: '3036', 1672: '30385', 1673: '30489', 1674: '30499', 1675: '30584', 1676: '30656', 1677: '30863', 1678: '30885', 1679: '30896', 1680: '30925', 1681: '30929', 1682: '3099', 1683: '31180', 1684: '31185', 1685: '31284', 1686: '31320', 1687: '3133', 1688: '31544', 1689: '31595', 1690: '31660', 1691: '31682', 1692: '31687', 1693: '31711', 1694: '31788', 1695: '31808', 1696: '31910', 1697: '32011', 1698: '32244', 1699: '32351', 1700: '32358', 1701: '32388', 1702: '32486', 1703: '32607', 1704: '32648', 1705: '32658', 1706: '32726', 1707: '32821', 1708: '32905', 1709: '32967', 1710: '3307', 1711: '33078', 1712: '33199', 1713: '33257', 1714: '33602', 1715: '33673', 1716: '33719', 1717: '33732', 1718: '33783', 1719: '33850', 1720: '33927', 1721: '33996', 1722: '34019', 1723: '34048', 1724: '34274', 1725: '34337', 1726: '34398', 1727: '34487', 1728: '34518', 1729: '34548', 1730: '34562', 1731: '3458', 1732: '34606', 1733: '34680', 1734: '34853', 1735: '34924', 1736: '34939', 1737: '34967', 1738: '34989', 1739: '35083', 1740: '35097', 1741: '35189', 1742: '3526', 1743: '35352', 1744: '35654', 1745: '35756', 1746: '3583', 1747: '35845', 1748: '35947', 1749: '35949', 1750: '36016', 1751: '36049', 1752: '36233', 1753: '36264', 1754: '36290', 1755: '36332', 1756: '36371', 1757: '36443', 1758: '36496', 1759: '36670', 1760: '36834', 1761: '36874', 1762: '36909', 1763: '36912', 1764: '36934', 1765: '36957', 1766: '37007', 1767: '37041', 1768: '3715', 1769: '3741', 1770: '37419', 1771: '37578', 1772: '37674', 1773: '37698', 1774: '37717', 1775: '37719', 1776: '37908', 1777: '38177', 1778: '38225', 1779: '38240', 1780: '38306', 1781: '38414', 1782: '38448', 1783: '38486', 1784: '38564', 1785: '38665', 1786: '38757', 1787: '38788', 1788: '38819', 1789: '38943', 1790: '39040', 1791: '39127', 1792: '39191', 1793: '39199', 1794: '39223', 1795: '39368', 1796: '39545', 1797: '3957', 1798: '39696', 1799: '39827', 1800: '39978', 1801: '40108', 1802: '40112', 1803: '40204', 1804: '40212', 1805: '40282', 1806: '40300', 1807: '40356', 1808: '40605', 1809: '40643', 1810: '40864', 1811: '40877', 1812: '40891', 1813: '41001', 1814: '41044', 1815: '41095', 1816: '41098', 1817: '41129', 1818: '41155', 1819: '41159', 1820: '4125', 1821: '41258', 1822: '41263', 1823: '41383', 1824: '41433', 1825: '41689', 1826: '41700', 1827: '41767', 1828: '41836', 1829: '41892', 1830: '42019', 1831: '4203', 1832: '42051', 1833: '42078', 1834: '42135', 1835: '42177', 1836: '42213', 1837: '42260', 1838: '42422', 1839: '4247', 1840: '42489', 1841: '4259', 1842: '42592', 1843: '42756', 1844: '42789', 1845: '42830', 1846: '42848', 1847: '42924', 1848: '42998', 1849: '43', 1850: '43018', 1851: '43256', 1852: '43371', 1853: '43703', 1854: '43713', 1855: '43866', 1856: '43880', 1857: '43984', 1858: '44077', 1859: '44107', 1860: '44148', 1861: '44181', 1862: '44194', 1863: '44318', 1864: '44325', 1865: '44339', 1866: '44433', 1867: '44435', 1868: '44726', 1869: '44836', 1870: '45062', 1871: '45139', 1872: '45181', 1873: '45248', 1874: '45308', 1875: '45392', 1876: '45492', 1877: '45494', 1878: '45534', 1879: '45536', 1880: '45548', 1881: '45649', 1882: '45659', 1883: '45696', 1884: '45720', 1885: '45800', 1886: '45865', 1887: '46019', 1888: '46057', 1889: '4617', 1890: '46188', 1891: '46257', 1892: '46440', 1893: '46441', 1894: '46515', 1895: '46541', 1896: '46545', 1897: '46601', 1898: '46638', 1899: '46668', 1900: '46694', 1901: '46816', 1902: '46941', 1903: '47070', 1904: '47161', 1905: '47218', 1906: '47275', 1907: '47322', 1908: '47324', 1909: '4747', 1910: '47502', 1911: '47557', 1912: '47560', 1913: '47561', 1914: '47684', 1915: '47690', 1916: '47752', 1917: '47784', 1918: '47793', 1919: '47879', 1920: '47920', 1921: '48010', 1922: '48084', 1923: '48085', 1924: '48185', 1925: '48385', 1926: '48445', 1927: '48481', 1928: '48563', 1929: '48571', 1930: '48632', 1931: '48634', 1932: '48704', 1933: '48743', 1934: '48882', 1935: '48906', 1936: '49074', 1937: '49209', 1938: '49471', 1939: '49490', 1940: '49597', 1941: '49624', 1942: '49690', 1943: '49898', 1944: '49950', 1945: '49962', 1946: '49968', 1947: '50077', 1948: '50085', 1949: '50102', 1950: '50226', 1951: '50271', 1952: '50273', 1953: '50290', 1954: '50505', 1955: '50542', 1956: '50581', 1957: '50782', 1958: '50806', 1959: '50879', 1960: '50927', 1961: '51114', 1962: '51130', 1963: '51243', 1964: '51284', 1965: '51310', 1966: '51352', 1967: '51393', 1968: '51514', 1969: '51590', 1970: '52037', 1971: '52047', 1972: '52094', 1973: '52116', 1974: '52306', 1975: '52354', 1976: '52415', 1977: '5246', 1978: '52554', 1979: '52586', 1980: '52604', 1981: '52908', 1982: '53005', 1983: '53067', 1984: '53069', 1985: '53086', 1986: '53088', 1987: '53103', 1988: '53455', 1989: '53501', 1990: '53529', 1991: '53661', 1992: '5367', 1993: '53765', 1994: '53951', 1995: '53995', 1996: '54050', 1997: '54180', 1998: '54183', 1999: '5419', 2000: '543', 2001: '54362', 2002: '54369', 2003: '54387', 2004: '54388', 2005: '54412', 2006: '54449', 2007: '54453', 2008: '54652', 2009: '54699', 2010: '54714', 2011: '54730', 2012: '54928', 2013: '55010', 2014: '55036', 2015: '55100', 2016: '55138', 2017: '55174', 2018: '55305', 2019: '55341', 2020: '55450', 2021: '55468', 2022: '55483', 2023: '55520', 2024: '55554', 2025: '55831', 2026: '55868', 2027: '55957', 2028: '55959', 2029: '56027', 2030: '56088', 2031: '56107', 2032: '56114', 2033: '56249', 2034: '56360', 2035: '56371', 2036: '56510', 2037: '56689', 2038: '56770', 2039: '56807', 2040: '56838', 2041: '56858', 2042: '56988', 2043: '57121', 2044: '57200', 2045: '5723', 2046: '57290', 2047: '57352', 2048: '57370', 2049: '57386', 2050: '57410', 2051: '57413', 2052: '57504', 2053: '57662', 2054: '57926', 2055: '57997', 2056: '58050', 2057: '5822', 2058: '58309', 2059: '58347', 2060: '5836', 2061: '58368', 2062: '5844', 2063: '58471', 2064: '58479', 2065: '58491', 2066: '58513', 2067: '58559', 2068: '5859', 2069: '58768', 2070: '58849', 2071: '58857', 2072: '58931', 2073: '58971', 2074: '5906', 2075: '59120', 2076: '59232', 2077: '59360', 2078: '59398', 2079: '59429', 2080: '5947', 2081: '59471', 2082: '59500', 2083: '59501', 2084: '59536', 2085: '59540', 2086: '59589', 2087: '59827', 2088: '59840', 2089: '59934', 2090: '59936', 2091: '60039', 2092: '60058', 2093: '6010', 2094: '60130', 2095: '60213', 2096: '60281', 2097: '60318', 2098: '60357', 2099: '60384', 2100: '60512', 2101: '60580', 2102: '60763', 2103: '60780', 2104: '60785', 2105: '60812', 2106: '60960', 2107: '61199', 2108: '6121', 2109: '61242', 2110: '61336', 2111: '61343', 2112: '61468', 2113: '61479', 2114: '61604', 2115: '61627', 2116: '61644', 2117: '61672', 2118: '61767', 2119: '61777', 2120: '61943', 2121: '61961', 2122: '61976', 2123: '61997', 2124: '6220', 2125: '62308', 2126: '62350', 2127: '62379', 2128: '62388', 2129: '62458', 2130: '6258', 2131: '62598', 2132: '62628', 2133: '62774', 2134: '62786', 2135: '62808', 2136: '62847', 2137: '62914', 2138: '62969', 2139: '63032', 2140: '63148', 2141: '63180', 2142: '63288', 2143: '63337', 2144: '63574', 2145: '63610', 2146: '63726', 2147: '63952', 2148: '6397', 2149: '64162', 2150: '64210', 2151: '64215', 2152: '64224', 2153: '64237', 2154: '64266', 2155: '64299', 2156: '64309', 2157: '64355', 2158: '64618', 2159: '64653', 2160: '64685', 2161: '64750', 2162: '64896', 2163: '6493', 2164: '64945', 2165: '6501', 2166: '65027', 2167: '65033', 2168: '65055', 2169: '65092', 2170: '65167', 2171: '65177', 2172: '65257', 2173: '65333', 2174: '65371', 2175: '65550', 2176: '65555', 2177: '65761', 2178: '65772', 2179: '65882', 2180: '66023', 2181: '66132', 2182: '66156', 2183: '66232', 2184: '66249', 2185: '66340', 2186: '66392', 2187: '6645', 2188: '66515', 2189: '66549', 2190: '66556', 2191: '66557', 2192: '66589', 2193: '66737', 2194: '66745', 2195: '66769', 2196: '66798', 2197: '66843', 2198: '67008', 2199: '67017', 2200: '67044', 2201: '67091', 2202: '67219', 2203: '67273', 2204: '67298', 2205: '67484', 2206: '67537', 2207: '67613', 2208: '67633', 2209: '67671', 2210: '67837', 2211: '67911', 2212: '67983', 2213: '6804', 2214: '68057', 2215: '68251', 2216: '68350', 2217: '68419', 2218: '68427', 2219: '68511', 2220: '68512', 2221: '68610', 2222: '68667', 2223: '68725', 2224: '68801', 2225: '68880', 2226: '69140', 2227: '69192', 2228: '69195', 2229: '69355', 2230: '6937', 2231: '69398', 2232: '69481', 2233: '69521', 2234: '69543', 2235: '69544', 2236: '6969', 2237: '69766', 2238: '69823', 2239: '69851', 2240: '69992', 2241: '69997', 2242: '70045', 2243: '70326', 2244: '70344', 2245: '70352', 2246: '70363', 2247: '70513', 2248: '70535', 2249: '70643', 2250: '70649', 2251: '70700', 2252: '70811', 2253: '70828', 2254: '70963', 2255: '70967', 2256: '71025', 2257: '71073', 2258: '71146', 2259: '71191', 2260: '71229', 2261: '7149', 2262: '71714', 2263: '7172', 2264: '71728', 2265: '7188', 2266: '71933', 2267: '71957', 2268: '72161', 2269: '72197', 2270: '72462', 2271: '72582', 2272: '72591', 2273: '7271', 2274: '7272', 2275: '72765', 2276: '72777', 2277: '72796', 2278: '72869', 2279: '72874', 2280: '72878', 2281: '72906', 2282: '72916', 2283: '7294', 2284: '72965', 2285: '73014', 2286: '73065', 2287: '73117', 2288: '73314', 2289: '73320', 2290: '73480', 2291: '73529', 2292: '73633', 2293: '73640', 2294: '73649', 2295: '73757', 2296: '73850', 2297: '73903', 2298: '73953', 2299: '74117', 2300: '74128', 2301: '74217', 2302: '74227', 2303: '74279', 2304: '74373', 2305: '74429', 2306: '7446', 2307: '74492', 2308: '74520', 2309: '74539', 2310: '74540', 2311: '74773', 2312: '74791', 2313: '74846', 2314: '74876', 2315: '74903', 2316: '7493', 2317: '74962', 2318: '75081', 2319: '75120', 2320: '75124', 2321: '75181', 2322: '75246', 2323: '75337', 2324: '75340', 2325: '75415', 2326: '75545', 2327: '75597', 2328: '75655', 2329: '75826', 2330: '75848', 2331: '75851', 2332: '75876', 2333: '75899', 2334: '75943', 2335: '75984', 2336: '75997', 2337: '76006', 2338: '76124', 2339: '76235', 2340: '76274', 2341: '76295', 2342: '76363', 2343: '76400', 2344: '76426', 2345: '76453', 2346: '76649', 2347: '7692', 2348: '76947', 2349: '77088', 2350: '77222', 2351: '77233', 2352: '77352', 2353: '77427', 2354: '77560', 2355: '77566', 2356: '776', 2357: '77606', 2358: '77664', 2359: '77674', 2360: '77716', 2361: '77732', 2362: '77805', 2363: '77830', 2364: '77887', 2365: '78020', 2366: '78227', 2367: '78248', 2368: '78268', 2369: '78284', 2370: '78317', 2371: '78409', 2372: '78525', 2373: '78562', 2374: '78578', 2375: '78581', 2376: '78741', 2377: '78765', 2378: '78808', 2379: '7881', 2380: '78825', 2381: '78849', 2382: '78859', 2383: '78912', 2384: '78964', 2385: '79011', 2386: '79047', 2387: '79094', 2388: '79186', 2389: '79189', 2390: '79247', 2391: '79462', 2392: '79474', 2393: '79478', 2394: '79552', 2395: '79650', 2396: '79664', 2397: '79695', 2398: '79719', 2399: '79745', 2400: '79774', 2401: '79887', 2402: '80085', 2403: '80301', 2404: '80323', 2405: '80352', 2406: '80421', 2407: '80536', 2408: '80744', 2409: '80758', 2410: '80826', 2411: '8091', 2412: '80939', 2413: '80994', 2414: '81026', 2415: '81110', 2416: '8119', 2417: '81225', 2418: '81236', 2419: '81335', 2420: '81418', 2421: '81513', 2422: '81778', 2423: '81783', 2424: '81791', 2425: '81796', 2426: '81851', 2427: '81877', 2428: '81953', 2429: '82022', 2430: '82038', 2431: '82214', 2432: '82225', 2433: '82253', 2434: '82335', 2435: '82461', 2436: '82508', 2437: '82559', 2438: '82788', 2439: '82816', 2440: '82847', 2441: '82862', 2442: '82935', 2443: '83131', 2444: '83197', 2445: '83207', 2446: '83234', 2447: '83245', 2448: '83262', 2449: '8328', 2450: '8337', 2451: '83387', 2452: '83403', 2453: '8341', 2454: '83535', 2455: '83690', 2456: '83734', 2457: '83739', 2458: '83753', 2459: '83796', 2460: '83897', 2461: '83921', 2462: '8419', 2463: '84217', 2464: '84225', 2465: '84269', 2466: '84288', 2467: '84301', 2468: '84326', 2469: '8441', 2470: '8444', 2471: '8476', 2472: '84979', 2473: '85000', 2474: '85102', 2475: '85103', 2476: '85135', 2477: '85150', 2478: '85153', 2479: '85155', 2480: '85252', 2481: '85359', 2482: '8538', 2483: '85381', 2484: '854', 2485: '85416', 2486: '85449', 2487: '8553', 2488: '85771', 2489: '85774', 2490: '8590', 2491: '85975', 2492: '86054', 2493: '86062', 2494: '86102', 2495: '86173', 2496: '86210', 2497: '8626', 2498: '86294', 2499: '86301', 2500: '86453', 2501: '86731', 2502: '86743', 2503: '86772', 2504: '86917', 2505: '86940', 2506: '86985', 2507: '87097', 2508: '87109', 2509: '8730', 2510: '87381', 2511: '87398', 2512: '87429', 2513: '87539', 2514: '87580', 2515: '87615', 2516: '87656', 2517: '87729', 2518: '8780', 2519: '87924', 2520: '87956', 2521: '88136', 2522: '88205', 2523: '88221', 2524: '8823', 2525: '88339', 2526: '8849', 2527: '88553', 2528: '88660', 2529: '88683', 2530: '88706', 2531: '88743', 2532: '88750', 2533: '88966', 2534: '88978', 2535: '88997', 2536: '89001', 2537: '89002', 2538: '89338', 2539: '89464', 2540: '89481', 2541: '89530', 2542: '8954', 2543: '89547', 2544: '89596', 2545: '89600', 2546: '89631', 2547: '89695', 2548: '89723', 2549: '89781', 2550: '89839', 2551: '8986', 2552: '89894', 2553: '89907', 2554: '89948', 2555: '89977', 2556: '90010', 2557: '90134', 2558: '90222', 2559: '90321', 2560: '90455', 2561: '90544', 2562: '90599', 2563: '90625', 2564: '90748', 2565: '90764', 2566: '90774', 2567: '90864', 2568: '90909', 2569: '90974', 2570: '91079', 2571: '91085', 2572: '91277', 2573: '9128', 2574: '91293', 2575: '91355', 2576: '91542', 2577: '91573', 2578: '91651', 2579: '91655', 2580: '9170', 2581: '91708', 2582: '91777', 2583: '91833', 2584: '92145', 2585: '92204', 2586: '92235', 2587: '92319', 2588: '92351', 2589: '92525', 2590: '92567', 2591: '92582', 2592: '92583', 2593: '92585', 2594: '92632', 2595: '92674', 2596: '92689', 2597: '92786', 2598: '92795', 2599: '92806', 2600: '92877', 2601: '92909', 2602: '92949', 2603: '9295', 2604: '93002', 2605: '93127', 2606: '93212', 2607: '93246', 2608: '93250', 2609: '93459', 2610: '93477', 2611: '9360', 2612: '93604', 2613: '93643', 2614: '93682', 2615: '93712', 2616: '93747', 2617: '93918', 2618: '93943', 2619: '94095', 2620: '94143', 2621: '94145', 2622: '94259', 2623: '94353', 2624: '94433', 2625: '94626', 2626: '94648', 2627: '94674', 2628: '94677', 2629: '94747', 2630: '94931', 2631: '94967', 2632: '94983', 2633: '95014', 2634: '95314', 2635: '95435', 2636: '95532', 2637: '95637', 2638: '9566', 2639: '95789', 2640: '95875', 2641: '95901', 2642: '95938', 2643: '95975', 2644: '96032', 2645: '9604', 2646: '9606', 2647: '96132', 2648: '96177', 2649: '96185', 2650: '96243', 2651: '9630', 2652: '96317', 2653: '9634', 2654: '96817', 2655: '97053', 2656: '97149', 2657: '97238', 2658: '97316', 2659: '97339', 2660: '97389', 2661: '9742', 2662: '97478', 2663: '97531', 2664: '97556', 2665: '97670', 2666: '97671', 2667: '97704', 2668: '97853', 2669: '97914', 2670: '98008', 2671: '98014', 2672: '98028', 2673: '98049', 2674: '98123', 2675: '98160', 2676: '98216', 2677: '9823', 2678: '98388', 2679: '98438', 2680: '98523', 2681: '9853', 2682: '98586', 2683: '98630', 2684: '98716', 2685: '98730', 2686: '98988', 2687: '98991', 2688: '99010', 2689: '99021', 2690: '99035', 2691: '99331', 2692: '99610', 2693: '99647', 2694: '99724', 2695: '99821', 2696: '99925', 2697: '99931', 2698: '99974'}\n"
     ]
    }
   ],
   "source": [
    "indices_to_class_labels_dict = {value : key for key, value in train_generator.class_indices.items()}\n",
    "print(len(indices_to_class_labels_dict))\n",
    "print(indices_to_class_labels_dict)\n",
    "with open(\"group8_indices_to_class_labels_dict.json\", \"wb\") as pickle_file:\n",
    "    pickle.dump(indices_to_class_labels_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks and Fitting Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFolder = 'checkpoints_group8'\n",
    "if not os.path.exists(outputFolder):\n",
    "    os.makedirs(outputFolder)\n",
    "checkpoint_filepath=outputFolder+\"/model-{epoch:02d}-{val_acc:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath, monitor='val_acc', verbose=1, mode='max',\n",
    "    save_best_only=True, save_weights_only=True,\n",
    "    save_frequency='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Apps/jupyterWorkspace/google_landmark_detection/Models\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-4c3d897e268e>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 6.9206 - acc: 0.0326\n",
      "Epoch 00001: val_acc improved from -inf to 0.14893, saving model to checkpoints_group8/model-01-0.15.hdf5\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 6.9206 - acc: 0.0326 - val_loss: 5.0856 - val_acc: 0.1489\n",
      "Epoch 2/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 4.5435 - acc: 0.2060\n",
      "Epoch 00002: val_acc improved from 0.14893 to 0.33486, saving model to checkpoints_group8/model-02-0.33.hdf5\n",
      "1821/1821 [==============================] - 575s 316ms/step - loss: 4.5435 - acc: 0.2060 - val_loss: 3.7151 - val_acc: 0.3349\n",
      "Epoch 3/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 3.5698 - acc: 0.3311\n",
      "Epoch 00003: val_acc improved from 0.33486 to 0.41796, saving model to checkpoints_group8/model-03-0.42.hdf5\n",
      "1821/1821 [==============================] - 575s 316ms/step - loss: 3.5698 - acc: 0.3311 - val_loss: 3.1665 - val_acc: 0.4180\n",
      "Epoch 4/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 2.9901 - acc: 0.4170\n",
      "Epoch 00004: val_acc improved from 0.41796 to 0.45629, saving model to checkpoints_group8/model-04-0.46.hdf5\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 2.9901 - acc: 0.4170 - val_loss: 2.9501 - val_acc: 0.4563\n",
      "Epoch 5/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 2.5951 - acc: 0.4743\n",
      "Epoch 00005: val_acc improved from 0.45629 to 0.45994, saving model to checkpoints_group8/model-05-0.46.hdf5\n",
      "1821/1821 [==============================] - 576s 316ms/step - loss: 2.5951 - acc: 0.4743 - val_loss: 2.9750 - val_acc: 0.4599\n",
      "Epoch 6/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 2.2809 - acc: 0.5261\n",
      "Epoch 00006: val_acc improved from 0.45994 to 0.51411, saving model to checkpoints_group8/model-06-0.51.hdf5\n",
      "1821/1821 [==============================] - 575s 316ms/step - loss: 2.2809 - acc: 0.5261 - val_loss: 2.6760 - val_acc: 0.5141\n",
      "Epoch 7/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 2.0198 - acc: 0.5693\n",
      "Epoch 00007: val_acc did not improve from 0.51411\n",
      "1821/1821 [==============================] - 576s 316ms/step - loss: 2.0198 - acc: 0.5693 - val_loss: 2.7222 - val_acc: 0.5080\n",
      "Epoch 8/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 1.8113 - acc: 0.6056\n",
      "Epoch 00008: val_acc improved from 0.51411 to 0.52596, saving model to checkpoints_group8/model-08-0.53.hdf5\n",
      "1821/1821 [==============================] - 575s 316ms/step - loss: 1.8113 - acc: 0.6056 - val_loss: 2.6314 - val_acc: 0.5260\n",
      "Epoch 9/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 1.6289 - acc: 0.6369\n",
      "Epoch 00009: val_acc improved from 0.52596 to 0.53533, saving model to checkpoints_group8/model-09-0.54.hdf5\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 1.6289 - acc: 0.6369 - val_loss: 2.6368 - val_acc: 0.5353\n",
      "Epoch 10/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 1.4714 - acc: 0.6648\n",
      "Epoch 00010: val_acc did not improve from 0.53533\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 1.4714 - acc: 0.6648 - val_loss: 2.7845 - val_acc: 0.5244\n",
      "Epoch 11/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 1.3337 - acc: 0.6906\n",
      "Epoch 00011: val_acc improved from 0.53533 to 0.56750, saving model to checkpoints_group8/model-11-0.57.hdf5\n",
      "1821/1821 [==============================] - 573s 315ms/step - loss: 1.3337 - acc: 0.6906 - val_loss: 2.5527 - val_acc: 0.5675\n",
      "Epoch 12/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 1.2136 - acc: 0.7121\n",
      "Epoch 00012: val_acc did not improve from 0.56750\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 1.2136 - acc: 0.7121 - val_loss: 2.5298 - val_acc: 0.5559\n",
      "Epoch 13/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 1.0995 - acc: 0.7355\n",
      "Epoch 00013: val_acc did not improve from 0.56750\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 1.0995 - acc: 0.7355 - val_loss: 2.6895 - val_acc: 0.5460\n",
      "Epoch 14/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 1.0015 - acc: 0.7553\n",
      "Epoch 00014: val_acc improved from 0.56750 to 0.56942, saving model to checkpoints_group8/model-14-0.57.hdf5\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 1.0015 - acc: 0.7553 - val_loss: 2.6274 - val_acc: 0.5694\n",
      "Epoch 15/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.9209 - acc: 0.7715\n",
      "Epoch 00015: val_acc improved from 0.56942 to 0.58158, saving model to checkpoints_group8/model-15-0.58.hdf5\n",
      "1821/1821 [==============================] - 575s 316ms/step - loss: 0.9209 - acc: 0.7715 - val_loss: 2.5192 - val_acc: 0.5816\n",
      "Epoch 16/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.8411 - acc: 0.7882\n",
      "Epoch 00016: val_acc did not improve from 0.58158\n",
      "1821/1821 [==============================] - 576s 316ms/step - loss: 0.8411 - acc: 0.7882 - val_loss: 2.8456 - val_acc: 0.5443\n",
      "Epoch 17/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.7714 - acc: 0.8034\n",
      "Epoch 00017: val_acc improved from 0.58158 to 0.60395, saving model to checkpoints_group8/model-17-0.60.hdf5\n",
      "1821/1821 [==============================] - 576s 316ms/step - loss: 0.7714 - acc: 0.8034 - val_loss: 2.5268 - val_acc: 0.6039\n",
      "Epoch 18/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.7122 - acc: 0.8159\n",
      "Epoch 00018: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 577s 317ms/step - loss: 0.7122 - acc: 0.8159 - val_loss: 3.3241 - val_acc: 0.5056\n",
      "Epoch 19/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.6540 - acc: 0.8292\n",
      "Epoch 00019: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 577s 317ms/step - loss: 0.6540 - acc: 0.8292 - val_loss: 2.6696 - val_acc: 0.5814\n",
      "Epoch 20/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.5969 - acc: 0.8431\n",
      "Epoch 00020: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 577s 317ms/step - loss: 0.5969 - acc: 0.8431 - val_loss: 2.7479 - val_acc: 0.5884\n",
      "Epoch 21/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.5628 - acc: 0.8502\n",
      "Epoch 00021: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 577s 317ms/step - loss: 0.5628 - acc: 0.8502 - val_loss: 2.6194 - val_acc: 0.6004\n",
      "Epoch 22/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.5212 - acc: 0.8609\n",
      "Epoch 00022: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 0.5212 - acc: 0.8609 - val_loss: 2.8273 - val_acc: 0.5970\n",
      "Epoch 23/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.4906 - acc: 0.8678\n",
      "Epoch 00023: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.4906 - acc: 0.8678 - val_loss: 2.9305 - val_acc: 0.5864\n",
      "Epoch 24/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.4544 - acc: 0.8771\n",
      "Epoch 00024: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 0.4544 - acc: 0.8771 - val_loss: 3.0853 - val_acc: 0.5688\n",
      "Epoch 25/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.4280 - acc: 0.8841\n",
      "Epoch 00025: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 0.4280 - acc: 0.8841 - val_loss: 3.1946 - val_acc: 0.5699\n",
      "Epoch 26/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.4017 - acc: 0.8909\n",
      "Epoch 00026: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 575s 316ms/step - loss: 0.4017 - acc: 0.8909 - val_loss: 2.9069 - val_acc: 0.5913\n",
      "Epoch 27/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3811 - acc: 0.8962\n",
      "Epoch 00027: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 577s 317ms/step - loss: 0.3811 - acc: 0.8962 - val_loss: 3.0636 - val_acc: 0.5975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3580 - acc: 0.9023\n",
      "Epoch 00028: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 577s 317ms/step - loss: 0.3580 - acc: 0.9023 - val_loss: 3.1347 - val_acc: 0.5699\n",
      "Epoch 29/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3409 - acc: 0.9067\n",
      "Epoch 00029: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 577s 317ms/step - loss: 0.3409 - acc: 0.9067 - val_loss: 2.9463 - val_acc: 0.5886\n",
      "Epoch 30/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3267 - acc: 0.9095\n",
      "Epoch 00030: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 577s 317ms/step - loss: 0.3267 - acc: 0.9095 - val_loss: 3.4668 - val_acc: 0.5434\n",
      "Epoch 31/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3082 - acc: 0.9150\n",
      "Epoch 00031: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 575s 316ms/step - loss: 0.3082 - acc: 0.9150 - val_loss: 3.1564 - val_acc: 0.5639\n",
      "Epoch 32/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2966 - acc: 0.9175\n",
      "Epoch 00032: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.2966 - acc: 0.9175 - val_loss: 3.4165 - val_acc: 0.5708\n",
      "Epoch 33/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2846 - acc: 0.9212\n",
      "Epoch 00033: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 0.2846 - acc: 0.9212 - val_loss: 3.5466 - val_acc: 0.5554\n",
      "Epoch 34/100\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2698 - acc: 0.9253\n",
      "Epoch 00034: val_acc did not improve from 0.60395\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.2698 - acc: 0.9253 - val_loss: 2.9279 - val_acc: 0.5998\n",
      "Epoch 35/100\n",
      "1320/1821 [====================>.........] - ETA: 2:23 - loss: 0.2555 - acc: 0.9295"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch = train_generator.n // batch_size,\n",
    "    validation_data = valid_generator,\n",
    "    validation_steps = valid_generator.n // batch_size,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    epochs = 100,\n",
    "    workers = 8,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"checkpoints_group8/model-17-0.60.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-e9b52b070972>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.7437 - acc: 0.8083\n",
      "Epoch 00001: val_acc improved from -inf to 0.58229, saving model to checkpoints_group8/model-01-0.58.hdf5\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.7437 - acc: 0.8083 - val_loss: 2.7319 - val_acc: 0.5823\n",
      "Epoch 2/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.6686 - acc: 0.8258\n",
      "Epoch 00002: val_acc did not improve from 0.58229\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.6686 - acc: 0.8258 - val_loss: 2.9384 - val_acc: 0.5670\n",
      "Epoch 3/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.6096 - acc: 0.8394\n",
      "Epoch 00003: val_acc did not improve from 0.58229\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.6096 - acc: 0.8394 - val_loss: 2.7553 - val_acc: 0.5770\n",
      "Epoch 4/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.5716 - acc: 0.8481\n",
      "Epoch 00004: val_acc improved from 0.58229 to 0.58560, saving model to checkpoints_group8/model-04-0.59.hdf5\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.5716 - acc: 0.8481 - val_loss: 2.8857 - val_acc: 0.5856\n",
      "Epoch 5/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.5278 - acc: 0.8587\n",
      "Epoch 00005: val_acc improved from 0.58560 to 0.59250, saving model to checkpoints_group8/model-05-0.59.hdf5\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.5278 - acc: 0.8587 - val_loss: 2.7887 - val_acc: 0.5925\n",
      "Epoch 6/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.4868 - acc: 0.8701\n",
      "Epoch 00006: val_acc did not improve from 0.59250\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.4868 - acc: 0.8701 - val_loss: 2.9132 - val_acc: 0.5850\n",
      "Epoch 7/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.4610 - acc: 0.8761\n",
      "Epoch 00007: val_acc improved from 0.59250 to 0.60692, saving model to checkpoints_group8/model-07-0.61.hdf5\n",
      "1821/1821 [==============================] - 575s 316ms/step - loss: 0.4610 - acc: 0.8761 - val_loss: 2.7659 - val_acc: 0.6069\n",
      "Epoch 8/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.4302 - acc: 0.8835\n",
      "Epoch 00008: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.4302 - acc: 0.8835 - val_loss: 2.9392 - val_acc: 0.5993\n",
      "Epoch 9/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.4035 - acc: 0.8895\n",
      "Epoch 00009: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.4035 - acc: 0.8895 - val_loss: 2.8333 - val_acc: 0.5945\n",
      "Epoch 10/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3843 - acc: 0.8953\n",
      "Epoch 00010: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.3843 - acc: 0.8953 - val_loss: 2.9185 - val_acc: 0.5857\n",
      "Epoch 11/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3652 - acc: 0.9004\n",
      "Epoch 00011: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.3652 - acc: 0.9004 - val_loss: 3.2981 - val_acc: 0.5662\n",
      "Epoch 12/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3448 - acc: 0.9048\n",
      "Epoch 00012: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.3448 - acc: 0.9048 - val_loss: 3.2809 - val_acc: 0.5742\n",
      "Epoch 13/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3282 - acc: 0.9096\n",
      "Epoch 00013: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.3282 - acc: 0.9096 - val_loss: 2.9035 - val_acc: 0.6062\n",
      "Epoch 14/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3141 - acc: 0.9128\n",
      "Epoch 00014: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.3141 - acc: 0.9128 - val_loss: 2.9062 - val_acc: 0.6000\n",
      "Epoch 15/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.3031 - acc: 0.9163\n",
      "Epoch 00015: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.3031 - acc: 0.9163 - val_loss: 3.1458 - val_acc: 0.5887\n",
      "Epoch 16/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2806 - acc: 0.9220\n",
      "Epoch 00016: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.2806 - acc: 0.9220 - val_loss: 3.2789 - val_acc: 0.5745\n",
      "Epoch 17/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2714 - acc: 0.9245\n",
      "Epoch 00017: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.2714 - acc: 0.9245 - val_loss: 3.1103 - val_acc: 0.5945\n",
      "Epoch 18/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2659 - acc: 0.9261\n",
      "Epoch 00018: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.2659 - acc: 0.9261 - val_loss: 4.3700 - val_acc: 0.4814\n",
      "Epoch 19/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2510 - acc: 0.9304\n",
      "Epoch 00019: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.2510 - acc: 0.9304 - val_loss: 3.0615 - val_acc: 0.6038\n",
      "Epoch 20/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2431 - acc: 0.9331\n",
      "Epoch 00020: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.2431 - acc: 0.9331 - val_loss: 3.1847 - val_acc: 0.6056\n",
      "Epoch 21/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2356 - acc: 0.9352\n",
      "Epoch 00021: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.2356 - acc: 0.9352 - val_loss: 3.4845 - val_acc: 0.5788\n",
      "Epoch 22/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2296 - acc: 0.9354\n",
      "Epoch 00022: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.2296 - acc: 0.9354 - val_loss: 4.4444 - val_acc: 0.4945\n",
      "Epoch 23/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2181 - acc: 0.9384\n",
      "Epoch 00023: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.2181 - acc: 0.9384 - val_loss: 3.6182 - val_acc: 0.5684\n",
      "Epoch 24/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2133 - acc: 0.9395\n",
      "Epoch 00024: val_acc did not improve from 0.60692\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.2133 - acc: 0.9395 - val_loss: 3.2604 - val_acc: 0.5944\n",
      "Epoch 25/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2078 - acc: 0.9423\n",
      "Epoch 00025: val_acc improved from 0.60692 to 0.60914, saving model to checkpoints_group8/model-25-0.61.hdf5\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.2078 - acc: 0.9423 - val_loss: 3.0147 - val_acc: 0.6091\n",
      "Epoch 26/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.2016 - acc: 0.9432\n",
      "Epoch 00026: val_acc did not improve from 0.60914\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 0.2016 - acc: 0.9432 - val_loss: 3.3484 - val_acc: 0.6055\n",
      "Epoch 27/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1945 - acc: 0.9461\n",
      "Epoch 00027: val_acc did not improve from 0.60914\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1945 - acc: 0.9461 - val_loss: 3.5657 - val_acc: 0.5859\n",
      "Epoch 28/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1884 - acc: 0.9476\n",
      "Epoch 00028: val_acc did not improve from 0.60914\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.1884 - acc: 0.9476 - val_loss: 3.3765 - val_acc: 0.6070\n",
      "Epoch 29/65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1910 - acc: 0.9450\n",
      "Epoch 00029: val_acc improved from 0.60914 to 0.60918, saving model to checkpoints_group8/model-29-0.61.hdf5\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1910 - acc: 0.9450 - val_loss: 3.1141 - val_acc: 0.6092\n",
      "Epoch 30/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1803 - acc: 0.9493\n",
      "Epoch 00030: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1803 - acc: 0.9493 - val_loss: 3.7513 - val_acc: 0.5649\n",
      "Epoch 31/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1770 - acc: 0.9499\n",
      "Epoch 00031: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1770 - acc: 0.9499 - val_loss: 3.4016 - val_acc: 0.6008\n",
      "Epoch 32/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1729 - acc: 0.9511\n",
      "Epoch 00032: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1729 - acc: 0.9511 - val_loss: 3.7176 - val_acc: 0.5662\n",
      "Epoch 33/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1694 - acc: 0.9521\n",
      "Epoch 00033: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1694 - acc: 0.9521 - val_loss: 3.3356 - val_acc: 0.6044\n",
      "Epoch 34/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1659 - acc: 0.9534\n",
      "Epoch 00034: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1659 - acc: 0.9534 - val_loss: 3.5529 - val_acc: 0.5792\n",
      "Epoch 35/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1603 - acc: 0.9545\n",
      "Epoch 00035: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1603 - acc: 0.9545 - val_loss: 3.4208 - val_acc: 0.5913\n",
      "Epoch 36/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1602 - acc: 0.9547\n",
      "Epoch 00036: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1602 - acc: 0.9547 - val_loss: 3.7320 - val_acc: 0.5970\n",
      "Epoch 37/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1555 - acc: 0.9561\n",
      "Epoch 00037: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1555 - acc: 0.9561 - val_loss: 3.3771 - val_acc: 0.5957\n",
      "Epoch 38/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1503 - acc: 0.9574\n",
      "Epoch 00038: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1503 - acc: 0.9574 - val_loss: 3.8615 - val_acc: 0.5813\n",
      "Epoch 39/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1487 - acc: 0.9572\n",
      "Epoch 00039: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1487 - acc: 0.9572 - val_loss: 3.5872 - val_acc: 0.6058\n",
      "Epoch 40/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1463 - acc: 0.9582\n",
      "Epoch 00040: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1463 - acc: 0.9582 - val_loss: 3.6576 - val_acc: 0.5955\n",
      "Epoch 41/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1450 - acc: 0.9589\n",
      "Epoch 00041: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1450 - acc: 0.9589 - val_loss: 3.7583 - val_acc: 0.5761\n",
      "Epoch 42/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1389 - acc: 0.9603\n",
      "Epoch 00042: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1389 - acc: 0.9603 - val_loss: 4.0614 - val_acc: 0.5515\n",
      "Epoch 43/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1385 - acc: 0.9604\n",
      "Epoch 00043: val_acc did not improve from 0.60918\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1385 - acc: 0.9604 - val_loss: 3.6102 - val_acc: 0.5995\n",
      "Epoch 44/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1364 - acc: 0.9612\n",
      "Epoch 00044: val_acc improved from 0.60918 to 0.61264, saving model to checkpoints_group8/model-44-0.61.hdf5\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.1364 - acc: 0.9612 - val_loss: 3.4512 - val_acc: 0.6126\n",
      "Epoch 45/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1290 - acc: 0.9631\n",
      "Epoch 00045: val_acc improved from 0.61264 to 0.61521, saving model to checkpoints_group8/model-45-0.62.hdf5\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1290 - acc: 0.9631 - val_loss: 3.3944 - val_acc: 0.6152\n",
      "Epoch 46/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1302 - acc: 0.9621\n",
      "Epoch 00046: val_acc did not improve from 0.61521\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1302 - acc: 0.9621 - val_loss: 3.5787 - val_acc: 0.6050\n",
      "Epoch 47/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1300 - acc: 0.9627\n",
      "Epoch 00047: val_acc improved from 0.61521 to 0.62189, saving model to checkpoints_group8/model-47-0.62.hdf5\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1300 - acc: 0.9627 - val_loss: 3.4671 - val_acc: 0.6219\n",
      "Epoch 48/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1241 - acc: 0.9641\n",
      "Epoch 00048: val_acc did not improve from 0.62189\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1241 - acc: 0.9641 - val_loss: 4.1764 - val_acc: 0.5530\n",
      "Epoch 49/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1239 - acc: 0.9647\n",
      "Epoch 00049: val_acc did not improve from 0.62189\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1239 - acc: 0.9647 - val_loss: 4.0286 - val_acc: 0.5782\n",
      "Epoch 50/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1222 - acc: 0.9651\n",
      "Epoch 00050: val_acc did not improve from 0.62189\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1222 - acc: 0.9651 - val_loss: 3.6982 - val_acc: 0.5903\n",
      "Epoch 51/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1217 - acc: 0.9650\n",
      "Epoch 00051: val_acc did not improve from 0.62189\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1217 - acc: 0.9650 - val_loss: 3.6917 - val_acc: 0.5997\n",
      "Epoch 52/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1163 - acc: 0.9668\n",
      "Epoch 00052: val_acc did not improve from 0.62189\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1163 - acc: 0.9668 - val_loss: 3.4561 - val_acc: 0.6147\n",
      "Epoch 53/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1175 - acc: 0.9665\n",
      "Epoch 00053: val_acc did not improve from 0.62189\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1175 - acc: 0.9665 - val_loss: 3.6318 - val_acc: 0.6151\n",
      "Epoch 54/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1156 - acc: 0.9671\n",
      "Epoch 00054: val_acc did not improve from 0.62189\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1156 - acc: 0.9671 - val_loss: 3.6020 - val_acc: 0.6058\n",
      "Epoch 55/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1162 - acc: 0.9664\n",
      "Epoch 00055: val_acc improved from 0.62189 to 0.62198, saving model to checkpoints_group8/model-55-0.62.hdf5\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1162 - acc: 0.9664 - val_loss: 3.4656 - val_acc: 0.6220\n",
      "Epoch 56/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1095 - acc: 0.9684\n",
      "Epoch 00056: val_acc did not improve from 0.62198\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1095 - acc: 0.9684 - val_loss: 3.6377 - val_acc: 0.6021\n",
      "Epoch 57/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1103 - acc: 0.9683\n",
      "Epoch 00057: val_acc improved from 0.62198 to 0.63117, saving model to checkpoints_group8/model-57-0.63.hdf5\n",
      "1821/1821 [==============================] - 571s 313ms/step - loss: 0.1103 - acc: 0.9683 - val_loss: 3.5009 - val_acc: 0.6312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1078 - acc: 0.9682\n",
      "Epoch 00058: val_acc did not improve from 0.63117\n",
      "1821/1821 [==============================] - 570s 313ms/step - loss: 0.1078 - acc: 0.9682 - val_loss: 4.0062 - val_acc: 0.5851\n",
      "Epoch 59/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1073 - acc: 0.9685\n",
      "Epoch 00059: val_acc did not improve from 0.63117\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 0.1073 - acc: 0.9685 - val_loss: 3.4204 - val_acc: 0.6224\n",
      "Epoch 60/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1043 - acc: 0.9699\n",
      "Epoch 00060: val_acc did not improve from 0.63117\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.1043 - acc: 0.9699 - val_loss: 3.6474 - val_acc: 0.6213\n",
      "Epoch 61/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1038 - acc: 0.9704\n",
      "Epoch 00061: val_acc did not improve from 0.63117\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.1038 - acc: 0.9704 - val_loss: 3.8069 - val_acc: 0.5849\n",
      "Epoch 62/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1053 - acc: 0.9698\n",
      "Epoch 00062: val_acc did not improve from 0.63117\n",
      "1821/1821 [==============================] - 574s 315ms/step - loss: 0.1053 - acc: 0.9698 - val_loss: 3.7228 - val_acc: 0.6094\n",
      "Epoch 63/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.0964 - acc: 0.9725\n",
      "Epoch 00063: val_acc did not improve from 0.63117\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 0.0964 - acc: 0.9725 - val_loss: 3.6078 - val_acc: 0.6300\n",
      "Epoch 64/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.1007 - acc: 0.9707\n",
      "Epoch 00064: val_acc did not improve from 0.63117\n",
      "1821/1821 [==============================] - 571s 314ms/step - loss: 0.1007 - acc: 0.9707 - val_loss: 3.5798 - val_acc: 0.6294\n",
      "Epoch 65/65\n",
      "1821/1821 [==============================] - ETA: 0s - loss: 0.0992 - acc: 0.9716\n",
      "Epoch 00065: val_acc did not improve from 0.63117\n",
      "1821/1821 [==============================] - 572s 314ms/step - loss: 0.0992 - acc: 0.9716 - val_loss: 4.2847 - val_acc: 0.5841\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch = train_generator.n // batch_size,\n",
    "    validation_data = valid_generator,\n",
    "    validation_steps = valid_generator.n // batch_size,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    epochs = 65,\n",
    "    workers = 8,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training accuracy')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsedrfgdffg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
